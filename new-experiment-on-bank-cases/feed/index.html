<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	>
<channel>
	<title>Comments on: New Experiment on Bank Cases</title>
	<atom:link href="http://certaindoubts.com/new-experiment-on-bank-cases/feed/" rel="self" type="application/rss+xml" />
	<link>http://certaindoubts.com/new-experiment-on-bank-cases/</link>
	<description>devoted to matters epistemic</description>
	<lastBuildDate>Wed, 10 Apr 2019 16:37:28 +0000</lastBuildDate>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.10</generator>
	<item>
		<title>By: Angel Pinillos</title>
		<link>http://certaindoubts.com/new-experiment-on-bank-cases/#comment-12475</link>
		<dc:creator><![CDATA[Angel Pinillos]]></dc:creator>
		<pubDate>Tue, 20 Jul 2010 22:39:11 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1999#comment-12475</guid>
		<description><![CDATA[Hi Wesley, one more thing. I got a stat. sig. difference between low stakes and ignorant high stakes (and no stat. sig. difference between high stakes and ignorant high stakes). This would not at all be expected given your suggestion. After all, if you are aware of high stakes, you will gather more evidence (before forming the relevant belief) than someone similarly situated but who was not aware of the high stakes of her situation.]]></description>
		<content:encoded><![CDATA[<p>Hi Wesley, one more thing. I got a stat. sig. difference between low stakes and ignorant high stakes (and no stat. sig. difference between high stakes and ignorant high stakes). This would not at all be expected given your suggestion. After all, if you are aware of high stakes, you will gather more evidence (before forming the relevant belief) than someone similarly situated but who was not aware of the high stakes of her situation.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Angel Pinillos</title>
		<link>http://certaindoubts.com/new-experiment-on-bank-cases/#comment-12474</link>
		<dc:creator><![CDATA[Angel Pinillos]]></dc:creator>
		<pubDate>Tue, 20 Jul 2010 22:29:52 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1999#comment-12474</guid>
		<description><![CDATA[Hi Wesley, 
Yes, I think your proposed experiment would work as you predict. As I mentioned above, this possible result does not tell against my position however. The best way of testing your suggestion (and Jennifer&#039;s) IMO is to simply stipulate in the vignette that Peter already believes/thinks that there are no typos. And now the question we ask subjects is this: &quot;how many times do you think Peter (who already believes there are no typos) has to check for typos before he knows there are no typos?&quot; I think you will still get a difference between high and low stakes. If we don&#039;t get a difference, then your objection would be vindicated. But my intuition about this modified case is that there will still be a difference.]]></description>
		<content:encoded><![CDATA[<p>Hi Wesley,<br />
Yes, I think your proposed experiment would work as you predict. As I mentioned above, this possible result does not tell against my position however. The best way of testing your suggestion (and Jennifer&#8217;s) IMO is to simply stipulate in the vignette that Peter already believes/thinks that there are no typos. And now the question we ask subjects is this: &#8220;how many times do you think Peter (who already believes there are no typos) has to check for typos before he knows there are no typos?&#8221; I think you will still get a difference between high and low stakes. If we don&#8217;t get a difference, then your objection would be vindicated. But my intuition about this modified case is that there will still be a difference.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Wesley</title>
		<link>http://certaindoubts.com/new-experiment-on-bank-cases/#comment-12472</link>
		<dc:creator><![CDATA[Wesley]]></dc:creator>
		<pubDate>Tue, 20 Jul 2010 22:14:04 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1999#comment-12472</guid>
		<description><![CDATA[Hey Angel, Sorry, I was thinking about it more in regards to your earlier discussion with Jennifer (over on the XPHIB under your original post) in terms of confidence to form the relevant belief.  Instead of using the ignorance cases, I was thinking that we could just set up a 2x2 to directly test the worry that she, and now I am raising about your results.  Namely, whether the differences in the amount of evidence collected between lo and hi stakes subjects arises not because third-person mental state attributions involving knowledge are intrinsically sensitive to stakes or anything like that, but rather because participants are just thinking high-stakes subjects are expected to collect more evidence than low-stakes subjects to actually have an outright belief on the issue at all.

So since I am a firm believer of proposing objections to studies that can actually be tested, and which plausibly account for the data in hand, I was suggesting we run the same hi/lo cases with other predicated besides knowledge to see if people are so influenced by this confidence issue, telegraphed by their propensity to more or less display the same asymmetry you detected for lots of different predicates besides knowledge.

When I run both of the (soon to be famous!) proofreading cases on myself, it seems to work for things like believes, decides and thinks.  Does it work for anybody else?]]></description>
		<content:encoded><![CDATA[<p>Hey Angel, Sorry, I was thinking about it more in regards to your earlier discussion with Jennifer (over on the XPHIB under your original post) in terms of confidence to form the relevant belief.  Instead of using the ignorance cases, I was thinking that we could just set up a 2&#215;2 to directly test the worry that she, and now I am raising about your results.  Namely, whether the differences in the amount of evidence collected between lo and hi stakes subjects arises not because third-person mental state attributions involving knowledge are intrinsically sensitive to stakes or anything like that, but rather because participants are just thinking high-stakes subjects are expected to collect more evidence than low-stakes subjects to actually have an outright belief on the issue at all.</p>
<p>So since I am a firm believer of proposing objections to studies that can actually be tested, and which plausibly account for the data in hand, I was suggesting we run the same hi/lo cases with other predicated besides knowledge to see if people are so influenced by this confidence issue, telegraphed by their propensity to more or less display the same asymmetry you detected for lots of different predicates besides knowledge.</p>
<p>When I run both of the (soon to be famous!) proofreading cases on myself, it seems to work for things like believes, decides and thinks.  Does it work for anybody else?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Jennifer Nagel</title>
		<link>http://certaindoubts.com/new-experiment-on-bank-cases/#comment-12455</link>
		<dc:creator><![CDATA[Jennifer Nagel]]></dc:creator>
		<pubDate>Tue, 20 Jul 2010 03:30:36 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1999#comment-12455</guid>
		<description><![CDATA[Hi guys,
(1) Josh (@8) and Jonathan (@56) have raised the worry that while raised stakes clearly matter to first-person knowledge ascription, it remains to be seen what the effects might be for how we perceive others.  My sense of the mental state ascription literature is that on all of the major theories there are &lt;b&gt;very&lt;/b&gt; close connections between first- and third-person assessments. In simulation theory (e.g. Goldman) we ascribe states of belief and knowledge to another by pretending to be in his position and running a self-assessment which is then projected onto the other. At the far end of the spectrum (e.g. Carruthers), we gauge our own epistemic position by running on ourselves the same mindreading competence we generally apply to others.  Even in fancy positions that do not make either of first- or third-person mindreading straightforwardly derivative of the other (e.g. Apperly), there is enormous overlap in the resources we have for readings ourselves and others.  If stakes matter to my epistemic self-evaluations, they are going to matter to my evaluations of others -- or at least there&#039;s a heavy burden of argument on anyone who wants to try to show otherwise.  

As for Jonathan&#039;s specific query about the relationship between mood and confidence in first- and third-person assessments, it is going to matter just how closely we attend to the mood of the person evaluated (close enough to suffer emotional contagion?), and we&#039;ll need to be careful to offset a number of complicated interactions involving mood, cognition, and mental state ascription (see Benjamin Converse&#039;s 2008 paper in &lt;i&gt;Emotion&lt;/i&gt; on that -- sorry, WordPress is freaking on me and won&#039;t let me link).  You won&#039;t be surprised to hear that it&#039;s going to be hard to get that experiment right.  Many really interesting experiments in mental state ascription stick to amazingly simple scenarios involving whether someone knows a colored ball is in a certain cup, exactly because we don&#039;t know enough yet to generate an accurate picture of what is going on in more complicated settings.  This is the sort of worry that makes me reluctant to take on Wesley&#039;s challenge to sort out exactly what&#039;s going on with the interaction between the various conditions he is running.  I think the data are pretty confusing because a number of effects are involved, including, but not limited to, effects involving Gricean implicatures, conversational vs. mental state ascriptive uses of mental state terms, accommodation, reading comprehension, attention, variations in attributed evidence, confidence and the possession of outright belief.  I understand the desire to &quot;explain all the results&quot;, but I don&#039;t think any simple single theory is going to do this for the particular set of results we&#039;re looking at here, and we&#039;ll learn more by running experiments that isolate these various factors as much as possible.  In order to do this, it probably helps to armor up somewhat on the existing literature on mental state ascription.

(2) Keith (@39) and Jonathan (@56) are concerned that social psychologists might be talking about something other than the thing we care about as epistemologists.  First, I have to say that social psychologists talk about many things, and that we as epistemologists also care about many things.  We could start with subjective probability, which is perhaps one of the possible meanings of &quot;confidence&quot; in our discussion, and what I take Abe to have more of than Zelda in Keith&#039;s 56.  That is, I think Abe is assigning a higher subjective probability to the proposition that the Magic will not win the conference final than Zelda is. Psychologists (e.g. Phil Tetlock) have shown that a shift to high stakes does tend to curb overconfidence, although there are some doubts about the magnitude and significance of the shift (Cesarini, Sandewall and Johannesson). For what it&#039;s worth I&#039;m not entirely convinced that psychologists would grant that Abe and Zelda would be focused on the same proposition here -- Zelda&#039;s low-stakes condition would make it natural for her to think in rough and qualitative terms &quot;The Magic are really pretty likely not to win&quot;, where Abe&#039;s high-stakes condition will trigger more controlled, numerical thinking about the odds of their victory, and the costs of those odds (I&#039;m thinking about dual process theorists like Evans, Stanovich and Sloman).  It would be natural enough for Abe to require more evidence to achieve evidence-based belief in his more fine-grained target proposition than what Zelda needs for hers.  But worries about whether we&#039;ve got the same proposition naturally attributed here could be set aside: we could write the case up to make it clear that each of these two sports fans is focused just on the categorical proposition &quot;The Magic will not win&quot;.  Whatever we think about knowledge, it does have some psychological attitude component, and we may still have a sense that it&#039;s not going to be the same for these two subjects.  For example, it&#039;s possible to read the case as one in which Zelda has an outright belief that the Magic will not win, and Abe is still trying to make up his  mind about that proposition. Here again the DPT crew can help -- Jonathan St. B. Evans, for example, argues in some detail that we typically (like Zelda) think of the single most plausible outcome, but when given (like Abe) suitable incentives, we construct more detailed mental models of what could happen, and subsequently require more evidence to satisfy ourselves about any particular prospect -- or, philosophers might say -- to achieve plain outright belief in a normatively appropriate evidence-based manner. 

However important subjective confidence might be for knowledge ascription, we can leave room for the idea that the possession or failure to possess a plain outright belief on a given question also matters.  Weatherson has argued that stakes could make a difference to the level of subjective confidence one needs to have in a proposition in order to count as an outright believer; I think there&#039;s something to his line of thought.  I also think that this something is backed up by a little something in social psychology.  It&#039;s not invariably the case that those who continue the hunt for evidence on a question are seen as lacking outright belief in it (one might be after an iteration of knowledge, if one is particularly self-conscious, and there&#039;s some interesting work on self-consciousness and accountability), but the pursuit of further evidence is often taken to indicate that one hasn&#039;t yet achieved the &quot;desired level of confidence&quot; (Rieskamp and Otto) where this level shifts up with a change in stakes.  On another way of looking at it, higher-stakes situations lead to something like a drop in the value of our evidence. Benjamin Newell&#039;s work is especially interesting here.  If anyone other than Jennifer Nagel has been reading it, let me know.  I&#039;m not at all sure I&#039;m getting it right.  But I do think that it looks relevant to a number of issues we are interested in as epistemologists.]]></description>
		<content:encoded><![CDATA[<p>Hi guys,<br />
(1) Josh (@8) and Jonathan (@56) have raised the worry that while raised stakes clearly matter to first-person knowledge ascription, it remains to be seen what the effects might be for how we perceive others.  My sense of the mental state ascription literature is that on all of the major theories there are <b>very</b> close connections between first- and third-person assessments. In simulation theory (e.g. Goldman) we ascribe states of belief and knowledge to another by pretending to be in his position and running a self-assessment which is then projected onto the other. At the far end of the spectrum (e.g. Carruthers), we gauge our own epistemic position by running on ourselves the same mindreading competence we generally apply to others.  Even in fancy positions that do not make either of first- or third-person mindreading straightforwardly derivative of the other (e.g. Apperly), there is enormous overlap in the resources we have for readings ourselves and others.  If stakes matter to my epistemic self-evaluations, they are going to matter to my evaluations of others &#8212; or at least there&#8217;s a heavy burden of argument on anyone who wants to try to show otherwise.  </p>
<p>As for Jonathan&#8217;s specific query about the relationship between mood and confidence in first- and third-person assessments, it is going to matter just how closely we attend to the mood of the person evaluated (close enough to suffer emotional contagion?), and we&#8217;ll need to be careful to offset a number of complicated interactions involving mood, cognition, and mental state ascription (see Benjamin Converse&#8217;s 2008 paper in <i>Emotion</i> on that &#8212; sorry, WordPress is freaking on me and won&#8217;t let me link).  You won&#8217;t be surprised to hear that it&#8217;s going to be hard to get that experiment right.  Many really interesting experiments in mental state ascription stick to amazingly simple scenarios involving whether someone knows a colored ball is in a certain cup, exactly because we don&#8217;t know enough yet to generate an accurate picture of what is going on in more complicated settings.  This is the sort of worry that makes me reluctant to take on Wesley&#8217;s challenge to sort out exactly what&#8217;s going on with the interaction between the various conditions he is running.  I think the data are pretty confusing because a number of effects are involved, including, but not limited to, effects involving Gricean implicatures, conversational vs. mental state ascriptive uses of mental state terms, accommodation, reading comprehension, attention, variations in attributed evidence, confidence and the possession of outright belief.  I understand the desire to &#8220;explain all the results&#8221;, but I don&#8217;t think any simple single theory is going to do this for the particular set of results we&#8217;re looking at here, and we&#8217;ll learn more by running experiments that isolate these various factors as much as possible.  In order to do this, it probably helps to armor up somewhat on the existing literature on mental state ascription.</p>
<p>(2) Keith (@39) and Jonathan (@56) are concerned that social psychologists might be talking about something other than the thing we care about as epistemologists.  First, I have to say that social psychologists talk about many things, and that we as epistemologists also care about many things.  We could start with subjective probability, which is perhaps one of the possible meanings of &#8220;confidence&#8221; in our discussion, and what I take Abe to have more of than Zelda in Keith&#8217;s 56.  That is, I think Abe is assigning a higher subjective probability to the proposition that the Magic will not win the conference final than Zelda is. Psychologists (e.g. Phil Tetlock) have shown that a shift to high stakes does tend to curb overconfidence, although there are some doubts about the magnitude and significance of the shift (Cesarini, Sandewall and Johannesson). For what it&#8217;s worth I&#8217;m not entirely convinced that psychologists would grant that Abe and Zelda would be focused on the same proposition here &#8212; Zelda&#8217;s low-stakes condition would make it natural for her to think in rough and qualitative terms &#8220;The Magic are really pretty likely not to win&#8221;, where Abe&#8217;s high-stakes condition will trigger more controlled, numerical thinking about the odds of their victory, and the costs of those odds (I&#8217;m thinking about dual process theorists like Evans, Stanovich and Sloman).  It would be natural enough for Abe to require more evidence to achieve evidence-based belief in his more fine-grained target proposition than what Zelda needs for hers.  But worries about whether we&#8217;ve got the same proposition naturally attributed here could be set aside: we could write the case up to make it clear that each of these two sports fans is focused just on the categorical proposition &#8220;The Magic will not win&#8221;.  Whatever we think about knowledge, it does have some psychological attitude component, and we may still have a sense that it&#8217;s not going to be the same for these two subjects.  For example, it&#8217;s possible to read the case as one in which Zelda has an outright belief that the Magic will not win, and Abe is still trying to make up his  mind about that proposition. Here again the DPT crew can help &#8212; Jonathan St. B. Evans, for example, argues in some detail that we typically (like Zelda) think of the single most plausible outcome, but when given (like Abe) suitable incentives, we construct more detailed mental models of what could happen, and subsequently require more evidence to satisfy ourselves about any particular prospect &#8212; or, philosophers might say &#8212; to achieve plain outright belief in a normatively appropriate evidence-based manner. </p>
<p>However important subjective confidence might be for knowledge ascription, we can leave room for the idea that the possession or failure to possess a plain outright belief on a given question also matters.  Weatherson has argued that stakes could make a difference to the level of subjective confidence one needs to have in a proposition in order to count as an outright believer; I think there&#8217;s something to his line of thought.  I also think that this something is backed up by a little something in social psychology.  It&#8217;s not invariably the case that those who continue the hunt for evidence on a question are seen as lacking outright belief in it (one might be after an iteration of knowledge, if one is particularly self-conscious, and there&#8217;s some interesting work on self-consciousness and accountability), but the pursuit of further evidence is often taken to indicate that one hasn&#8217;t yet achieved the &#8220;desired level of confidence&#8221; (Rieskamp and Otto) where this level shifts up with a change in stakes.  On another way of looking at it, higher-stakes situations lead to something like a drop in the value of our evidence. Benjamin Newell&#8217;s work is especially interesting here.  If anyone other than Jennifer Nagel has been reading it, let me know.  I&#8217;m not at all sure I&#8217;m getting it right.  But I do think that it looks relevant to a number of issues we are interested in as epistemologists.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Angel Pinillos</title>
		<link>http://certaindoubts.com/new-experiment-on-bank-cases/#comment-12454</link>
		<dc:creator><![CDATA[Angel Pinillos]]></dc:creator>
		<pubDate>Tue, 20 Jul 2010 03:01:56 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1999#comment-12454</guid>
		<description><![CDATA[Hi Wesley, even if the numbers for the knowledge prompt and the belief prompt which you suggest to do come out the same, I don&#039;t see why this would cast doubt on my interpretation of the original results. We could explain this by assuming that subjects accept the very plausible idea that Peter will form the belief when he knows there are no typos and not before (otherwise he would be violating a knowledge norm).  Perhaps another way to address your worry is by adding a sentence in the vignette to the effect that Peter has already formed the belief that there are no typos (even before he checked for typos). I can&#039;t imagine that this would affect the responses. It seems to me that even if Peter believes there are no typos in his paper, he still needs to gather a lot of evidence (for the high stakes case) before he can count as knowing (compared to the low stakes case). I suspect the subjects will feel the same way. It would be very easy to run this sort of probe.]]></description>
		<content:encoded><![CDATA[<p>Hi Wesley, even if the numbers for the knowledge prompt and the belief prompt which you suggest to do come out the same, I don&#8217;t see why this would cast doubt on my interpretation of the original results. We could explain this by assuming that subjects accept the very plausible idea that Peter will form the belief when he knows there are no typos and not before (otherwise he would be violating a knowledge norm).  Perhaps another way to address your worry is by adding a sentence in the vignette to the effect that Peter has already formed the belief that there are no typos (even before he checked for typos). I can&#8217;t imagine that this would affect the responses. It seems to me that even if Peter believes there are no typos in his paper, he still needs to gather a lot of evidence (for the high stakes case) before he can count as knowing (compared to the low stakes case). I suspect the subjects will feel the same way. It would be very easy to run this sort of probe.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Wesley</title>
		<link>http://certaindoubts.com/new-experiment-on-bank-cases/#comment-12453</link>
		<dc:creator><![CDATA[Wesley]]></dc:creator>
		<pubDate>Tue, 20 Jul 2010 00:14:18 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1999#comment-12453</guid>
		<description><![CDATA[Hey Angel, I was thinking maybe I shouldn’t have called my idea about explaining the contrary results of your study predicate blindness.  I didn’t mean to suggest that participants were making some kind of error in reasoning or doing anything wrong in answering the question you asked.  Instead, what I was questioning was your interpretation, whether these data actually speak to the crucial question of people’s mental state attributions particularly having to do with knowledge.

The worry, reminiscent of our earlier discussions of confounding factors in the bank cases, is that you are accidentally manipulating other more traditional factors, like for instance, whether or not the subject in your vignette has the relevant belief.  So it’s totally an empirical question about whether people are doing this in your study, but the thought here is that if participants display the same hi/lo asymmetry when asking the same question you did about knowledge, about belief, then we would have good reason to doubt your inference that considerations of stakes impact the relevant mental state attributions in these discussions about knowledge.  Does that make sense?]]></description>
		<content:encoded><![CDATA[<p>Hey Angel, I was thinking maybe I shouldn’t have called my idea about explaining the contrary results of your study predicate blindness.  I didn’t mean to suggest that participants were making some kind of error in reasoning or doing anything wrong in answering the question you asked.  Instead, what I was questioning was your interpretation, whether these data actually speak to the crucial question of people’s mental state attributions particularly having to do with knowledge.</p>
<p>The worry, reminiscent of our earlier discussions of confounding factors in the bank cases, is that you are accidentally manipulating other more traditional factors, like for instance, whether or not the subject in your vignette has the relevant belief.  So it’s totally an empirical question about whether people are doing this in your study, but the thought here is that if participants display the same hi/lo asymmetry when asking the same question you did about knowledge, about belief, then we would have good reason to doubt your inference that considerations of stakes impact the relevant mental state attributions in these discussions about knowledge.  Does that make sense?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Jason Stanley</title>
		<link>http://certaindoubts.com/new-experiment-on-bank-cases/#comment-12452</link>
		<dc:creator><![CDATA[Jason Stanley]]></dc:creator>
		<pubDate>Mon, 19 Jul 2010 22:36:38 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1999#comment-12452</guid>
		<description><![CDATA[@39 - I can&#039;t answer Keith right now, since I&#039;m in Budapest running a summer school on context-dependence and linguistic interpretation. I just gave a bunch of lectures, and have to prepare for tomorrow. I&#039;ll address this in August. I don&#039;t think this is how the psychologists I&#039;ve read are thinking about confidence, but I&#039;m going to have to go back to look. Hopefully, Jennifer can address this before I can.]]></description>
		<content:encoded><![CDATA[<p>@39 &#8211; I can&#8217;t answer Keith right now, since I&#8217;m in Budapest running a summer school on context-dependence and linguistic interpretation. I just gave a bunch of lectures, and have to prepare for tomorrow. I&#8217;ll address this in August. I don&#8217;t think this is how the psychologists I&#8217;ve read are thinking about confidence, but I&#8217;m going to have to go back to look. Hopefully, Jennifer can address this before I can.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Jason Stanley</title>
		<link>http://certaindoubts.com/new-experiment-on-bank-cases/#comment-12451</link>
		<dc:creator><![CDATA[Jason Stanley]]></dc:creator>
		<pubDate>Mon, 19 Jul 2010 22:32:22 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1999#comment-12451</guid>
		<description><![CDATA[Jonathan,

I should stress that I am a complete novice on the social psychology literature on stakes and confidence - I have read something like three or four articles from the 1980s, which clearly show the effects of stakes on confidence. I don&#039;t think any of us besides Jennifer Nagel knows the work by the scientists well enough to make any sort of judgment about it yet. That&#039;s why I&#039;m crying out for somebody besides Jennifer Nagel to take a look at what the scientists have done. Before we talk about the deficiencies of the scientists, and the superiority of the philosophers, we should study their work carefully (this is what we language people do when we talk about language!). Once we summarize and get a good handle on that literature, we can turn to filling in the questions they don&#039;t address. You guys are the philosophers of psychology - you shouldn&#039;t be leaving it to me to read the scientists&#039; work. I&#039;ve got enough on my plate reading linguists.

On to IRI. I take the basic case for IRI to be from first-person knowledge ascriptions. When you are in a low-stakes situation, you take yourself to know things that you take yourself not to know in a high-stakes situation. As you rightly say Jonathan, this is a clear prediction from the psychology literature on stakes and confidence. 

I also take the basic evidence for contextualism to be from first-person cases. When I am outside the epistemology classroom, I take myself to know I have hands. When I am inside the epistemology classroom, I take myself not to know that I have hands. Why is this? 

The function of the bank cases is to provide a non-philosophically controversial example of this phenomenon - that is, one that occurs outside the epistemology classroom. 

As you say Jonathan, the psychology basically entails that first-person knowledge ascriptions are stakes-sensitive. I&#039;ve got theoretical views that can explain this (namely, it&#039;s because knowledge is stakes-sensitive). These theoretical views predict that certain third-person knowledge attributions are true, but which intuitively seem to us to be false. I&#039;ve got a theoretical explanation for this too (and Hawthorne has another, and Fantl and McGrath yet another). All three books advocating IRI concede that it has trouble with third-person cases, and go out of their way to provide explanations. 

I also show that for each kind of third-person cases IRI has problems with, one can use propositional anaphora to construct a third-person case that the contextualist has problems with (everybody in the literature has seemed to miss this argument). 

The fact that there are some intuitions IRI can&#039;t capture would be a problem if the case for IRI was based on a summary of intuitions. It isn&#039;t. The case for IRI is based on principles linking knowledge and action, and claims about the value of knowledge. IRI is a metaphysical claim about the knowledge relation, and there is no reason to think we have deep insight into the metaphysical determinants of the properties and relations about which we speak.

Of course, if there are the links between knowledge and action that I say there are, then we should see some evidence of that in our behavior. And we do - in high-stakes situations, we are reluctant to act on certain beliefs that we are not reluctant to act on in low stakes cases. That&#039;s uncontroversial. I think it&#039;s linked to the fact that we (now uncontroversially - thanks Jonathan!) take ourselves to know things in low-stakes cases that we don&#039;t take ourselves to know in high-stakes cases. For some reason, the fact that the knowledge relation constituitively involves stakes is less apparent to us when we are thinking about others. Maybe it&#039;s because we care less about them, and we have a hard time putting ourselves into their practical situations.

At any rate, we all agree that there are various third-person cases that IRI gives the wrong predictions on, and contextualism gives the right ones. I&#039;ve argued that there are exactly similar third-person cases, using propositional anaphora, where contextualism gives the same predictions as IRI. So contextualism actually doesn&#039;t give a uniform account of third-person cases. At least IRI gives the same result for both explicit third person knowledge-attributions, and uses of propositional anaphora. That&#039;s my argument that IRI is better off - even the contextualist can&#039;t avoid an error theory about many third-person cases.]]></description>
		<content:encoded><![CDATA[<p>Jonathan,</p>
<p>I should stress that I am a complete novice on the social psychology literature on stakes and confidence &#8211; I have read something like three or four articles from the 1980s, which clearly show the effects of stakes on confidence. I don&#8217;t think any of us besides Jennifer Nagel knows the work by the scientists well enough to make any sort of judgment about it yet. That&#8217;s why I&#8217;m crying out for somebody besides Jennifer Nagel to take a look at what the scientists have done. Before we talk about the deficiencies of the scientists, and the superiority of the philosophers, we should study their work carefully (this is what we language people do when we talk about language!). Once we summarize and get a good handle on that literature, we can turn to filling in the questions they don&#8217;t address. You guys are the philosophers of psychology &#8211; you shouldn&#8217;t be leaving it to me to read the scientists&#8217; work. I&#8217;ve got enough on my plate reading linguists.</p>
<p>On to IRI. I take the basic case for IRI to be from first-person knowledge ascriptions. When you are in a low-stakes situation, you take yourself to know things that you take yourself not to know in a high-stakes situation. As you rightly say Jonathan, this is a clear prediction from the psychology literature on stakes and confidence. </p>
<p>I also take the basic evidence for contextualism to be from first-person cases. When I am outside the epistemology classroom, I take myself to know I have hands. When I am inside the epistemology classroom, I take myself not to know that I have hands. Why is this? </p>
<p>The function of the bank cases is to provide a non-philosophically controversial example of this phenomenon &#8211; that is, one that occurs outside the epistemology classroom. </p>
<p>As you say Jonathan, the psychology basically entails that first-person knowledge ascriptions are stakes-sensitive. I&#8217;ve got theoretical views that can explain this (namely, it&#8217;s because knowledge is stakes-sensitive). These theoretical views predict that certain third-person knowledge attributions are true, but which intuitively seem to us to be false. I&#8217;ve got a theoretical explanation for this too (and Hawthorne has another, and Fantl and McGrath yet another). All three books advocating IRI concede that it has trouble with third-person cases, and go out of their way to provide explanations. </p>
<p>I also show that for each kind of third-person cases IRI has problems with, one can use propositional anaphora to construct a third-person case that the contextualist has problems with (everybody in the literature has seemed to miss this argument). </p>
<p>The fact that there are some intuitions IRI can&#8217;t capture would be a problem if the case for IRI was based on a summary of intuitions. It isn&#8217;t. The case for IRI is based on principles linking knowledge and action, and claims about the value of knowledge. IRI is a metaphysical claim about the knowledge relation, and there is no reason to think we have deep insight into the metaphysical determinants of the properties and relations about which we speak.</p>
<p>Of course, if there are the links between knowledge and action that I say there are, then we should see some evidence of that in our behavior. And we do &#8211; in high-stakes situations, we are reluctant to act on certain beliefs that we are not reluctant to act on in low stakes cases. That&#8217;s uncontroversial. I think it&#8217;s linked to the fact that we (now uncontroversially &#8211; thanks Jonathan!) take ourselves to know things in low-stakes cases that we don&#8217;t take ourselves to know in high-stakes cases. For some reason, the fact that the knowledge relation constituitively involves stakes is less apparent to us when we are thinking about others. Maybe it&#8217;s because we care less about them, and we have a hard time putting ourselves into their practical situations.</p>
<p>At any rate, we all agree that there are various third-person cases that IRI gives the wrong predictions on, and contextualism gives the right ones. I&#8217;ve argued that there are exactly similar third-person cases, using propositional anaphora, where contextualism gives the same predictions as IRI. So contextualism actually doesn&#8217;t give a uniform account of third-person cases. At least IRI gives the same result for both explicit third person knowledge-attributions, and uses of propositional anaphora. That&#8217;s my argument that IRI is better off &#8211; even the contextualist can&#8217;t avoid an error theory about many third-person cases.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: jonathan weinberg</title>
		<link>http://certaindoubts.com/new-experiment-on-bank-cases/#comment-12449</link>
		<dc:creator><![CDATA[jonathan weinberg]]></dc:creator>
		<pubDate>Mon, 19 Jul 2010 18:02:29 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1999#comment-12449</guid>
		<description><![CDATA[Dang it, why do these interesting threads always pop up when I&#039;m travelling and away from reliable internet access???

So much interesting &#038; valuable stuff up there, I&#039;ll just poke at a little bit.

(1) I share JK&#039;s puzzlement at how the social psych literature that JS is appealing to is supposed to have any obvious consequences for either third-personal knowledge attributions, and/or our evaluations of other people&#039;s first-personal knowledge attributions.  It is a fairly clear prediction from that literature that first-personal knowledge attributions should be stakes-sensitive, but I&#039;m just not seeing why we should _expect_ it, on its face, to have the same impact when evaluating others.  It very well might have that impact, but work would have to be done showing that it did so.

For example, we also know from the psychology literature that inducing negative affect will lower subjective confidence.  But we wouldn&#039;t expect (would we?) that simply changing the affective state of a target in an epistemology vignette would lead subjects to change their attributions of knowledge in such a vignette.  It seems to me that the psychology literature thus far is fairly silent on third-person knowledge attributions -- not completely so, but enough that it is simply not in much of a position to inform these sorts of debates about IRI, etc.

(2) I also have a concern very near the one raised by KD, that &quot;confidence&quot; as used in the psychology literature is not at all obviously the same thing as the philosopher&#039;s notion as something like &quot;that which indicates what a person takes their evidence to be&quot;. E.g., it is often glossed in the psych literature as a concept with a clear practical dimension, e.g., when it is operationalized in terms of betting behaviors.  We care about what sort of distinctions, if any, are to be drawn between the practical and the epistemic here, but I don&#039;t think that the psychologists on the whole have been interested in that.

(The above two points underscore one of the reasons that it is essential to have philosophers involved in x-phi: the distinctions and issues that are important to us just aren&#039;t necessarily, or perhaps even often, the same as those that are important to those who make their homes in more traditionally experimental disciplines.)

(3) There seems to be an assumption made by several people on the thread that the disparate attributions of knowledge in the bank cases is a consensus view in the epistemology community.  Is that really so?  My impression has been that it is, while at least a moderately common set of intuitions, not at all a consensus one.   (I, for one, have never had those intuitions!  But I&#039;ve always had nonstandard intuitions about lots of cases, unfortunately.)  Is it really a consensus?  Or at least a consensus that it&#039;s a consensus?]]></description>
		<content:encoded><![CDATA[<p>Dang it, why do these interesting threads always pop up when I&#8217;m travelling and away from reliable internet access???</p>
<p>So much interesting &amp; valuable stuff up there, I&#8217;ll just poke at a little bit.</p>
<p>(1) I share JK&#8217;s puzzlement at how the social psych literature that JS is appealing to is supposed to have any obvious consequences for either third-personal knowledge attributions, and/or our evaluations of other people&#8217;s first-personal knowledge attributions.  It is a fairly clear prediction from that literature that first-personal knowledge attributions should be stakes-sensitive, but I&#8217;m just not seeing why we should _expect_ it, on its face, to have the same impact when evaluating others.  It very well might have that impact, but work would have to be done showing that it did so.</p>
<p>For example, we also know from the psychology literature that inducing negative affect will lower subjective confidence.  But we wouldn&#8217;t expect (would we?) that simply changing the affective state of a target in an epistemology vignette would lead subjects to change their attributions of knowledge in such a vignette.  It seems to me that the psychology literature thus far is fairly silent on third-person knowledge attributions &#8212; not completely so, but enough that it is simply not in much of a position to inform these sorts of debates about IRI, etc.</p>
<p>(2) I also have a concern very near the one raised by KD, that &#8220;confidence&#8221; as used in the psychology literature is not at all obviously the same thing as the philosopher&#8217;s notion as something like &#8220;that which indicates what a person takes their evidence to be&#8221;. E.g., it is often glossed in the psych literature as a concept with a clear practical dimension, e.g., when it is operationalized in terms of betting behaviors.  We care about what sort of distinctions, if any, are to be drawn between the practical and the epistemic here, but I don&#8217;t think that the psychologists on the whole have been interested in that.</p>
<p>(The above two points underscore one of the reasons that it is essential to have philosophers involved in x-phi: the distinctions and issues that are important to us just aren&#8217;t necessarily, or perhaps even often, the same as those that are important to those who make their homes in more traditionally experimental disciplines.)</p>
<p>(3) There seems to be an assumption made by several people on the thread that the disparate attributions of knowledge in the bank cases is a consensus view in the epistemology community.  Is that really so?  My impression has been that it is, while at least a moderately common set of intuitions, not at all a consensus one.   (I, for one, have never had those intuitions!  But I&#8217;ve always had nonstandard intuitions about lots of cases, unfortunately.)  Is it really a consensus?  Or at least a consensus that it&#8217;s a consensus?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Angel Pinillos</title>
		<link>http://certaindoubts.com/new-experiment-on-bank-cases/#comment-12448</link>
		<dc:creator><![CDATA[Angel Pinillos]]></dc:creator>
		<pubDate>Mon, 19 Jul 2010 17:05:55 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1999#comment-12448</guid>
		<description><![CDATA[Hi Jason,

Here&#039;s a question for you based on the comment you made in 52: &quot;I’m all for the project in general [Experimental Philosophy. I’d just like to see more engagement with the methods and literature of psychology.&quot; 
I agree with you here. But I think the same could be said about traditional philosophy. One method of traditional philosophy involves the presenting of thought experiments and the reporting of our natural reactions to these. The difference between this method and those of x-phiers is that the former &quot;experiments&quot; are done in private, by experts and without the usual controls. But I would think that some of the problems we raise for X-phi (the confounds we have been talking about in the bank cases) would just as easily arise when traditional philosophers use thought experiments. So it seems like your criticism of experimental philosophy also applies, perhaps even with more force, to traditional philosophy. Do you think that when philosophers present thought experiments and ask readers to share intuitions, philosophers should have read all the relevant psychology literature about the possible confounds etc? If so, this is an interesting suggestion and would surely involve a radical change to current practice. On the other hand, my characterization of some of what we do as traditional philosophers is perhaps mistaken. Personally, I have found it very difficult to get straight on what is really going on when philosophers use thought experiments to get &quot;intuitive&quot; reactions.]]></description>
		<content:encoded><![CDATA[<p>Hi Jason,</p>
<p>Here&#8217;s a question for you based on the comment you made in 52: &#8220;I’m all for the project in general [Experimental Philosophy. I’d just like to see more engagement with the methods and literature of psychology.&#8221;<br />
I agree with you here. But I think the same could be said about traditional philosophy. One method of traditional philosophy involves the presenting of thought experiments and the reporting of our natural reactions to these. The difference between this method and those of x-phiers is that the former &#8220;experiments&#8221; are done in private, by experts and without the usual controls. But I would think that some of the problems we raise for X-phi (the confounds we have been talking about in the bank cases) would just as easily arise when traditional philosophers use thought experiments. So it seems like your criticism of experimental philosophy also applies, perhaps even with more force, to traditional philosophy. Do you think that when philosophers present thought experiments and ask readers to share intuitions, philosophers should have read all the relevant psychology literature about the possible confounds etc? If so, this is an interesting suggestion and would surely involve a radical change to current practice. On the other hand, my characterization of some of what we do as traditional philosophers is perhaps mistaken. Personally, I have found it very difficult to get straight on what is really going on when philosophers use thought experiments to get &#8220;intuitive&#8221; reactions.</p>
]]></content:encoded>
	</item>
</channel>
</rss>
