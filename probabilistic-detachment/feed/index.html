<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	>
<channel>
	<title>Comments on: Probabilistic Detachment</title>
	<atom:link href="http://certaindoubts.com/probabilistic-detachment/feed/" rel="self" type="application/rss+xml" />
	<link>http://certaindoubts.com/probabilistic-detachment/</link>
	<description>devoted to matters epistemic</description>
	<lastBuildDate>Wed, 10 Apr 2019 16:37:28 +0000</lastBuildDate>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.10</generator>
	<item>
		<title>By: Stephen Fogdall</title>
		<link>http://certaindoubts.com/probabilistic-detachment/#comment-8632</link>
		<dc:creator><![CDATA[Stephen Fogdall]]></dc:creator>
		<pubDate>Sun, 15 Oct 2006 15:42:17 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=601#comment-8632</guid>
		<description><![CDATA[The real puzzle about detachment as I see it is that, if it is legitimate at all, it must include a &quot;bookkeeping&quot; component, so that detached propositions still carry some marker indicating the support they received from their evidence.  Suppose we have two propositions P1 and P2, which we have detached from their probabilistic evidence by some rule of detachment we have decided is acceptable.  Let us say that the probability of P1 relative to its evidence is .75 and the probability of P2 relative to its evidence is .99.  Even though we have detached full beliefs in both P1 and P2 (this is what a rule of detachment is supposed to license), we cannot lose sight of the fact that P2 is more strongly supported by its evidence than P1, because we cannot be as confident employing P1 in subsequent inferences than we can P2.  An adequate account of probabilistic detachment (if there is any) has to explain this.  How do we have full belief in both P1 and P2, and yet remain sensitive to the degree of support these propositions received from their respective evidence?  And how does this sensitivity impact subsequent inferences?]]></description>
		<content:encoded><![CDATA[<p>The real puzzle about detachment as I see it is that, if it is legitimate at all, it must include a &#8220;bookkeeping&#8221; component, so that detached propositions still carry some marker indicating the support they received from their evidence.  Suppose we have two propositions P1 and P2, which we have detached from their probabilistic evidence by some rule of detachment we have decided is acceptable.  Let us say that the probability of P1 relative to its evidence is .75 and the probability of P2 relative to its evidence is .99.  Even though we have detached full beliefs in both P1 and P2 (this is what a rule of detachment is supposed to license), we cannot lose sight of the fact that P2 is more strongly supported by its evidence than P1, because we cannot be as confident employing P1 in subsequent inferences than we can P2.  An adequate account of probabilistic detachment (if there is any) has to explain this.  How do we have full belief in both P1 and P2, and yet remain sensitive to the degree of support these propositions received from their respective evidence?  And how does this sensitivity impact subsequent inferences?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: nick shackel</title>
		<link>http://certaindoubts.com/probabilistic-detachment/#comment-4018</link>
		<dc:creator><![CDATA[nick shackel]]></dc:creator>
		<pubDate>Thu, 12 Oct 2006 15:49:43 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=601#comment-4018</guid>
		<description><![CDATA[Jon, OK, so these seem to be some issues that bear on correct detachment: 1. the relation of knowledge to doxastic hesitancy; 2. the relation of safety to correct detachment; 3. whether an account of knowledge can reasonably be expected to slice circumstances and persons as finely as may be necessary for correct detachment; 4. whether thresholds for knowledge and correct detachment can be distinct.; 5. the distinction between doxastic confidence and mathematical probability.
That’s a lot to talk about! So instead I shall just make some brief remarks. 1. I take it that knowledge implies full belief and full belief rules out certain kinds of doxastic hesitancy; I think I can reason to probably p and then correctly reason to p, thereby coming to fully believe p; so knowledge and correct detachment share in rule out certain hesitancies because of their relation to full belief. 2. Your introduction of the word makes it sounds as if you think safety might be a criterion. 4. Certainly, if the criterion for correct detachment is safety and the probabilistic  threshold for safety is distinct from the threshold for knowledge then my proposal will fall. However, if knowledge also requires safety we could strengthen my buck passing proposal by adding ‘and sufficiently safe for knowledge’  (although at that point, and depending on the exact account of safety, probably safety does all the work, so I pass the buck to safety rather than knowledge).  5.  ‘Probably p’ might express doxastic confidence that is not certainty or doxastic certainty about a mathematical probability. So when thinking about detachment we might be thinking about moving from confidence that is less than full belief to full belief, from certainty in a mathematical probability to confidence, or from certainty in a mathematical probability to full belief. I’ve been uncomfortably aware that my remarks  might be off the point for you since I am thinking mostly about doxastic confidence but you introduced the question perhaps to address mainly the issue of correct detachment for mathematical probability.]]></description>
		<content:encoded><![CDATA[<p>Jon, OK, so these seem to be some issues that bear on correct detachment: 1. the relation of knowledge to doxastic hesitancy; 2. the relation of safety to correct detachment; 3. whether an account of knowledge can reasonably be expected to slice circumstances and persons as finely as may be necessary for correct detachment; 4. whether thresholds for knowledge and correct detachment can be distinct.; 5. the distinction between doxastic confidence and mathematical probability.<br />
That’s a lot to talk about! So instead I shall just make some brief remarks. 1. I take it that knowledge implies full belief and full belief rules out certain kinds of doxastic hesitancy; I think I can reason to probably p and then correctly reason to p, thereby coming to fully believe p; so knowledge and correct detachment share in rule out certain hesitancies because of their relation to full belief. 2. Your introduction of the word makes it sounds as if you think safety might be a criterion. 4. Certainly, if the criterion for correct detachment is safety and the probabilistic  threshold for safety is distinct from the threshold for knowledge then my proposal will fall. However, if knowledge also requires safety we could strengthen my buck passing proposal by adding ‘and sufficiently safe for knowledge’  (although at that point, and depending on the exact account of safety, probably safety does all the work, so I pass the buck to safety rather than knowledge).  5.  ‘Probably p’ might express doxastic confidence that is not certainty or doxastic certainty about a mathematical probability. So when thinking about detachment we might be thinking about moving from confidence that is less than full belief to full belief, from certainty in a mathematical probability to confidence, or from certainty in a mathematical probability to full belief. I’ve been uncomfortably aware that my remarks  might be off the point for you since I am thinking mostly about doxastic confidence but you introduced the question perhaps to address mainly the issue of correct detachment for mathematical probability.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: kvanvig</title>
		<link>http://certaindoubts.com/probabilistic-detachment/#comment-4017</link>
		<dc:creator><![CDATA[kvanvig]]></dc:creator>
		<pubDate>Tue, 10 Oct 2006 19:50:39 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=601#comment-4017</guid>
		<description><![CDATA[Clayton, here&#039;s the reason not to weaken commitment to p in the face of known uncertainty:  I fully believe that full belief doesn&#039;t need certainty.  So it is perfectly fitting to know that p is not certain for me and yet be comfortable with full belief in p.

Of course, one might insist that the meta-belief in question needs to be justified, so that if it isn&#039;t, I don&#039;t have a good reason.  But the content is just the sort you&#039;d expect when asking for a good reason here. It is not a reason that entails the claim in question, but good reasons are often non-entailing.]]></description>
		<content:encoded><![CDATA[<p>Clayton, here&#8217;s the reason not to weaken commitment to p in the face of known uncertainty:  I fully believe that full belief doesn&#8217;t need certainty.  So it is perfectly fitting to know that p is not certain for me and yet be comfortable with full belief in p.</p>
<p>Of course, one might insist that the meta-belief in question needs to be justified, so that if it isn&#8217;t, I don&#8217;t have a good reason.  But the content is just the sort you&#8217;d expect when asking for a good reason here. It is not a reason that entails the claim in question, but good reasons are often non-entailing.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Clayton</title>
		<link>http://certaindoubts.com/probabilistic-detachment/#comment-4016</link>
		<dc:creator><![CDATA[Clayton]]></dc:creator>
		<pubDate>Tue, 10 Oct 2006 15:07:40 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=601#comment-4016</guid>
		<description><![CDATA[Jon wrote:
&lt;i&gt;The inclination to think so is just a holdover from a bad epistemology, one that hankers for certainty before making doxastic commitments.&lt;/i&gt;

That would be surprising if true as I don&#039;t personally believe that certainty in any form is necessary for making doxastic commitments.  It just sounds weird to me and lots of other folk just as &#039;p but I don&#039;t know it&#039; sounds weird and I&#039;m not at all inclined to think that knowledge is required for making doxastic commitments (There&#039;s nothing wrong with holding a Gettiered belief, for example).  All I suggested was that there was something wrong with full belief that p and known uncertainty.  I can&#039;t see any reason to not weaken the commitment wrt to p in the face of known uncertainty.


You might be right that there are some things justifiably believed that you can&#039;t have warrant to assert, but I thought that it is was a pretty standard assumption in the literature on Moore&#039;s Paradox to say that an approach that focuses only only on conversational implicatures is essentially incomplete as it does nothing to address what strikes us as odd with someone&#039;s believing &#039;p but I don&#039;t believe p&#039;.  Maybe there&#039;s something wrong with that assumption or maybe there are perfectly adequate solutions to that paradox that don&#039;t try to account for the clash in terms of defeat or norms of theoretical reason but I&#039;d have to see what they looked like before I was convinced that there wasn&#039;t some cost you&#039;d incur by saying that there&#039;s nothing wrong with believing something like &#039;Custer died at Little Big Horn but there&#039;s a chance he didn&#039;t&#039;.]]></description>
		<content:encoded><![CDATA[<p>Jon wrote:<br />
<i>The inclination to think so is just a holdover from a bad epistemology, one that hankers for certainty before making doxastic commitments.</i></p>
<p>That would be surprising if true as I don&#8217;t personally believe that certainty in any form is necessary for making doxastic commitments.  It just sounds weird to me and lots of other folk just as &#8216;p but I don&#8217;t know it&#8217; sounds weird and I&#8217;m not at all inclined to think that knowledge is required for making doxastic commitments (There&#8217;s nothing wrong with holding a Gettiered belief, for example).  All I suggested was that there was something wrong with full belief that p and known uncertainty.  I can&#8217;t see any reason to not weaken the commitment wrt to p in the face of known uncertainty.</p>
<p>You might be right that there are some things justifiably believed that you can&#8217;t have warrant to assert, but I thought that it is was a pretty standard assumption in the literature on Moore&#8217;s Paradox to say that an approach that focuses only only on conversational implicatures is essentially incomplete as it does nothing to address what strikes us as odd with someone&#8217;s believing &#8216;p but I don&#8217;t believe p&#8217;.  Maybe there&#8217;s something wrong with that assumption or maybe there are perfectly adequate solutions to that paradox that don&#8217;t try to account for the clash in terms of defeat or norms of theoretical reason but I&#8217;d have to see what they looked like before I was convinced that there wasn&#8217;t some cost you&#8217;d incur by saying that there&#8217;s nothing wrong with believing something like &#8216;Custer died at Little Big Horn but there&#8217;s a chance he didn&#8217;t&#8217;.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: kvanvig</title>
		<link>http://certaindoubts.com/probabilistic-detachment/#comment-4015</link>
		<dc:creator><![CDATA[kvanvig]]></dc:creator>
		<pubDate>Tue, 10 Oct 2006 11:19:53 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=601#comment-4015</guid>
		<description><![CDATA[Nick, you say,
&lt;blockquote&gt;Now let’s consider your challenge: it might be that in some epistemic circumstance, C, according to my suggestion Pr(p) = 0.55 will make inferring p correct, and but surely Pr(p) = 0.55 is too low to make the inference correct. OK, but consider. If this is the case then it is only so because our account of knowledge implies that in epistemic circumstance C, it is possible for Pr(p)=0.55 and for p to be known. If that really is the case, if there really are circumstances in which p can have such a low probability and yet be *known*, then my intuition is that inferring p from Pr(p) = 0.55 is not unreasonable, and being not unreasonable is sufficient for being rationally correct. If, on the other hand, this really *isn’t* rationally correct (and perhaps it isn’t), then I’d feel entitled to pass the buck back to the account of knowledge that allows for Pr(p)=0.55 and for p to be known. Surely the error is to be located in that account of knowledge rather than my proposal (isn’t buck passing *great*!).&lt;/blockquote&gt;

Nick, I don&#039;t think there&#039;s any reason to think that the account of knowledge will imply that the doxastically hesitant can&#039;t have knowledge.  From that it doesn&#039;t follow that whenever the threshold of probability that the doxastically hesitant have is achieved, that the inference is safe.  That looks like a mistake.

Now, we could relativize the principle to individuals, and have the theory of knowledge imply for a given person with all their doxastic anxieties and hesitancies a given threshold.  And, as you note above, the account will have to be relativized as well to the given propositional content, since we already know that the threshold for knowledge of lottery propositions is higher than the threshold for knowledge of perceptual propositions, for example.  I wonder if this isn&#039;t too much to expect a knowledge claim to imply.

Your example about the lottery ticket and the newspaper raises an interesting question as well.  Suppose we can&#039;t infer that our ticket is a loser from a probability below n, but we can know that our ticket will lose from reading it even though the probability is below n.  That is, the threshold for knowledge is one number and the threshold for safe probabilistic detachment a different number, presumably higher.  That&#039;s what I meant by suggesting that the account is too weak.  The difference is what information the belief is being based on.]]></description>
		<content:encoded><![CDATA[<p>Nick, you say,</p>
<blockquote><p>Now let’s consider your challenge: it might be that in some epistemic circumstance, C, according to my suggestion Pr(p) = 0.55 will make inferring p correct, and but surely Pr(p) = 0.55 is too low to make the inference correct. OK, but consider. If this is the case then it is only so because our account of knowledge implies that in epistemic circumstance C, it is possible for Pr(p)=0.55 and for p to be known. If that really is the case, if there really are circumstances in which p can have such a low probability and yet be *known*, then my intuition is that inferring p from Pr(p) = 0.55 is not unreasonable, and being not unreasonable is sufficient for being rationally correct. If, on the other hand, this really *isn’t* rationally correct (and perhaps it isn’t), then I’d feel entitled to pass the buck back to the account of knowledge that allows for Pr(p)=0.55 and for p to be known. Surely the error is to be located in that account of knowledge rather than my proposal (isn’t buck passing *great*!).</p></blockquote>
<p>Nick, I don&#8217;t think there&#8217;s any reason to think that the account of knowledge will imply that the doxastically hesitant can&#8217;t have knowledge.  From that it doesn&#8217;t follow that whenever the threshold of probability that the doxastically hesitant have is achieved, that the inference is safe.  That looks like a mistake.</p>
<p>Now, we could relativize the principle to individuals, and have the theory of knowledge imply for a given person with all their doxastic anxieties and hesitancies a given threshold.  And, as you note above, the account will have to be relativized as well to the given propositional content, since we already know that the threshold for knowledge of lottery propositions is higher than the threshold for knowledge of perceptual propositions, for example.  I wonder if this isn&#8217;t too much to expect a knowledge claim to imply.</p>
<p>Your example about the lottery ticket and the newspaper raises an interesting question as well.  Suppose we can&#8217;t infer that our ticket is a loser from a probability below n, but we can know that our ticket will lose from reading it even though the probability is below n.  That is, the threshold for knowledge is one number and the threshold for safe probabilistic detachment a different number, presumably higher.  That&#8217;s what I meant by suggesting that the account is too weak.  The difference is what information the belief is being based on.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: nick shackel</title>
		<link>http://certaindoubts.com/probabilistic-detachment/#comment-4014</link>
		<dc:creator><![CDATA[nick shackel]]></dc:creator>
		<pubDate>Tue, 10 Oct 2006 05:26:16 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=601#comment-4014</guid>
		<description><![CDATA[Jon: well, yes, maybe it is too weak, but here is how I thought it might work. You were interested in the circumstances in which detachment would be acceptable and my suggestion, broadly put, is that we determine those circumstances in terms of their correlation with a probabilistic necessary condition on knowledge: the probability being greater than the minimum necessary for knowledge. However finely we have to slice circumstances for knowledge, just so finely must we slice circumstances for correct inductive inference. So if for knowledge we must distinguish the circumstance of just having a lottery ticket meaning you don&#039;t know you will lose from the circumstance in which you read in the paper the winning number and now you do know you lost (despite the chance of the paper printing an error), so too will we have to distinguish those circumstances for correct inductive inference. So let&#039;s call circumstances distinguished in this way epistemic circumstances. I&#039;m allowed to do this without ending up in circularity precisely because I&#039;m offering a buck-passing account. I can leave the non-circular definition of epistemic circumstances to the theory of knowledge.

It turned out to be a bit tricky to specify that necessary condition, since the natural first thought is Kp implies Pr(p) greater than x, but as James pointed out, that is trivial and we need the notion of greatest lower bound. If I remember rightly, the existence of a greatest lower bound for any set of numbers is implied by the completeness axiom for the real numbers, so we can be sure that any set of probabilities will have a glb.

For precision, then, suppose that for a type of epistemic circumstance C the greatest lower bound on the set of probabilities of propositions known or knowable in all token circumstances of type C is x. If you are in a token circumstance of type C then, if Pr(p) is greater than x then inferring p from probably p is correct.
Now let&#039;s consider your challenge: it might be that in some epistemic circumstance, C,  according to my suggestion Pr(p) = 0.55 will make inferring p correct, and but surely Pr(p) = 0.55 is too low to make the inference correct. OK, but consider. If this is the case then it is only so because our account of knowledge implies that in epistemic circumstance C, it is possible for Pr(p)=0.55 and for p to be known.  If that really is the case, if there really are circumstances in which p can have such a low probability and yet be *known*, then my intuition is that inferring p from Pr(p) = 0.55 is not unreasonable, and being not unreasonable is sufficient for being rationally correct. If, on the other hand, this really *isn&#039;t* rationally correct (and perhaps it isn&#039;t), then I&#039;d feel entitled to pass the buck back to the account of knowledge that allows for Pr(p)=0.55 and for p to be known. Surely the error is to be located  in that account of knowledge rather than my proposal (isn&#039;t buck passing *great*!).]]></description>
		<content:encoded><![CDATA[<p>Jon: well, yes, maybe it is too weak, but here is how I thought it might work. You were interested in the circumstances in which detachment would be acceptable and my suggestion, broadly put, is that we determine those circumstances in terms of their correlation with a probabilistic necessary condition on knowledge: the probability being greater than the minimum necessary for knowledge. However finely we have to slice circumstances for knowledge, just so finely must we slice circumstances for correct inductive inference. So if for knowledge we must distinguish the circumstance of just having a lottery ticket meaning you don&#8217;t know you will lose from the circumstance in which you read in the paper the winning number and now you do know you lost (despite the chance of the paper printing an error), so too will we have to distinguish those circumstances for correct inductive inference. So let&#8217;s call circumstances distinguished in this way epistemic circumstances. I&#8217;m allowed to do this without ending up in circularity precisely because I&#8217;m offering a buck-passing account. I can leave the non-circular definition of epistemic circumstances to the theory of knowledge.</p>
<p>It turned out to be a bit tricky to specify that necessary condition, since the natural first thought is Kp implies Pr(p) greater than x, but as James pointed out, that is trivial and we need the notion of greatest lower bound. If I remember rightly, the existence of a greatest lower bound for any set of numbers is implied by the completeness axiom for the real numbers, so we can be sure that any set of probabilities will have a glb.</p>
<p>For precision, then, suppose that for a type of epistemic circumstance C the greatest lower bound on the set of probabilities of propositions known or knowable in all token circumstances of type C is x. If you are in a token circumstance of type C then, if Pr(p) is greater than x then inferring p from probably p is correct.<br />
Now let&#8217;s consider your challenge: it might be that in some epistemic circumstance, C,  according to my suggestion Pr(p) = 0.55 will make inferring p correct, and but surely Pr(p) = 0.55 is too low to make the inference correct. OK, but consider. If this is the case then it is only so because our account of knowledge implies that in epistemic circumstance C, it is possible for Pr(p)=0.55 and for p to be known.  If that really is the case, if there really are circumstances in which p can have such a low probability and yet be *known*, then my intuition is that inferring p from Pr(p) = 0.55 is not unreasonable, and being not unreasonable is sufficient for being rationally correct. If, on the other hand, this really *isn&#8217;t* rationally correct (and perhaps it isn&#8217;t), then I&#8217;d feel entitled to pass the buck back to the account of knowledge that allows for Pr(p)=0.55 and for p to be known. Surely the error is to be located  in that account of knowledge rather than my proposal (isn&#8217;t buck passing *great*!).</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: kvanvig</title>
		<link>http://certaindoubts.com/probabilistic-detachment/#comment-4013</link>
		<dc:creator><![CDATA[kvanvig]]></dc:creator>
		<pubDate>Mon, 09 Oct 2006 23:02:10 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=601#comment-4013</guid>
		<description><![CDATA[Jeremy, that&#039;s an interesting suggestion.  You worry about it in the last paragraph for just the reasons I would.  I can&#039;t see any reason why an unrestricted detachment rule would work for iterated probability operators.  In particular, where the operator is what I suggested, we&#039;re considering allowing the following:
when it is more probable than not that it is more probable than not that p, we can infer that it is more probable than not that p.

That seems as obviously mistaken as inferring that it is raining when it is more probable than not that it is raining.]]></description>
		<content:encoded><![CDATA[<p>Jeremy, that&#8217;s an interesting suggestion.  You worry about it in the last paragraph for just the reasons I would.  I can&#8217;t see any reason why an unrestricted detachment rule would work for iterated probability operators.  In particular, where the operator is what I suggested, we&#8217;re considering allowing the following:<br />
when it is more probable than not that it is more probable than not that p, we can infer that it is more probable than not that p.</p>
<p>That seems as obviously mistaken as inferring that it is raining when it is more probable than not that it is raining.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: kvanvig</title>
		<link>http://certaindoubts.com/probabilistic-detachment/#comment-4012</link>
		<dc:creator><![CDATA[kvanvig]]></dc:creator>
		<pubDate>Mon, 09 Oct 2006 22:58:08 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=601#comment-4012</guid>
		<description><![CDATA[Clayton, I don&#039;t think the conjunction you cite is absurd.  The inclination to think so is just a holdover from a bad epistemology, one that hankers for certainty before making doxastic commitments.  Thorough-going fallibilists ought to willing to expand their doxastic commitments with the second conjunct in most cases of rational belief.

I also think your principle about Moorean absurdities is false.  What may be true is that Moorean absurdities are not assertible because of conversational implicatures that they carry, but that&#039;s compatible with them being justifiably believed.  There&#039;s an argument in chapter 2 of my knowability book, I think, to the effect that you have to endorse a too-strong closure principle about justification to generate the result that conjunctions of the sort Moore worried about can&#039;t be rationally believed.  (I&#039;m not being careful here about distinctions between rationality and justification, but I don&#039;t think that matters in this context.)]]></description>
		<content:encoded><![CDATA[<p>Clayton, I don&#8217;t think the conjunction you cite is absurd.  The inclination to think so is just a holdover from a bad epistemology, one that hankers for certainty before making doxastic commitments.  Thorough-going fallibilists ought to willing to expand their doxastic commitments with the second conjunct in most cases of rational belief.</p>
<p>I also think your principle about Moorean absurdities is false.  What may be true is that Moorean absurdities are not assertible because of conversational implicatures that they carry, but that&#8217;s compatible with them being justifiably believed.  There&#8217;s an argument in chapter 2 of my knowability book, I think, to the effect that you have to endorse a too-strong closure principle about justification to generate the result that conjunctions of the sort Moore worried about can&#8217;t be rationally believed.  (I&#8217;m not being careful here about distinctions between rationality and justification, but I don&#8217;t think that matters in this context.)</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: kvanvig</title>
		<link>http://certaindoubts.com/probabilistic-detachment/#comment-4011</link>
		<dc:creator><![CDATA[kvanvig]]></dc:creator>
		<pubDate>Mon, 09 Oct 2006 22:46:03 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=601#comment-4011</guid>
		<description><![CDATA[Lots of very nice commentary here.  A couple of quick questions.  First, Nick, why think there is any greatest lower bound for either knowledge of justification?  Maybe we can approach a positive answer here if we relativize to individuals and circumstances, but without that relativization, I think the rule K will be too permissive.  And if we do relativize, I&#039;m still not sure why we should think there is a greatest lower bound implied by knowledge or justification.  I like the buck-passing idea, but I&#039;m not convinced that there is any such implication.  Or, more carefully, if there is such an implication, it will be so weak (allowing probabilities too close to .5) so that detachment is permissible in cases where we shouldn&#039;t want to allow it.  What do you think?]]></description>
		<content:encoded><![CDATA[<p>Lots of very nice commentary here.  A couple of quick questions.  First, Nick, why think there is any greatest lower bound for either knowledge of justification?  Maybe we can approach a positive answer here if we relativize to individuals and circumstances, but without that relativization, I think the rule K will be too permissive.  And if we do relativize, I&#8217;m still not sure why we should think there is a greatest lower bound implied by knowledge or justification.  I like the buck-passing idea, but I&#8217;m not convinced that there is any such implication.  Or, more carefully, if there is such an implication, it will be so weak (allowing probabilities too close to .5) so that detachment is permissible in cases where we shouldn&#8217;t want to allow it.  What do you think?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/probabilistic-detachment/#comment-4010</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Sun, 08 Oct 2006 11:19:06 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=601#comment-4010</guid>
		<description><![CDATA[Another thought which might be useful for slotting into arguments on this topic: not all reasoning is demonstrative reasoning, by which I mean that not all arguments are demonstrative arguments.

There is an argument in Nelkin&#039;s lottery paper against rational acceptance to the effect that you can have all of the pieces on the left-hand side (premises) of your argument, reason correctly, and yet embrace a false conclusion but be in no position to find the mistake in your reasoning or your premises. This is bad, she thinks.

But this is the standard one applies to demonstrative arguments. It is the standard one uses to unpick a failed proof, and is not what one does to evaluate a non-demonstrative, inductive inference.

Instead, in non-demonstrative arguments we should draw a distinction between &lt;i&gt;error&lt;/i&gt; and &lt;i&gt;mistakes&lt;/i&gt;. And statistical methods do just this. The aim is to control error, not eliminate it, and to give you some measure of the epistemic risk you face for accepting falsehoods and rejecting truths. Mistakes, on the other hand, are things you can eliminate. These include botched calculations, a poor fit between the statistical model you&#039;ve selected and your data...whether my epistemic position w.r.t. flying to Denver is like a random draw from a big urn of plane rides, for example. These mistakes are roughly similar to making a invalid inference or working with unsound premises, respectively.

Detachment, I think, is a technical (syntactic) property that is natural to want in a formal representation of systems that behave like what I am describing, which is textbook inferential statistics. The recent trend is to notice that this behaviour doesn&#039;t slot cleanly into standard probabilistic logic, and to (cleverly) argue that basic probabilistic logic does behave a lot like how we seem to talk about our epistemic position, and then draw the conclusion that we must not need detachment.]]></description>
		<content:encoded><![CDATA[<p>Another thought which might be useful for slotting into arguments on this topic: not all reasoning is demonstrative reasoning, by which I mean that not all arguments are demonstrative arguments.</p>
<p>There is an argument in Nelkin&#8217;s lottery paper against rational acceptance to the effect that you can have all of the pieces on the left-hand side (premises) of your argument, reason correctly, and yet embrace a false conclusion but be in no position to find the mistake in your reasoning or your premises. This is bad, she thinks.</p>
<p>But this is the standard one applies to demonstrative arguments. It is the standard one uses to unpick a failed proof, and is not what one does to evaluate a non-demonstrative, inductive inference.</p>
<p>Instead, in non-demonstrative arguments we should draw a distinction between <i>error</i> and <i>mistakes</i>. And statistical methods do just this. The aim is to control error, not eliminate it, and to give you some measure of the epistemic risk you face for accepting falsehoods and rejecting truths. Mistakes, on the other hand, are things you can eliminate. These include botched calculations, a poor fit between the statistical model you&#8217;ve selected and your data&#8230;whether my epistemic position w.r.t. flying to Denver is like a random draw from a big urn of plane rides, for example. These mistakes are roughly similar to making a invalid inference or working with unsound premises, respectively.</p>
<p>Detachment, I think, is a technical (syntactic) property that is natural to want in a formal representation of systems that behave like what I am describing, which is textbook inferential statistics. The recent trend is to notice that this behaviour doesn&#8217;t slot cleanly into standard probabilistic logic, and to (cleverly) argue that basic probabilistic logic does behave a lot like how we seem to talk about our epistemic position, and then draw the conclusion that we must not need detachment.</p>
]]></content:encoded>
	</item>
</channel>
</rss>
