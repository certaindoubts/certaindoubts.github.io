<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	>
<channel>
	<title>Comments on: Knowledge and Rational Action</title>
	<atom:link href="http://certaindoubts.com/knowledge-and-rational-action/feed/" rel="self" type="application/rss+xml" />
	<link>http://certaindoubts.com/knowledge-and-rational-action/</link>
	<description>devoted to matters epistemic</description>
	<lastBuildDate>Wed, 10 Apr 2019 16:37:28 +0000</lastBuildDate>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.10</generator>
	<item>
		<title>By: Dougherty</title>
		<link>http://certaindoubts.com/knowledge-and-rational-action/#comment-11732</link>
		<dc:creator><![CDATA[Dougherty]]></dc:creator>
		<pubDate>Thu, 27 May 2010 13:59:26 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1878#comment-11732</guid>
		<description><![CDATA[Good thoughts Matt,

1. The idea was that if we have knowledge in lottery cases--and I&#039;m in agreement with Hawthorne 2004 that we at least sometimes do--then that causes problems for KAP&#039;s.  Maybe you think they don&#039;t.

2a. I think there&#039;s a huge difference between the kinds of cases, though it&#039;s hard to put.  In the PHIL&#038;CASH case the properties are connected in ways that we are very familiar with in everyday life, that we think a lot about explicitly.  We know how PHIL and cash come together, PHIL goes to school, gets a job, gets a little slip in the mail, etc.  

But with K&#038;A it&#039;s far more subtle, it&#039;s part of how we think and talk and we don&#039;t really think and talk much about how we think and talk unless we&#039;re weirdos (as we are!).  

So the move from K&#038;A, K&#038;A, K&#038;A, K&#038;A... to For all x, if Kx then Ax without even noticing it.  It&#039;s part and parcel of my theory that K is a heuristic concept, a tool for not having to think of the details.  

2b. To me, the challenge is similar to that arising from the kind of linguistic evidence Unger gives for K entails C(ertainty) (Jason has a very detailed discussion of this).  I think K often is a stand-in for C-e where e (sorry, don&#039;t know how to make an epsilon) is negligible.  So instances of utterances of the form &quot;K &#038; ~C&quot; sound just as weird as &quot;K &#038; ~A&quot; or perhaps even weirder.  Yet neither of us fallibilists think K entails C.  

Here&#039;s a little chestnut on that score sent to me by my colleague Todd Burus.  It&#039;s from an NPR story on the treacherous world of global climate change reporting.

&quot;We miscommunicate all the time,&quot; Matson says.
&quot;For example, we use the term &#039;uncertainty&#039; all the time in science; it represents a quantitative statement of how well we know something. But think of what uncertainty means for most people -- it means we don&#039;t know.&quot;&quot;

Why is that?  The skeptic has a simple explanation.  I&#039;m not worried too much about it though because I have the same kind of error theory: people are constantly treating knowledge as certain, it&#039;s part of the epistemic adaptive toolbox.  C is a stand-in for C-e and K and C are constantly conjoined, K&#038;C, K&#038;C, K&#038;C, K&#038;C...so the mind just goes along with for all x, if Kx, then Cx.  The folk don&#039;t think much about this, it&#039;s a habit of thought and speech.  But, thankfully, teach someone a bit of fallibilist epistemology and we realize the unconscious mind had over-generalized.  Praise be the reflective mind!  :-)

3. I think &quot;reasonable doubt&quot; is ambiguous.  The speech you give is expressive of doubt and is &quot;reasonable&quot; in the sense that you wouldn&#039;t have to be dysfunctional in order to fixate on that doubt a bit in high stakes.  However, I don&#039;t think that kind of psychological doubt has epistemic effects (well, it might remove belief, but it doesn&#039;t, on my view, affect strength of epistemic position directly).  

I think the standard Rich and Earl have in mind is that for there  to be a reasonable doubt concerning p is for there to be some proposition d which if true would substantively disconfirm p and for which I which I have a good reason to believe.  I don&#039;t think there&#039;s any good reason to believe the bank won&#039;t be open.  I just think that&#039;s being mildly paranoid.

4. Right, I&#039;m talking about reasons people possess, which is really tricky to talk about, like the issue of whether &quot;A has highest EU&quot; is a reason or a statement about reasons (I take the latter position to avoid double counting).  

There are two ways to go here that I&#039;ve been going back and forth between.  One, which you won&#039;t like, is that the item you mention is a reason I have simply in virtue of my believing it.  I tend to think that the normative status of the belief is quite separate from the normative status of the decision that issues from it.

Another way to go is to say that I do have to accept it at some level based--conscious or unconsciously--based on some kind of evidnece, but I don&#039;t have to know it.  This is the Jeffreyan route.  It&#039;s just probabilities &quot;all the way down.&quot;  I think we often do know it, but I don&#039;t see why we&#039;d need to.  This raises a tricky issue of higher-order probabilities, and I don&#039;t have a solution to that problem, but I think that problem has a life of it&#039;s own.  

As far as I can tell the issue is exactly parallel with the probability judgment component of decision theory.  I judge Pr(O/A) = r.  Do I *know* that?  I don&#039;t know.  I know I&#039;m not certain of it.  The latter is all it takes to raise the nasty higher-order probability question.]]></description>
		<content:encoded><![CDATA[<p>Good thoughts Matt,</p>
<p>1. The idea was that if we have knowledge in lottery cases&#8211;and I&#8217;m in agreement with Hawthorne 2004 that we at least sometimes do&#8211;then that causes problems for KAP&#8217;s.  Maybe you think they don&#8217;t.</p>
<p>2a. I think there&#8217;s a huge difference between the kinds of cases, though it&#8217;s hard to put.  In the PHIL&amp;CASH case the properties are connected in ways that we are very familiar with in everyday life, that we think a lot about explicitly.  We know how PHIL and cash come together, PHIL goes to school, gets a job, gets a little slip in the mail, etc.  </p>
<p>But with K&amp;A it&#8217;s far more subtle, it&#8217;s part of how we think and talk and we don&#8217;t really think and talk much about how we think and talk unless we&#8217;re weirdos (as we are!).  </p>
<p>So the move from K&amp;A, K&amp;A, K&amp;A, K&amp;A&#8230; to For all x, if Kx then Ax without even noticing it.  It&#8217;s part and parcel of my theory that K is a heuristic concept, a tool for not having to think of the details.  </p>
<p>2b. To me, the challenge is similar to that arising from the kind of linguistic evidence Unger gives for K entails C(ertainty) (Jason has a very detailed discussion of this).  I think K often is a stand-in for C-e where e (sorry, don&#8217;t know how to make an epsilon) is negligible.  So instances of utterances of the form &#8220;K &amp; ~C&#8221; sound just as weird as &#8220;K &amp; ~A&#8221; or perhaps even weirder.  Yet neither of us fallibilists think K entails C.  </p>
<p>Here&#8217;s a little chestnut on that score sent to me by my colleague Todd Burus.  It&#8217;s from an NPR story on the treacherous world of global climate change reporting.</p>
<p>&#8220;We miscommunicate all the time,&#8221; Matson says.<br />
&#8220;For example, we use the term &#8216;uncertainty&#8217; all the time in science; it represents a quantitative statement of how well we know something. But think of what uncertainty means for most people &#8212; it means we don&#8217;t know.&#8221;&#8221;</p>
<p>Why is that?  The skeptic has a simple explanation.  I&#8217;m not worried too much about it though because I have the same kind of error theory: people are constantly treating knowledge as certain, it&#8217;s part of the epistemic adaptive toolbox.  C is a stand-in for C-e and K and C are constantly conjoined, K&amp;C, K&amp;C, K&amp;C, K&amp;C&#8230;so the mind just goes along with for all x, if Kx, then Cx.  The folk don&#8217;t think much about this, it&#8217;s a habit of thought and speech.  But, thankfully, teach someone a bit of fallibilist epistemology and we realize the unconscious mind had over-generalized.  Praise be the reflective mind!  🙂</p>
<p>3. I think &#8220;reasonable doubt&#8221; is ambiguous.  The speech you give is expressive of doubt and is &#8220;reasonable&#8221; in the sense that you wouldn&#8217;t have to be dysfunctional in order to fixate on that doubt a bit in high stakes.  However, I don&#8217;t think that kind of psychological doubt has epistemic effects (well, it might remove belief, but it doesn&#8217;t, on my view, affect strength of epistemic position directly).  </p>
<p>I think the standard Rich and Earl have in mind is that for there  to be a reasonable doubt concerning p is for there to be some proposition d which if true would substantively disconfirm p and for which I which I have a good reason to believe.  I don&#8217;t think there&#8217;s any good reason to believe the bank won&#8217;t be open.  I just think that&#8217;s being mildly paranoid.</p>
<p>4. Right, I&#8217;m talking about reasons people possess, which is really tricky to talk about, like the issue of whether &#8220;A has highest EU&#8221; is a reason or a statement about reasons (I take the latter position to avoid double counting).  </p>
<p>There are two ways to go here that I&#8217;ve been going back and forth between.  One, which you won&#8217;t like, is that the item you mention is a reason I have simply in virtue of my believing it.  I tend to think that the normative status of the belief is quite separate from the normative status of the decision that issues from it.</p>
<p>Another way to go is to say that I do have to accept it at some level based&#8211;conscious or unconsciously&#8211;based on some kind of evidnece, but I don&#8217;t have to know it.  This is the Jeffreyan route.  It&#8217;s just probabilities &#8220;all the way down.&#8221;  I think we often do know it, but I don&#8217;t see why we&#8217;d need to.  This raises a tricky issue of higher-order probabilities, and I don&#8217;t have a solution to that problem, but I think that problem has a life of it&#8217;s own.  </p>
<p>As far as I can tell the issue is exactly parallel with the probability judgment component of decision theory.  I judge Pr(O/A) = r.  Do I *know* that?  I don&#8217;t know.  I know I&#8217;m not certain of it.  The latter is all it takes to raise the nasty higher-order probability question.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Matt McGrath</title>
		<link>http://certaindoubts.com/knowledge-and-rational-action/#comment-11730</link>
		<dc:creator><![CDATA[Matt McGrath]]></dc:creator>
		<pubDate>Thu, 27 May 2010 12:55:56 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1878#comment-11730</guid>
		<description><![CDATA[Just a few final thoughts, and then I&#039;m done.

You write:

&quot;1a. “Saying “you know that’s not going to happen” is a common way of criticizing someone for worrying about a bad possibility.“

It’s also a common way of criticizing someone for hoping in a good possibility. “You know you’re not going to win the lottery, so get back to work.” So, while I admit that there’s some linguistic evidence for knowledge action principles (KAP’s), I think there’s also evidence against them, just as there are in the case of lottery knowledge.&quot;

I don&#039;t get this argument.  Whether &quot;you know you&#039;re not going to win the lottery&quot; is true or not, it is being used to criticize action (or urge action).  Why is this evidence against a knowledge/action connection?  

Second, I&#039;m worried that your &quot;generalization&quot; move is going to overgeneralize.  I see lots of philosophers and they are all non-billionaries.  But I don&#039;t find any clash in &quot;She&#039;s a philosopher but also a billionaire.&quot;  Surprising, yes; clash, no.  The conditional &quot;If X is a philosopher, then X is a non-billionaire&quot; doesn&#039;t seem necessarily true.  And the reasoning:  &quot;So and so is a billionaire, and so is not philosopher&quot; is not exactly powerful.

You suggest that evidence beyond a reasonable doubt will seem enough for actionability.  Perhaps.  But at the same time, consider what the high stakes bank person, who is worrying whether it&#039;s open the next day, will say.  She&#039;ll say things like &quot;well, hmm, it might be closed tomorrow; and I can&#039;t just take for granted that it&#039;ll be open tomorrow.&quot; It isn&#039;t clear to me that this person doesn&#039;t have doubts, and that the doubts aren&#039;t reasonable.  In other words, I don&#039;t think it&#039;s clear that in Bank Case B the speaker has justification beyond a reasonable doubt:  he seems to doubt and seems to do so reasonably. You&#039;ll need to argue that this is a mistake, just as is the case for the intuition that the Bank Case B guy doesn&#039;t know.

On reasons.  People usually distinguish between there being reasons for a person to PHI and a person having or possessing a reason to PHI.  I think Feldman makes this distinction (there being evidence out there and one&#039;s having or possessing it).  When Jeremy and I talk of reasons in the book, we had in mind having or possessing reasons.  It&#039;s not enough to have P as a reason to PHI that P is true.  It looks like one needs to know, or at least have some justification for P for it to be a reason you have.  And then the question of how much arises. This question arises just as much if the reason is some fact about the world as if it is something like this:  &quot;Outcome O would be good to degree d and the conditional probability of getting O given I do A is r&quot;.]]></description>
		<content:encoded><![CDATA[<p>Just a few final thoughts, and then I&#8217;m done.</p>
<p>You write:</p>
<p>&#8220;1a. “Saying “you know that’s not going to happen” is a common way of criticizing someone for worrying about a bad possibility.“</p>
<p>It’s also a common way of criticizing someone for hoping in a good possibility. “You know you’re not going to win the lottery, so get back to work.” So, while I admit that there’s some linguistic evidence for knowledge action principles (KAP’s), I think there’s also evidence against them, just as there are in the case of lottery knowledge.&#8221;</p>
<p>I don&#8217;t get this argument.  Whether &#8220;you know you&#8217;re not going to win the lottery&#8221; is true or not, it is being used to criticize action (or urge action).  Why is this evidence against a knowledge/action connection?  </p>
<p>Second, I&#8217;m worried that your &#8220;generalization&#8221; move is going to overgeneralize.  I see lots of philosophers and they are all non-billionaries.  But I don&#8217;t find any clash in &#8220;She&#8217;s a philosopher but also a billionaire.&#8221;  Surprising, yes; clash, no.  The conditional &#8220;If X is a philosopher, then X is a non-billionaire&#8221; doesn&#8217;t seem necessarily true.  And the reasoning:  &#8220;So and so is a billionaire, and so is not philosopher&#8221; is not exactly powerful.</p>
<p>You suggest that evidence beyond a reasonable doubt will seem enough for actionability.  Perhaps.  But at the same time, consider what the high stakes bank person, who is worrying whether it&#8217;s open the next day, will say.  She&#8217;ll say things like &#8220;well, hmm, it might be closed tomorrow; and I can&#8217;t just take for granted that it&#8217;ll be open tomorrow.&#8221; It isn&#8217;t clear to me that this person doesn&#8217;t have doubts, and that the doubts aren&#8217;t reasonable.  In other words, I don&#8217;t think it&#8217;s clear that in Bank Case B the speaker has justification beyond a reasonable doubt:  he seems to doubt and seems to do so reasonably. You&#8217;ll need to argue that this is a mistake, just as is the case for the intuition that the Bank Case B guy doesn&#8217;t know.</p>
<p>On reasons.  People usually distinguish between there being reasons for a person to PHI and a person having or possessing a reason to PHI.  I think Feldman makes this distinction (there being evidence out there and one&#8217;s having or possessing it).  When Jeremy and I talk of reasons in the book, we had in mind having or possessing reasons.  It&#8217;s not enough to have P as a reason to PHI that P is true.  It looks like one needs to know, or at least have some justification for P for it to be a reason you have.  And then the question of how much arises. This question arises just as much if the reason is some fact about the world as if it is something like this:  &#8220;Outcome O would be good to degree d and the conditional probability of getting O given I do A is r&#8221;.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Dougherty</title>
		<link>http://certaindoubts.com/knowledge-and-rational-action/#comment-11712</link>
		<dc:creator><![CDATA[Dougherty]]></dc:creator>
		<pubDate>Thu, 27 May 2010 02:41:25 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1878#comment-11712</guid>
		<description><![CDATA[Jonathan, I am with you that to defuse SAC is to defuse Bank Case B.  Yet I&#039;ve been challenged to give an account of reasons talk&#039;s connection with decision theory, and I want to meet that challenge for a number of reasons.  

One, is that it seems like a cool thing to do.  Another is that Fantl and McGrath do it, so I ought to do it too.  Another is that one of my mentors has a theory that suggests to me a very natural way to do it.  Another is that another mentor of mine is the one who challenged me, and I want to attempt to do so for strictly personal reasons as well.  

In fact, the challenge I&#039;m attempting to address here came at the Pragmatic Encroachment Workshop (https://sites.google.com/site/orangebeachusa/) where Jenefer Nagel was as well.  I do indeed think she&#039;s done a great job of marshaling some cog-sci stuff.  But I&#039;m more interested in the epistemic status stuff than the doxastic picture.

That is, it is not enough for me to explain why people would fail to ascribe knowledge in high stakes (via lack of confidence, lack of closure do to source monitoring, or for any other reason).  What I want to know is whether there are cases where one&#039;s *strength of epistemic position* is sufficient for knowledge in one case, but insufficient in another, where the only thing that changes are practical interests.  

Matt thinks type-B Bank Cases provide some evidence, but it&#039;s the argument from reasons in Chapter 3 that&#039;s the real meat and potatoes.  

That kind of argument caries *much* more force than the ordinary language arguments Jason uses (though of course, as in the case of Austin there is both a laudable attention to detail and an impressive command of linguistic data).  

I&#039;m a grandson of Chisholm, so I&#039;m accustomed to the need for paraphrase.  It never would have occurred to me to think of ordinary language as anything but a blunt instrument.  Carnap had the right idea and Maher&#039;s explication of explication is really good.  Expected utility analysis explicates the ordinary practice of practical deliberation, confirmation theory explicates the ordinary notion of theoretical deliberation.]]></description>
		<content:encoded><![CDATA[<p>Jonathan, I am with you that to defuse SAC is to defuse Bank Case B.  Yet I&#8217;ve been challenged to give an account of reasons talk&#8217;s connection with decision theory, and I want to meet that challenge for a number of reasons.  </p>
<p>One, is that it seems like a cool thing to do.  Another is that Fantl and McGrath do it, so I ought to do it too.  Another is that one of my mentors has a theory that suggests to me a very natural way to do it.  Another is that another mentor of mine is the one who challenged me, and I want to attempt to do so for strictly personal reasons as well.  </p>
<p>In fact, the challenge I&#8217;m attempting to address here came at the Pragmatic Encroachment Workshop (<a href="https://sites.google.com/site/orangebeachusa/" rel="nofollow">https://sites.google.com/site/orangebeachusa/</a>) where Jenefer Nagel was as well.  I do indeed think she&#8217;s done a great job of marshaling some cog-sci stuff.  But I&#8217;m more interested in the epistemic status stuff than the doxastic picture.</p>
<p>That is, it is not enough for me to explain why people would fail to ascribe knowledge in high stakes (via lack of confidence, lack of closure do to source monitoring, or for any other reason).  What I want to know is whether there are cases where one&#8217;s *strength of epistemic position* is sufficient for knowledge in one case, but insufficient in another, where the only thing that changes are practical interests.  </p>
<p>Matt thinks type-B Bank Cases provide some evidence, but it&#8217;s the argument from reasons in Chapter 3 that&#8217;s the real meat and potatoes.  </p>
<p>That kind of argument caries *much* more force than the ordinary language arguments Jason uses (though of course, as in the case of Austin there is both a laudable attention to detail and an impressive command of linguistic data).  </p>
<p>I&#8217;m a grandson of Chisholm, so I&#8217;m accustomed to the need for paraphrase.  It never would have occurred to me to think of ordinary language as anything but a blunt instrument.  Carnap had the right idea and Maher&#8217;s explication of explication is really good.  Expected utility analysis explicates the ordinary practice of practical deliberation, confirmation theory explicates the ordinary notion of theoretical deliberation.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: jonathan weinberg</title>
		<link>http://certaindoubts.com/knowledge-and-rational-action/#comment-11707</link>
		<dc:creator><![CDATA[jonathan weinberg]]></dc:creator>
		<pubDate>Wed, 26 May 2010 23:10:58 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1878#comment-11707</guid>
		<description><![CDATA[Sorry, I&#039;m missing a step, I think.  Why can&#039;t the same explanation handle Bank Case B and its ilk?  (At a minimum, I don&#039;t see what the &quot;expectations&quot; move will help with, that the tacitly-and-illicitly-endorsed-universal-generalization doesn&#039;t take care of already.)  The would-be knower in the case ends up wanting to endorse the plan of action of going to the bank, even though they took themselves to know that it would be open; because of the illicit universal generalization, they think (mistakenly) that this means they need to yield on the knowledge claim.  And it sounds good to the reader when the reader has the same tacit universal generalization operative as they follow along. 

Putting it differently: the would-be knower in the case only has to yield their knowledge because they don&#039;t want to end up in a situation where they seem committed to a SAC.  So any account that defuses SACs, thereby defuses Bank Case Bs, too.  No?

Btw, I really like Jenifer Nagel&#039;s take on these issues, too, if you&#039;re not already familiar with it.]]></description>
		<content:encoded><![CDATA[<p>Sorry, I&#8217;m missing a step, I think.  Why can&#8217;t the same explanation handle Bank Case B and its ilk?  (At a minimum, I don&#8217;t see what the &#8220;expectations&#8221; move will help with, that the tacitly-and-illicitly-endorsed-universal-generalization doesn&#8217;t take care of already.)  The would-be knower in the case ends up wanting to endorse the plan of action of going to the bank, even though they took themselves to know that it would be open; because of the illicit universal generalization, they think (mistakenly) that this means they need to yield on the knowledge claim.  And it sounds good to the reader when the reader has the same tacit universal generalization operative as they follow along. </p>
<p>Putting it differently: the would-be knower in the case only has to yield their knowledge because they don&#8217;t want to end up in a situation where they seem committed to a SAC.  So any account that defuses SACs, thereby defuses Bank Case Bs, too.  No?</p>
<p>Btw, I really like Jenifer Nagel&#8217;s take on these issues, too, if you&#8217;re not already familiar with it.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Dougherty</title>
		<link>http://certaindoubts.com/knowledge-and-rational-action/#comment-11703</link>
		<dc:creator><![CDATA[Dougherty]]></dc:creator>
		<pubDate>Wed, 26 May 2010 20:49:21 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1878#comment-11703</guid>
		<description><![CDATA[Jonathan, I do think that&#039;s a sufficient explanation of why the SAC&#039;s sound bad.  I hope you cog sci folks will supply me with some nice empirical confirmation!

The reason I go on to tell the story about reasons is to explain a separate but related phenomenon: why Bank Case B reasoning sounds good.

Matt has pressed me really hard to do that in a more theoretical way.  I think the expectation stuff works here too, it&#039;s just that I&#039;ve been challenged repeatedly to give an alternative account of the relationship between reasons talk and EU talk.  

Since Jeremy and Matt have discussed how this looks on their view at length, I figured the least I could do was sketch my view!  :-)]]></description>
		<content:encoded><![CDATA[<p>Jonathan, I do think that&#8217;s a sufficient explanation of why the SAC&#8217;s sound bad.  I hope you cog sci folks will supply me with some nice empirical confirmation!</p>
<p>The reason I go on to tell the story about reasons is to explain a separate but related phenomenon: why Bank Case B reasoning sounds good.</p>
<p>Matt has pressed me really hard to do that in a more theoretical way.  I think the expectation stuff works here too, it&#8217;s just that I&#8217;ve been challenged repeatedly to give an alternative account of the relationship between reasons talk and EU talk.  </p>
<p>Since Jeremy and Matt have discussed how this looks on their view at length, I figured the least I could do was sketch my view!  🙂</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: jonathan weinberg</title>
		<link>http://certaindoubts.com/knowledge-and-rational-action/#comment-11700</link>
		<dc:creator><![CDATA[jonathan weinberg]]></dc:creator>
		<pubDate>Wed, 26 May 2010 18:42:37 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1878#comment-11700</guid>
		<description><![CDATA[Trent, I don&#039;t quite understand why you&#039;re telling the story in the way you are, with the stuff about &quot;expectations&quot;.  You&#039;ve already put in play the line that people have made a hasty generalization, probably tacitly, to a universally-quantified principle. Why isn&#039;t that, by itself, basically enough to accomplish the TASK from the main post?  The SACs sound bad, because people have a plausible-but-false belief that they are false, based on that principle?]]></description>
		<content:encoded><![CDATA[<p>Trent, I don&#8217;t quite understand why you&#8217;re telling the story in the way you are, with the stuff about &#8220;expectations&#8221;.  You&#8217;ve already put in play the line that people have made a hasty generalization, probably tacitly, to a universally-quantified principle. Why isn&#8217;t that, by itself, basically enough to accomplish the TASK from the main post?  The SACs sound bad, because people have a plausible-but-false belief that they are false, based on that principle?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Dougherty</title>
		<link>http://certaindoubts.com/knowledge-and-rational-action/#comment-11633</link>
		<dc:creator><![CDATA[Dougherty]]></dc:creator>
		<pubDate>Mon, 24 May 2010 19:36:39 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1878#comment-11633</guid>
		<description><![CDATA[Mssr. Benton, (Sorry, two Matts!  :-))

1. What kinds of cases?  Who’s the alleged conflater?

2. I tend to think we can control for the non-epistemic readings of the modals, so it seems false to me that “if you keep the deontic notion epistemic across (SAC), it tends to sound bad.“

I think I can screen off the professional obligation without having to do this, but you can just focus on some other person, a parent, say, and they’ll give the same verdict.  Worried about parental obligations (whatever those are)?  Then make it a robot or something.  Maybe that will help.

Plus, there’s bad and there’s bad.  There’s prima facie bad and then there’s bad even upon reflection.  My point has been that if we reflect on the peculiar kinds of cases that constitute the clearest kinds of counter-examples, we can see why they seem weird.  

That we’ll get such cases seems guaranteed by a set threshold of evidence for knowing and a variable threshold of evidence for acting (since the EU can remain flat as costs rise and fall, probability of success required to act rises and falls too).  You just have to plug in the right numbers and you get trouble.  I got a paper that really laid this out rejected because by the time the reviewer got it Cohen’s PPR symposium piece had already come out.  

3. Back to conflation.  The “have to” in the SAC is simple practical rationality.  I take it that two things in my Specialist and Ubertest are clear: 1. The Doctor knows she doesn’t have the desease, and 2. Not running the test has neg EU.  The numbers just guarantee this unless you take a radical view on lotteries which I think will land you in utter skepticism or utterly disassociate epistemic reasons from probability (we can make the Dr’s knowledge causal and then translate it to a probability for the EU calc).

SACF also is intended to be read not morally, but simply in terms of practical rationality.  If someone wants to make a moral theory identical with that, good luck (I’d kind of like to myself) but I don’t think that’s going to make a difference here. 

4. Thanks for the Douven reference!  I’ll check that out.  So far I’m not convience by the problem, since, in addition to being really surpised by an utterance in a context, the contact also provides *other* expectations.  This leads to a kind of harmless contrastivism.  When someone says a Moorean thing, we have a particular type of expectation-thwarting: “Why would someone say *that* and then *that*?!”  When someone says that other thing (which, by the way, is now my Facebook status), we have a much more general kind of surprise, one which might best be charactarzed as WTF!]]></description>
		<content:encoded><![CDATA[<p>Mssr. Benton, (Sorry, two Matts!  :-))</p>
<p>1. What kinds of cases?  Who’s the alleged conflater?</p>
<p>2. I tend to think we can control for the non-epistemic readings of the modals, so it seems false to me that “if you keep the deontic notion epistemic across (SAC), it tends to sound bad.“</p>
<p>I think I can screen off the professional obligation without having to do this, but you can just focus on some other person, a parent, say, and they’ll give the same verdict.  Worried about parental obligations (whatever those are)?  Then make it a robot or something.  Maybe that will help.</p>
<p>Plus, there’s bad and there’s bad.  There’s prima facie bad and then there’s bad even upon reflection.  My point has been that if we reflect on the peculiar kinds of cases that constitute the clearest kinds of counter-examples, we can see why they seem weird.  </p>
<p>That we’ll get such cases seems guaranteed by a set threshold of evidence for knowing and a variable threshold of evidence for acting (since the EU can remain flat as costs rise and fall, probability of success required to act rises and falls too).  You just have to plug in the right numbers and you get trouble.  I got a paper that really laid this out rejected because by the time the reviewer got it Cohen’s PPR symposium piece had already come out.  </p>
<p>3. Back to conflation.  The “have to” in the SAC is simple practical rationality.  I take it that two things in my Specialist and Ubertest are clear: 1. The Doctor knows she doesn’t have the desease, and 2. Not running the test has neg EU.  The numbers just guarantee this unless you take a radical view on lotteries which I think will land you in utter skepticism or utterly disassociate epistemic reasons from probability (we can make the Dr’s knowledge causal and then translate it to a probability for the EU calc).</p>
<p>SACF also is intended to be read not morally, but simply in terms of practical rationality.  If someone wants to make a moral theory identical with that, good luck (I’d kind of like to myself) but I don’t think that’s going to make a difference here. </p>
<p>4. Thanks for the Douven reference!  I’ll check that out.  So far I’m not convience by the problem, since, in addition to being really surpised by an utterance in a context, the contact also provides *other* expectations.  This leads to a kind of harmless contrastivism.  When someone says a Moorean thing, we have a particular type of expectation-thwarting: “Why would someone say *that* and then *that*?!”  When someone says that other thing (which, by the way, is now my Facebook status), we have a much more general kind of surprise, one which might best be charactarzed as WTF!</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Dougherty</title>
		<link>http://certaindoubts.com/knowledge-and-rational-action/#comment-11632</link>
		<dc:creator><![CDATA[Dougherty]]></dc:creator>
		<pubDate>Mon, 24 May 2010 19:15:27 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1878#comment-11632</guid>
		<description><![CDATA[Matt, just got back from the Plantinga retirement celebration and have had a chance to read your comments.

1a. “Saying “you know that’s not going to happen” is a common way of criticizing someone for worrying about a bad possibility.“

It’s also a common way of criticizing someone for hoping in a good possibility.  “You know you’re not going to win the lottery, so get back to work.”  So, while I admit that there’s some linguistic evidence for knowledge action principles (KAP’s), I think there’s also evidence against them, just as there are in the case of lottery knowledge.  That’s one reason I discount that type of evidence quite a bit.  But we seem to be in agreement (in conversation and the closing lines of your book) that theory can triumph over cases.  We seem to both be in search of the best overall theory, and that sometimes means sacrificing intuitions.  My main reason for endorsing this is the ambiguity of such evidence.

1b  In particular, I think all those examples you give are great examples of the ambiguity phenomenon.

i. “we should “know that oxygen is flowing, even if the bag doesn’t inflate; so there is no need to worry.“  I.e. we should *realize* or *accept* or *be aware of the fact that*, etc.  

ii. “ “I just didn’t know that it would work out ok at the time.”“  I.e. I couldn’t be *sure*.

iii. “Stop the LHC until we know it is safe!”“  I.e. know for sure.

My point, in part, is that these statements could support a knowledge-certainty principle as well that I don’t think either of us would accept as fallibilists.  Though I like a lot about Austin, in a way I see him as doing exactly the wrong thing.  I love the way he teases out differences in usage between seemingly synonymous phrases, but the fact is that blurring is right there in the common use that ultimately accounts for the so-called lexical meaning.  The phenomenon people should really pay attention to, methodologically, is the contrast between loose and strict speech.  I think this is one of the keys to a purist take on the linguistic evidence for contextualism and interest relativism.  (Chisholm, Unger, and others thought this a pretty important phenomenon.)

2a. “The idea that in normal low stakes cases knowledge is enough for actionability just doesn’t seem to predict that it will seem to be enough in high stakes cases.“

Well, stakes are a smear and I think it’s only the very odd cases indeed that knowledge isn’t enough, since lots of justification is almost always enough.

2b. “Why is it that knowledge seems enough for actionability in these cases but “having good evidence” doesn’t seem to be enough? After all, normally having good evidence is enough for actionability.“

i. This is a good question, but I do think there is a sufficient answer.  One part of the issue concerns the various implicatures of “good”.  I think this is very complicated.  Saying of something that it was “good” is often do “damn with faint praise”.  From what I’ve been able to tell, what “good” conveys is more sensitive to intonation than any term I can think of.  Think of really strong and really week expressions of “That was a good meal.”  That can clearly be damning or celebrating.  

ii. Also, many purists think that knowledge requires not just good evidence, but *very* good evidence.  Feldman likes “beyond a reasonable doubt” and Conee tends to lean even stronger.  That can be pretty high.

iii.  We can apply both these considerations to the “good evidence” version of the challenge why “the move from “We know we’ll be able to deposit the checks tomorrow without the hassle of waiting in long lines” to “it makes more sense to come back tomorrow” should sound so plausible.“  What that gets us is the question: How plausible the move from “We have very good evidence we’ll be able to deposit the check tomorrow without the hassle” or “It’s beyond a reasonable doubt that we’ll be able to...” to “it makes more sense to come back tomorrow”?

My answer: Very plausible.  My thesis: Evidence is doing all the work.  [Side note: The Jeffreyan “radical probabilist” in me still resists the idea that all evidence need be knowledge, but I’m not opposed to the idea that all our evidence--if evidence is propositional instead of experiential--might *turn out* to be items of knowledge, simply in virtue of, say, being directly acquainted with their truth-makers.  This would require a pretty deflated account of belief or a rejection of K --&#062; B (neither of which seem so bad to me), but I don’t really talk much about outright belief, degrees of belief seem to do all the work.]

3. Re: reasons.  I met with Jonathan Dancy a month or so ago and he have me a very nice reading list on reasons, he was very helpful, and all those are on there.  I’m inclined to accept the proposal made early (only to be rejected) by Williams that there are (at least) two kinds of reasons. For reasons that won’t surprise you, I think the kinds of reasons that are primary are the kind a person can have and be motivated by.  But it is a complicated literature and I do plan to do some writing on this.

4. “ My reason for believing I’ll die unless I jump back might be that the approaching car will hit me unless I jump back“

I’m not sure how technical you are intending to be here, but I don’t see why I can’t accept this.  I do have a draft of a reply to the recent very nice piece on the ontology of reasons by John Turri which he discussed on CD not too long ago.  There are a lot of details there too, but it’s all stuff I really like to think about.  This is all such fun stuff!

5. I typically *don’t* talk in terms of reasons for action unless I’m talking loosely.  I took it that the Weirichian picture I painted above was a way of making reasons talk and EU fit together hand-in-glove.  In particular, I don’t take “A has the highest EU” to be a reason but rather a summary of reasons.  After all EU flanks and *identity* symbol which has what I consider the reasons on the other side.  EU = ([pros] - [cons]) where each reason has the form U(Oi)*Pr(Oi if A).  Often, though, we loosely just refer to the Oi.  When we get a little more sophisticated we consider their “intensity” (Franklin).  A bit more sophistication leads to a “weight” (Port Royal Logic), and yet more to a general summation.  

PS - Sorry for missing the reference to JB, I was pretty intently looking for changes from 2007 in order to update my critique thereof.  I’ll be going through it very systematically now.]]></description>
		<content:encoded><![CDATA[<p>Matt, just got back from the Plantinga retirement celebration and have had a chance to read your comments.</p>
<p>1a. “Saying “you know that’s not going to happen” is a common way of criticizing someone for worrying about a bad possibility.“</p>
<p>It’s also a common way of criticizing someone for hoping in a good possibility.  “You know you’re not going to win the lottery, so get back to work.”  So, while I admit that there’s some linguistic evidence for knowledge action principles (KAP’s), I think there’s also evidence against them, just as there are in the case of lottery knowledge.  That’s one reason I discount that type of evidence quite a bit.  But we seem to be in agreement (in conversation and the closing lines of your book) that theory can triumph over cases.  We seem to both be in search of the best overall theory, and that sometimes means sacrificing intuitions.  My main reason for endorsing this is the ambiguity of such evidence.</p>
<p>1b  In particular, I think all those examples you give are great examples of the ambiguity phenomenon.</p>
<p>i. “we should “know that oxygen is flowing, even if the bag doesn’t inflate; so there is no need to worry.“  I.e. we should *realize* or *accept* or *be aware of the fact that*, etc.  </p>
<p>ii. “ “I just didn’t know that it would work out ok at the time.”“  I.e. I couldn’t be *sure*.</p>
<p>iii. “Stop the LHC until we know it is safe!”“  I.e. know for sure.</p>
<p>My point, in part, is that these statements could support a knowledge-certainty principle as well that I don’t think either of us would accept as fallibilists.  Though I like a lot about Austin, in a way I see him as doing exactly the wrong thing.  I love the way he teases out differences in usage between seemingly synonymous phrases, but the fact is that blurring is right there in the common use that ultimately accounts for the so-called lexical meaning.  The phenomenon people should really pay attention to, methodologically, is the contrast between loose and strict speech.  I think this is one of the keys to a purist take on the linguistic evidence for contextualism and interest relativism.  (Chisholm, Unger, and others thought this a pretty important phenomenon.)</p>
<p>2a. “The idea that in normal low stakes cases knowledge is enough for actionability just doesn’t seem to predict that it will seem to be enough in high stakes cases.“</p>
<p>Well, stakes are a smear and I think it’s only the very odd cases indeed that knowledge isn’t enough, since lots of justification is almost always enough.</p>
<p>2b. “Why is it that knowledge seems enough for actionability in these cases but “having good evidence” doesn’t seem to be enough? After all, normally having good evidence is enough for actionability.“</p>
<p>i. This is a good question, but I do think there is a sufficient answer.  One part of the issue concerns the various implicatures of “good”.  I think this is very complicated.  Saying of something that it was “good” is often do “damn with faint praise”.  From what I’ve been able to tell, what “good” conveys is more sensitive to intonation than any term I can think of.  Think of really strong and really week expressions of “That was a good meal.”  That can clearly be damning or celebrating.  </p>
<p>ii. Also, many purists think that knowledge requires not just good evidence, but *very* good evidence.  Feldman likes “beyond a reasonable doubt” and Conee tends to lean even stronger.  That can be pretty high.</p>
<p>iii.  We can apply both these considerations to the “good evidence” version of the challenge why “the move from “We know we’ll be able to deposit the checks tomorrow without the hassle of waiting in long lines” to “it makes more sense to come back tomorrow” should sound so plausible.“  What that gets us is the question: How plausible the move from “We have very good evidence we’ll be able to deposit the check tomorrow without the hassle” or “It’s beyond a reasonable doubt that we’ll be able to&#8230;” to “it makes more sense to come back tomorrow”?</p>
<p>My answer: Very plausible.  My thesis: Evidence is doing all the work.  [Side note: The Jeffreyan “radical probabilist” in me still resists the idea that all evidence need be knowledge, but I’m not opposed to the idea that all our evidence&#8211;if evidence is propositional instead of experiential&#8211;might *turn out* to be items of knowledge, simply in virtue of, say, being directly acquainted with their truth-makers.  This would require a pretty deflated account of belief or a rejection of K &#8211;&gt; B (neither of which seem so bad to me), but I don’t really talk much about outright belief, degrees of belief seem to do all the work.]</p>
<p>3. Re: reasons.  I met with Jonathan Dancy a month or so ago and he have me a very nice reading list on reasons, he was very helpful, and all those are on there.  I’m inclined to accept the proposal made early (only to be rejected) by Williams that there are (at least) two kinds of reasons. For reasons that won’t surprise you, I think the kinds of reasons that are primary are the kind a person can have and be motivated by.  But it is a complicated literature and I do plan to do some writing on this.</p>
<p>4. “ My reason for believing I’ll die unless I jump back might be that the approaching car will hit me unless I jump back“</p>
<p>I’m not sure how technical you are intending to be here, but I don’t see why I can’t accept this.  I do have a draft of a reply to the recent very nice piece on the ontology of reasons by John Turri which he discussed on CD not too long ago.  There are a lot of details there too, but it’s all stuff I really like to think about.  This is all such fun stuff!</p>
<p>5. I typically *don’t* talk in terms of reasons for action unless I’m talking loosely.  I took it that the Weirichian picture I painted above was a way of making reasons talk and EU fit together hand-in-glove.  In particular, I don’t take “A has the highest EU” to be a reason but rather a summary of reasons.  After all EU flanks and *identity* symbol which has what I consider the reasons on the other side.  EU = ([pros] &#8211; [cons]) where each reason has the form U(Oi)*Pr(Oi if A).  Often, though, we loosely just refer to the Oi.  When we get a little more sophisticated we consider their “intensity” (Franklin).  A bit more sophistication leads to a “weight” (Port Royal Logic), and yet more to a general summation.  </p>
<p>PS &#8211; Sorry for missing the reference to JB, I was pretty intently looking for changes from 2007 in order to update my critique thereof.  I’ll be going through it very systematically now.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Matt Benton</title>
		<link>http://certaindoubts.com/knowledge-and-rational-action/#comment-11610</link>
		<dc:creator><![CDATA[Matt Benton]]></dc:creator>
		<pubDate>Thu, 20 May 2010 20:37:30 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1878#comment-11610</guid>
		<description><![CDATA[Hi Trent: 

I get confused in these kinds of cases that (appear to) conflate epistemic obligations with other (e.g. professional) obligations.  

(SAC) sounds abominable when you take the deontic &quot;have to&quot; of its second conjunct to have an epistemic flavor to it (epistemically have to); but when it carries a deontic of professional obligation, it doesn&#039;t sound bad at all - your expert (who sounds like a doctor) may have a professional obligation to confirm or check or whatever, since their procedural guidelines require it, etc. (Same thing goes for Brown&#039;s cases, and something similar infects many of Lackey&#039;s testimony cases where she has a teacher or a doctor testifying that p... To such cases I want to protest that Lackey is conflating epistemic reasons with other professional or prudential reasons, and in doing so gets some traction with one&#039;s judgments about the cases.) 

So if you keep the deontic notion epistemic across (SAC), it tends to sound bad.  

But (SACF) is a different animal, since it invokes a moral deontic notion by mentioning &quot;best consequences&quot;.  So it too conflates deontic senses, and twice over depending on your favor moral theory:  e.g. plug in &quot;torture this prisoner&quot; for &quot;F&quot; in (SACF).  If you&#039;re not a consequentialist, then (SACF) will sound fine.

Finally, I worry about your appeal to expectations to account for felicity.  Douven (Phil Review, 2006: 473ff.) once made this kind of pragmatic move (which he has since retracted) to try to account for Moorean paradoxes like &quot;P, but I don&#039;t know that p&quot;.  But (to borrow someone else&#039;s example), we also aren&#039;t used to, and don&#039;t expect, hearing things like &quot;I am the vegetarian chipmunk Beyonce.&quot;  Yet the Moorean construction seems far more infelicitous than the latter; indeed, the latter seems utterly felicitous (it&#039;s grammatical, I can grasp its meaning, etc.).]]></description>
		<content:encoded><![CDATA[<p>Hi Trent: </p>
<p>I get confused in these kinds of cases that (appear to) conflate epistemic obligations with other (e.g. professional) obligations.  </p>
<p>(SAC) sounds abominable when you take the deontic &#8220;have to&#8221; of its second conjunct to have an epistemic flavor to it (epistemically have to); but when it carries a deontic of professional obligation, it doesn&#8217;t sound bad at all &#8211; your expert (who sounds like a doctor) may have a professional obligation to confirm or check or whatever, since their procedural guidelines require it, etc. (Same thing goes for Brown&#8217;s cases, and something similar infects many of Lackey&#8217;s testimony cases where she has a teacher or a doctor testifying that p&#8230; To such cases I want to protest that Lackey is conflating epistemic reasons with other professional or prudential reasons, and in doing so gets some traction with one&#8217;s judgments about the cases.) </p>
<p>So if you keep the deontic notion epistemic across (SAC), it tends to sound bad.  </p>
<p>But (SACF) is a different animal, since it invokes a moral deontic notion by mentioning &#8220;best consequences&#8221;.  So it too conflates deontic senses, and twice over depending on your favor moral theory:  e.g. plug in &#8220;torture this prisoner&#8221; for &#8220;F&#8221; in (SACF).  If you&#8217;re not a consequentialist, then (SACF) will sound fine.</p>
<p>Finally, I worry about your appeal to expectations to account for felicity.  Douven (Phil Review, 2006: 473ff.) once made this kind of pragmatic move (which he has since retracted) to try to account for Moorean paradoxes like &#8220;P, but I don&#8217;t know that p&#8221;.  But (to borrow someone else&#8217;s example), we also aren&#8217;t used to, and don&#8217;t expect, hearing things like &#8220;I am the vegetarian chipmunk Beyonce.&#8221;  Yet the Moorean construction seems far more infelicitous than the latter; indeed, the latter seems utterly felicitous (it&#8217;s grammatical, I can grasp its meaning, etc.).</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Matt McGrath</title>
		<link>http://certaindoubts.com/knowledge-and-rational-action/#comment-11606</link>
		<dc:creator><![CDATA[Matt McGrath]]></dc:creator>
		<pubDate>Thu, 20 May 2010 03:17:55 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1878#comment-11606</guid>
		<description><![CDATA[Just a few more thoughts.  I won&#039;t try to answer all your interesting points (this comment will be long enough as it is).

I meant to say before that I would be worried if there were no linguistic evidence (or evidence from the way we speak) that suggested a knowledge-action connection.  I do think there is such evidence. Saying &quot;you know that&#039;s not going to happen&quot; is a common way of criticizing someone for worrying about a bad possibility.  On the way back from PEW, the in-flight safety video told us that when we put on the oxygen mask we should &quot;know that oxygen is flowing, even if the bag doesn&#039;t inflate; so there is no need to worry.&quot;  Similarly, one can defend one&#039;s playing it safe by saying &quot;I just didn&#039;t know that it would work out ok at the time.&quot;  And so on.  All these patterns of use are found even when the stakes are high (e.g., it&#039;s high stakes if I&#039;m wearing that oxygen mask!).  Another gem, from a website against the Large Hadron Collider:  &quot;Stop the LHC until we know it is safe!&quot;  

I don&#039;t think any of this is decisive evidence for a strong knowledge/action link, but it is some evidence.  And the fact that the patterns are found in high stakes situations is a problem for any view according to which knowledge is only sufficient for actionability in low stakes cases.  (Btw, we do reply to Jessica Brown in chapter 3, section 1.)

I do worry that your approach isn&#039;t going to explain why the move from &quot;We know we&#039;ll be able to deposit the checks tomorrow without the hassle of waiting in long lines&quot; to &quot;it makes more sense to come back tomorrow&quot; should sound so plausible.  The idea that in normal low stakes cases knowledge is enough for actionability just doesn&#039;t seem to predict that it will seem to be enough in high stakes cases.  Why is it that knowledge seems enough for actionability in these cases but &quot;having good evidence&quot; doesn&#039;t seem to be enough?  After all, normally having good evidence is enough for actionability.

I think it would be interesting if in order to resist arguments like ours one needed to deny that propositions/facts about how things stand in the world -- such as *the car will hit me unless I get out of the way* -- can be reasons for action.  That would be surprising, I&#039;d think.  You might check the literature on practical reasons on that (Dancy, Broome, Scanlon, Schroeder, Raz, etc.).  

Moreover, I&#039;d think you&#039;ll end up thinking that reasons are a motley crew.  Reasons for belief very often are about how things stand in the world.  My reason for believing I&#039;ll die unless I jump back might be that the approaching car will hit me unless I jump back.  This reason seems to be about things that matter very much to my practical life.  But you&#039;ll have to say it&#039;s the wrong kind of thing to be a practical reason.  I wonder if you&#039;d think reasons for joy, grief, pride, etc. must be probabilistic, too, or is it just action? 

I suspect, also, that you&#039;ll end up embracing something like our principles (this relates to what you said about the Stanley paper).  Presumably, merely having a belief that action A has the highest expected utility isn&#039;t enough to give one a justifying reason to do A -- the belief could be irrational, based on wishful thinking.  But then how warranted must the belief be?  Must you have probability 1 for *A has the highest expected utility* for that to be among your reasons?  That would be too strong, wouldn&#039;t it?  The idea that knowledge is enough to make these things a reason becomes attractive.  

I wonder why you don&#039;t plump for the hard-core Bayesian view that scraps reasons altogether.  An action is rational iff it has the highest expected utility of the available actions. End of story!

Finally, just to be clear: Jeremy and I don&#039;t want to rule out an expected utility account of rational action.  The hope was that the two stories -- the reasons story and the expected utility story -- would &quot;agree&quot;, so that whenever a reason justified you in PHI-ing, PHI-ing would also have the highest expected utility.]]></description>
		<content:encoded><![CDATA[<p>Just a few more thoughts.  I won&#8217;t try to answer all your interesting points (this comment will be long enough as it is).</p>
<p>I meant to say before that I would be worried if there were no linguistic evidence (or evidence from the way we speak) that suggested a knowledge-action connection.  I do think there is such evidence. Saying &#8220;you know that&#8217;s not going to happen&#8221; is a common way of criticizing someone for worrying about a bad possibility.  On the way back from PEW, the in-flight safety video told us that when we put on the oxygen mask we should &#8220;know that oxygen is flowing, even if the bag doesn&#8217;t inflate; so there is no need to worry.&#8221;  Similarly, one can defend one&#8217;s playing it safe by saying &#8220;I just didn&#8217;t know that it would work out ok at the time.&#8221;  And so on.  All these patterns of use are found even when the stakes are high (e.g., it&#8217;s high stakes if I&#8217;m wearing that oxygen mask!).  Another gem, from a website against the Large Hadron Collider:  &#8220;Stop the LHC until we know it is safe!&#8221;  </p>
<p>I don&#8217;t think any of this is decisive evidence for a strong knowledge/action link, but it is some evidence.  And the fact that the patterns are found in high stakes situations is a problem for any view according to which knowledge is only sufficient for actionability in low stakes cases.  (Btw, we do reply to Jessica Brown in chapter 3, section 1.)</p>
<p>I do worry that your approach isn&#8217;t going to explain why the move from &#8220;We know we&#8217;ll be able to deposit the checks tomorrow without the hassle of waiting in long lines&#8221; to &#8220;it makes more sense to come back tomorrow&#8221; should sound so plausible.  The idea that in normal low stakes cases knowledge is enough for actionability just doesn&#8217;t seem to predict that it will seem to be enough in high stakes cases.  Why is it that knowledge seems enough for actionability in these cases but &#8220;having good evidence&#8221; doesn&#8217;t seem to be enough?  After all, normally having good evidence is enough for actionability.</p>
<p>I think it would be interesting if in order to resist arguments like ours one needed to deny that propositions/facts about how things stand in the world &#8212; such as *the car will hit me unless I get out of the way* &#8212; can be reasons for action.  That would be surprising, I&#8217;d think.  You might check the literature on practical reasons on that (Dancy, Broome, Scanlon, Schroeder, Raz, etc.).  </p>
<p>Moreover, I&#8217;d think you&#8217;ll end up thinking that reasons are a motley crew.  Reasons for belief very often are about how things stand in the world.  My reason for believing I&#8217;ll die unless I jump back might be that the approaching car will hit me unless I jump back.  This reason seems to be about things that matter very much to my practical life.  But you&#8217;ll have to say it&#8217;s the wrong kind of thing to be a practical reason.  I wonder if you&#8217;d think reasons for joy, grief, pride, etc. must be probabilistic, too, or is it just action? </p>
<p>I suspect, also, that you&#8217;ll end up embracing something like our principles (this relates to what you said about the Stanley paper).  Presumably, merely having a belief that action A has the highest expected utility isn&#8217;t enough to give one a justifying reason to do A &#8212; the belief could be irrational, based on wishful thinking.  But then how warranted must the belief be?  Must you have probability 1 for *A has the highest expected utility* for that to be among your reasons?  That would be too strong, wouldn&#8217;t it?  The idea that knowledge is enough to make these things a reason becomes attractive.  </p>
<p>I wonder why you don&#8217;t plump for the hard-core Bayesian view that scraps reasons altogether.  An action is rational iff it has the highest expected utility of the available actions. End of story!</p>
<p>Finally, just to be clear: Jeremy and I don&#8217;t want to rule out an expected utility account of rational action.  The hope was that the two stories &#8212; the reasons story and the expected utility story &#8212; would &#8220;agree&#8221;, so that whenever a reason justified you in PHI-ing, PHI-ing would also have the highest expected utility.</p>
]]></content:encoded>
	</item>
</channel>
</rss>
