<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	>
<channel>
	<title>Comments on: Defeasible Consequences</title>
	<atom:link href="http://certaindoubts.com/defeasible-consequences/feed/" rel="self" type="application/rss+xml" />
	<link>http://certaindoubts.com/defeasible-consequences/</link>
	<description>devoted to matters epistemic</description>
	<lastBuildDate>Wed, 10 Apr 2019 16:37:28 +0000</lastBuildDate>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.10</generator>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/defeasible-consequences/#comment-1491</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Thu, 17 Mar 2005 18:50:47 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=244#comment-1491</guid>
		<description><![CDATA[Hi Trent,

Thanks for pointing out Weirich&#039;s work and for your comments. I&#039;m mostly familiar with Levi&#039;s project to ground belief change in decision theoretic terms and have thought about this topic in comparing Levi&#039;s approach to Hans Rott&#039;s (including work with Pagnucco) project of grounding belief change &lt;i&gt; and&lt;/i&gt; non-monotonic logic  in rational choice.

I don&#039;t deny that Bayesian updating is useful. I also, being slightly more careful now, don&#039;t deny that modeling ideal agents is useful--theoretically useful, even. Theoretically, I&#039;m much more sympathetic to Levi&#039;s project than Jim&#039;s approach, because I think genuine inductive expansion of our beliefs is uncontestable--since I accept statistical evidence statements, if for no other reason--and so tend to not favor views that ask me to think that inductive expansion is contestable or to think that it is really just deduction at bottom. I&#039;m inclinded to think, in either case, that the topic has changed. This particular debate is an old but important one, deserving our continued attention. I suspect that particular people involved in this debate get bored with talking past one another and return to their respective, likeminded groups (some existing largely outside of philosophy proper, or outside of the `english speaking world&#039;, or both, it should be added), content to ignore each other. However, if my sociological conjecture is true, I think it is a mistake that costs us all dearly. The issues exercised by this clash continue to crop up in other places: I&#039;m seeing more AI papers looking like epistemology papers, several of which grapple with pieces of this issue. They tend to come to it by comparing the behavior of models with what they wanted to model in the first place, which in matter epistemic is, or should be, the province of philosophy.  These are the main themes I have wanted to sound in my posts.

I would dispute your characterization of the laws of deduction, since I think they in themselves tell us very little about how to `maximize cognitive excellence&#039;, &lt;i&gt;particularly&lt;/i&gt; with respect to `reasonable cost limitations&#039;. Here I&#039;m sympathetic to Gilbert Harman&#039;s view that logic and epistemology have fundamentally distinct aims, although I don&#039;t accept the view that the two are not related at all. I&#039;ve argued this second point in joint work with Pereira appearing in JAL.

I&#039;m denying, in this instance, that we&#039;ve clearly in hand the appropriate ideal model to fall back on--denying, that is, the initial premise that starts Jim&#039;s &lt;i&gt;normative&lt;/i&gt; program. Citing that the probability measure is intuitively compelling is not sufficient grounds for regarding its structure (i.e., the mathematical structure of the measure) to be isomorphic to the structure of the notion you wish to model, which, in this case, is defeasible consequence.

Hence, I pressed Jim for a positive account for adopting this view of defeasible consequence. Jim mentions Dutch Book arguments. It would be interesting to see how a dutch book argument would go for adopting this view of defeasible consequence, and whether it would fair better or worse than dutch book arguments for strict Bayesianism. (On Dutch Books: A PSA panel in Vancouver consisting of Kyburg, Levi and Seidenfeld addressed replies to various versions of the argument a few years back. It is the most recent and comprehensive review that I am aware of, although I don&#039;t know whether the panel papers were published by PSA.)

Does that address your comments? Forgive my delay, I was out of the office yesterday.]]></description>
		<content:encoded><![CDATA[<p>Hi Trent,</p>
<p>Thanks for pointing out Weirich&#8217;s work and for your comments. I&#8217;m mostly familiar with Levi&#8217;s project to ground belief change in decision theoretic terms and have thought about this topic in comparing Levi&#8217;s approach to Hans Rott&#8217;s (including work with Pagnucco) project of grounding belief change <i> and</i> non-monotonic logic  in rational choice.</p>
<p>I don&#8217;t deny that Bayesian updating is useful. I also, being slightly more careful now, don&#8217;t deny that modeling ideal agents is useful&#8211;theoretically useful, even. Theoretically, I&#8217;m much more sympathetic to Levi&#8217;s project than Jim&#8217;s approach, because I think genuine inductive expansion of our beliefs is uncontestable&#8211;since I accept statistical evidence statements, if for no other reason&#8211;and so tend to not favor views that ask me to think that inductive expansion is contestable or to think that it is really just deduction at bottom. I&#8217;m inclinded to think, in either case, that the topic has changed. This particular debate is an old but important one, deserving our continued attention. I suspect that particular people involved in this debate get bored with talking past one another and return to their respective, likeminded groups (some existing largely outside of philosophy proper, or outside of the `english speaking world&#8217;, or both, it should be added), content to ignore each other. However, if my sociological conjecture is true, I think it is a mistake that costs us all dearly. The issues exercised by this clash continue to crop up in other places: I&#8217;m seeing more AI papers looking like epistemology papers, several of which grapple with pieces of this issue. They tend to come to it by comparing the behavior of models with what they wanted to model in the first place, which in matter epistemic is, or should be, the province of philosophy.  These are the main themes I have wanted to sound in my posts.</p>
<p>I would dispute your characterization of the laws of deduction, since I think they in themselves tell us very little about how to `maximize cognitive excellence&#8217;, <i>particularly</i> with respect to `reasonable cost limitations&#8217;. Here I&#8217;m sympathetic to Gilbert Harman&#8217;s view that logic and epistemology have fundamentally distinct aims, although I don&#8217;t accept the view that the two are not related at all. I&#8217;ve argued this second point in joint work with Pereira appearing in JAL.</p>
<p>I&#8217;m denying, in this instance, that we&#8217;ve clearly in hand the appropriate ideal model to fall back on&#8211;denying, that is, the initial premise that starts Jim&#8217;s <i>normative</i> program. Citing that the probability measure is intuitively compelling is not sufficient grounds for regarding its structure (i.e., the mathematical structure of the measure) to be isomorphic to the structure of the notion you wish to model, which, in this case, is defeasible consequence.</p>
<p>Hence, I pressed Jim for a positive account for adopting this view of defeasible consequence. Jim mentions Dutch Book arguments. It would be interesting to see how a dutch book argument would go for adopting this view of defeasible consequence, and whether it would fair better or worse than dutch book arguments for strict Bayesianism. (On Dutch Books: A PSA panel in Vancouver consisting of Kyburg, Levi and Seidenfeld addressed replies to various versions of the argument a few years back. It is the most recent and comprehensive review that I am aware of, although I don&#8217;t know whether the panel papers were published by PSA.)</p>
<p>Does that address your comments? Forgive my delay, I was out of the office yesterday.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Trent Dougherty</title>
		<link>http://certaindoubts.com/defeasible-consequences/#comment-1490</link>
		<dc:creator><![CDATA[Trent Dougherty]]></dc:creator>
		<pubDate>Tue, 15 Mar 2005 15:29:20 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=244#comment-1490</guid>
		<description><![CDATA[Greg,

You write: â??Iâ??m interested in uncertain reasoning,â??where the idealization assumptions are difficult if not impossible to satisfy. â?? one might demonstrate how a piece of cogent uncertain reasoning conforms to the constraints of an ideal agent. But, Iâ??m not sure what guidance such an agent is in these cases prior to reconstruction. It would seem that the epistemic force of this model would hinge on this question.â??

This is exactly the issue Iâ??m interested in Greg.  I think it helps to think of the issue on analogy with deductive reasoning.  Psychological states are not closed under entailment, but if I&#039;m highly confident of P and see that P entails Q, I should feel uneasy if I donâ??t believe Q.  This is because the laws of deduction are normative for cognitive agents whose goal is maximal cognitive excellence within reasonable cost limitations.  Thatâ??s just another way of saying that it approximates a cognitive ideal.  This can be represented anthropocentrically by ideal agents.  We are not idea agents so we scale back rules of deduction to reasonable psychological limits.  This is just what Paul Weirich is doing with decision theory in his recent _Realistic Decision Theory: Rules for Nonideal Agents in Nonideal Circumstances_ (I really urge you to check it out, you can look in the TOC on Amazon).  But like Jim says, we&#039;ve got to have an ideal to scale back first.  I think itâ??s just a plain fact that learning (truly learning, i.e. internalizing) some logic helps you think better or at least more clearly in many cases (especially when youâ??re trying), i.e.  provides *guidance* in what to believe.  I think probability theory is even better at this.  There have been at least three papers where I assigned intervals to the constituents of my argument only to find that I was probabilistically incoherent.  After some reflection, my probabilities settled down more coherently (because, I would say, of certain standing dispositions being triggered by the reflection).  What more practical guidance could one want?  I have a fellow Bayesian with whom Iâ??ve written some papers and heâ??d say the same thing I think.

Iâ??d prefer to say simply that I want a maximally accurate and comprehensive representational system (within reasonable cost limitations) and I donâ??t think probabilistically incoherent systems can be accurate, thus I desire coherence.  But another, and fair, way to say that is that Iâ??m trying to do my cognitive best, trying to achieve a cognitive ideal, trying to be more and more like an ideal cognitive agent (even though my realistic psychology lets me cut myself some slack, I donâ??t fret that I donâ??t believe the deductive consequences of all that I believe or that, no doubt, Iâ??ve still got lots of incoherence - I do what I can.  Part of what I can do is reflect upon the coherence of by belief system, in particular how far it deviates from what I consider necessary conditions for ideal performance).

Trent]]></description>
		<content:encoded><![CDATA[<p>Greg,</p>
<p>You write: â??Iâ??m interested in uncertain reasoning,â??where the idealization assumptions are difficult if not impossible to satisfy. â?? one might demonstrate how a piece of cogent uncertain reasoning conforms to the constraints of an ideal agent. But, Iâ??m not sure what guidance such an agent is in these cases prior to reconstruction. It would seem that the epistemic force of this model would hinge on this question.â??</p>
<p>This is exactly the issue Iâ??m interested in Greg.  I think it helps to think of the issue on analogy with deductive reasoning.  Psychological states are not closed under entailment, but if I&#8217;m highly confident of P and see that P entails Q, I should feel uneasy if I donâ??t believe Q.  This is because the laws of deduction are normative for cognitive agents whose goal is maximal cognitive excellence within reasonable cost limitations.  Thatâ??s just another way of saying that it approximates a cognitive ideal.  This can be represented anthropocentrically by ideal agents.  We are not idea agents so we scale back rules of deduction to reasonable psychological limits.  This is just what Paul Weirich is doing with decision theory in his recent _Realistic Decision Theory: Rules for Nonideal Agents in Nonideal Circumstances_ (I really urge you to check it out, you can look in the TOC on Amazon).  But like Jim says, we&#8217;ve got to have an ideal to scale back first.  I think itâ??s just a plain fact that learning (truly learning, i.e. internalizing) some logic helps you think better or at least more clearly in many cases (especially when youâ??re trying), i.e.  provides *guidance* in what to believe.  I think probability theory is even better at this.  There have been at least three papers where I assigned intervals to the constituents of my argument only to find that I was probabilistically incoherent.  After some reflection, my probabilities settled down more coherently (because, I would say, of certain standing dispositions being triggered by the reflection).  What more practical guidance could one want?  I have a fellow Bayesian with whom Iâ??ve written some papers and heâ??d say the same thing I think.</p>
<p>Iâ??d prefer to say simply that I want a maximally accurate and comprehensive representational system (within reasonable cost limitations) and I donâ??t think probabilistically incoherent systems can be accurate, thus I desire coherence.  But another, and fair, way to say that is that Iâ??m trying to do my cognitive best, trying to achieve a cognitive ideal, trying to be more and more like an ideal cognitive agent (even though my realistic psychology lets me cut myself some slack, I donâ??t fret that I donâ??t believe the deductive consequences of all that I believe or that, no doubt, Iâ??ve still got lots of incoherence &#8211; I do what I can.  Part of what I can do is reflect upon the coherence of by belief system, in particular how far it deviates from what I consider necessary conditions for ideal performance).</p>
<p>Trent</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/defeasible-consequences/#comment-1489</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Mon, 14 Mar 2005 16:43:29 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=244#comment-1489</guid>
		<description><![CDATA[Hi Jim,

Yes, exactly, it is *the big question*. We&#039;re probably anticipating each other&#039;s answers, but, still, I think it is important that we (royal, hopefully; you and I, worst case?) grapple with it. It seems that we (royal: epistemologists) have largely stopped addressing this question--perhaps because it is prone to frustration, or maybe because folk became bored with the topic and went off into their own circles to work. I think that it is a mistake for epistemologists to abandon this topic, however, since it is being confronted in other domains...particularly in AI. I&#039;m aware of some misgivings in philosophy about AI, and AI has certainly invited some of this skepticism with its at times overly optimistic research programs. But, at bottom, it is addressing many of the core set of issues exercising epistemologists. And the field is bursting with activity. The computational stance (for lack of a better term) offers constraints to work within that address some of the components of the big question that various posts to this, and other threads, have addressed.

In (brief) reply to the role of ideal agents: I&#039;m interested in uncertain reasoning, which in many (most?) cases arises where the idealization assumptions are difficult if not impossible to satisfy. That&#039;s not to say that, retrospectively, one might demonstrate how a piece of cogent uncertain reasoning conforms to the constraints of an ideal agent. But, I&#039;m not sure what &lt;i&gt;guidance&lt;/i&gt; such an agent is in these cases prior to reconstruction. It would seem that the epistemic force of this model would hinge on this question. This is far from knock down, of course.

Formally, the problem of defeasible consequence is a problem for applied logic. Which, I&#039;ve offered, has two components. The first is some pre-theoretic idea (those outside epistemology would call this some `philosophy&#039;) about the notion to be modeled: what defeasible consequence is, what logical consequence is, what identity is (if you&#039;re a type theorist, say), and so on. The other component, of course, is the formal system, which is a piece of mathematics: here we examine structures, namely sets on which relations and functions have been defined and correspondences shown to hold between these structures. Then, we see whether a given formal system is faithful to the pre-theoretic notion we are working with. In this respect, it is a descriptive exercise. Perhaps we learn about limits--no empty domains, some terms don&#039;t get a type, whatever--and then incorporate those into the boundary conditions for applying this formal system: we say, modulo these conditions, that the system is faithful to such-in-such notion and so, under these conditions, may be thought of as giving prescriptive advice.

I tend to think that we&#039;re still at the descriptive stage of understanding defeasible consequence. There are epistemic benefits, however. By having this &lt;i&gt;plurality&lt;/i&gt; of formal models of defeasible consequence, with their corresponding motivating examples, we&#039;ll have a set of benchmark examples mapped into structures that we may &lt;i&gt;then&lt;/i&gt; study formally. And that looks exciting, to me.

I stress the term &#039;plurality&#039; since I do think we hurt ourselves, collectively, by focusing too much on our respective schools. It is this idea that motivates my pestering you with counter-examples (not all of which are successful, granted) or pressing the case in general for us to look outside of Bayesianism.

So the big, big question is whether (formal) epistemologists will let someone else eat their lunch.]]></description>
		<content:encoded><![CDATA[<p>Hi Jim,</p>
<p>Yes, exactly, it is *the big question*. We&#8217;re probably anticipating each other&#8217;s answers, but, still, I think it is important that we (royal, hopefully; you and I, worst case?) grapple with it. It seems that we (royal: epistemologists) have largely stopped addressing this question&#8211;perhaps because it is prone to frustration, or maybe because folk became bored with the topic and went off into their own circles to work. I think that it is a mistake for epistemologists to abandon this topic, however, since it is being confronted in other domains&#8230;particularly in AI. I&#8217;m aware of some misgivings in philosophy about AI, and AI has certainly invited some of this skepticism with its at times overly optimistic research programs. But, at bottom, it is addressing many of the core set of issues exercising epistemologists. And the field is bursting with activity. The computational stance (for lack of a better term) offers constraints to work within that address some of the components of the big question that various posts to this, and other threads, have addressed.</p>
<p>In (brief) reply to the role of ideal agents: I&#8217;m interested in uncertain reasoning, which in many (most?) cases arises where the idealization assumptions are difficult if not impossible to satisfy. That&#8217;s not to say that, retrospectively, one might demonstrate how a piece of cogent uncertain reasoning conforms to the constraints of an ideal agent. But, I&#8217;m not sure what <i>guidance</i> such an agent is in these cases prior to reconstruction. It would seem that the epistemic force of this model would hinge on this question. This is far from knock down, of course.</p>
<p>Formally, the problem of defeasible consequence is a problem for applied logic. Which, I&#8217;ve offered, has two components. The first is some pre-theoretic idea (those outside epistemology would call this some `philosophy&#8217;) about the notion to be modeled: what defeasible consequence is, what logical consequence is, what identity is (if you&#8217;re a type theorist, say), and so on. The other component, of course, is the formal system, which is a piece of mathematics: here we examine structures, namely sets on which relations and functions have been defined and correspondences shown to hold between these structures. Then, we see whether a given formal system is faithful to the pre-theoretic notion we are working with. In this respect, it is a descriptive exercise. Perhaps we learn about limits&#8211;no empty domains, some terms don&#8217;t get a type, whatever&#8211;and then incorporate those into the boundary conditions for applying this formal system: we say, modulo these conditions, that the system is faithful to such-in-such notion and so, under these conditions, may be thought of as giving prescriptive advice.</p>
<p>I tend to think that we&#8217;re still at the descriptive stage of understanding defeasible consequence. There are epistemic benefits, however. By having this <i>plurality</i> of formal models of defeasible consequence, with their corresponding motivating examples, we&#8217;ll have a set of benchmark examples mapped into structures that we may <i>then</i> study formally. And that looks exciting, to me.</p>
<p>I stress the term &#8216;plurality&#8217; since I do think we hurt ourselves, collectively, by focusing too much on our respective schools. It is this idea that motivates my pestering you with counter-examples (not all of which are successful, granted) or pressing the case in general for us to look outside of Bayesianism.</p>
<p>So the big, big question is whether (formal) epistemologists will let someone else eat their lunch.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Jim Hawthorne</title>
		<link>http://certaindoubts.com/defeasible-consequences/#comment-1488</link>
		<dc:creator><![CDATA[Jim Hawthorne]]></dc:creator>
		<pubDate>Sat, 12 Mar 2005 01:45:46 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=244#comment-1488</guid>
		<description><![CDATA[Greg,

If I&#039;m understanding you, your question is *the big question* of the following sort: why consider functions that satisfy the axioms of probability theory to be an appropriate representation of ideal belief? People have given various kinds of answers to this question -- e.g. &quot;dutch-book avoidance&quot; (e.g. Ramsey, de Finetti); &quot;calibration&quot; (e.g. van Fraassen, Joyce, and others); how well it fits the &quot;belief component&quot; of a theory of preference, belief, and decision (e.g. L.J. Savage and others). I think all of these give us some reason to think that probability is a good model of ideal degrees of confidence (or degrees of belief). But one of the things that I find most convincing is that there is a rather simple axiomatization of a binary relation that can be interpreted as &quot; the agent is at least as confident in A as in B&quot; (or &quot;believes A at least as strongly as she believes B&quot;), and this set of axioms look to me intuitively like very plausible constraints on ideal belief. I&#039;m talking about a version of the axioms for so-called &quot;qualitative probabilities&quot;. Furthermore, it can be proved that any such relation (that satisfies these axioms) can be modeled uniquely by a standard quantitative probability functions.

When you use a logic to model statistical arguments, you may not employ ideal agents directly. But presumably you think that real agent&#039;s beliefs should &quot;conform to&quot; such arguments as much as possible. (Otherwise, what is the epistemic import of the logic -- or is ir epistemically useless?) It seems to me that to the extent that you think this logic should inform belief, there is something very much like an ideal agent lurking in the background. Perhaps my point is most easily made in terms of standard deductive entailment. The semantics of deductive logic need not draw on agents of any kind. But as soon as you try to describe the epistemic import of the logic, it is hard to avoid saying things like: &quot;whenever the premises are true, the conclusion has to be true -- so an agent who is (ideally) logically coherent and believes the premises must not believe the conclusion to be false.&quot; There may be important caveats, but you get the picture.

One might say, &quot;fine -- but the semantics for deductive logic doesn&#039;t involve ideal agents, only the epistemological application does.&quot;  If that&#039;s what&#039;s bothering you, I can provide what seems to me to be a compelling account of qualitiative probability (and of quantitative probability), defined on sentences, that interprets the relation (or the function) in terms of &quot;weightiness measures&quot; on sets of possible worlds. In this version the ideal agent only comes in afterwords, as in the deductive case. An agent who is (ideally) coherent can only measure the weightiness of possibilities in certain ways (which are constrained by the previously mentioned intuitively plausible axioms on the weightnesses of possibilities); and an (ideal) agent&#039;s belief strengths (or confidence levels) should fit coherently with some such measures if her belief strengths are to match how weighty the possiblities are (or can be), given how possibility weightings must work (given the intuitively plausible axioms for such weightings).

Does that help?]]></description>
		<content:encoded><![CDATA[<p>Greg,</p>
<p>If I&#8217;m understanding you, your question is *the big question* of the following sort: why consider functions that satisfy the axioms of probability theory to be an appropriate representation of ideal belief? People have given various kinds of answers to this question &#8212; e.g. &#8220;dutch-book avoidance&#8221; (e.g. Ramsey, de Finetti); &#8220;calibration&#8221; (e.g. van Fraassen, Joyce, and others); how well it fits the &#8220;belief component&#8221; of a theory of preference, belief, and decision (e.g. L.J. Savage and others). I think all of these give us some reason to think that probability is a good model of ideal degrees of confidence (or degrees of belief). But one of the things that I find most convincing is that there is a rather simple axiomatization of a binary relation that can be interpreted as &#8221; the agent is at least as confident in A as in B&#8221; (or &#8220;believes A at least as strongly as she believes B&#8221;), and this set of axioms look to me intuitively like very plausible constraints on ideal belief. I&#8217;m talking about a version of the axioms for so-called &#8220;qualitative probabilities&#8221;. Furthermore, it can be proved that any such relation (that satisfies these axioms) can be modeled uniquely by a standard quantitative probability functions.</p>
<p>When you use a logic to model statistical arguments, you may not employ ideal agents directly. But presumably you think that real agent&#8217;s beliefs should &#8220;conform to&#8221; such arguments as much as possible. (Otherwise, what is the epistemic import of the logic &#8212; or is ir epistemically useless?) It seems to me that to the extent that you think this logic should inform belief, there is something very much like an ideal agent lurking in the background. Perhaps my point is most easily made in terms of standard deductive entailment. The semantics of deductive logic need not draw on agents of any kind. But as soon as you try to describe the epistemic import of the logic, it is hard to avoid saying things like: &#8220;whenever the premises are true, the conclusion has to be true &#8212; so an agent who is (ideally) logically coherent and believes the premises must not believe the conclusion to be false.&#8221; There may be important caveats, but you get the picture.</p>
<p>One might say, &#8220;fine &#8212; but the semantics for deductive logic doesn&#8217;t involve ideal agents, only the epistemological application does.&#8221;  If that&#8217;s what&#8217;s bothering you, I can provide what seems to me to be a compelling account of qualitiative probability (and of quantitative probability), defined on sentences, that interprets the relation (or the function) in terms of &#8220;weightiness measures&#8221; on sets of possible worlds. In this version the ideal agent only comes in afterwords, as in the deductive case. An agent who is (ideally) coherent can only measure the weightiness of possibilities in certain ways (which are constrained by the previously mentioned intuitively plausible axioms on the weightnesses of possibilities); and an (ideal) agent&#8217;s belief strengths (or confidence levels) should fit coherently with some such measures if her belief strengths are to match how weighty the possiblities are (or can be), given how possibility weightings must work (given the intuitively plausible axioms for such weightings).</p>
<p>Does that help?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/defeasible-consequences/#comment-1487</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Fri, 11 Mar 2005 21:30:08 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=244#comment-1487</guid>
		<description><![CDATA[Hi Jim,

Thanks for your reply, and the charity in reading my hastily composed post. I&#039;ll try pressing the point once again. I don&#039;t think that I am committed to &#039;a logic of real belief&#039;, since I&#039;m not sure what that would be, or how to identify such a thing--which is what drives my question of asking how you do it. I am very sympathetic to the &lt;i&gt;use&lt;/i&gt; of logics to model various types of arguments and processes, including non-monotonic arguments. (But I think that certain kinds of arguments, such as statistical arguments, give us firm enough input-output constraints to address this problem of fit that arises when considering candidate logics for this KR purpose.) I don&#039;t have anything like an ideal agent in mind in carrying out this project. Nor do I need one.

In order to make the comparative judgment between the ideal reasoner and the actual reasoning, I would think one would need to establish that the model you propose is the model of an ideal defeasible reasoner. If the question is, given a model of defeasible reasoning such and such, where the two disagree, is the actual person mistaken, then, sure, you are correct: the person is mistaken; this follows trivially. But we can ask whether the theory is an appropriate one to apply to model defeasible reasoning too, and I am asking how you would defend the claim that your proposal is the correct standard for defeasible reasoning. Forgive me for pressing and apologies for my typing. I find the discussion very stimulating; thanks for your comments.]]></description>
		<content:encoded><![CDATA[<p>Hi Jim,</p>
<p>Thanks for your reply, and the charity in reading my hastily composed post. I&#8217;ll try pressing the point once again. I don&#8217;t think that I am committed to &#8216;a logic of real belief&#8217;, since I&#8217;m not sure what that would be, or how to identify such a thing&#8211;which is what drives my question of asking how you do it. I am very sympathetic to the <i>use</i> of logics to model various types of arguments and processes, including non-monotonic arguments. (But I think that certain kinds of arguments, such as statistical arguments, give us firm enough input-output constraints to address this problem of fit that arises when considering candidate logics for this KR purpose.) I don&#8217;t have anything like an ideal agent in mind in carrying out this project. Nor do I need one.</p>
<p>In order to make the comparative judgment between the ideal reasoner and the actual reasoning, I would think one would need to establish that the model you propose is the model of an ideal defeasible reasoner. If the question is, given a model of defeasible reasoning such and such, where the two disagree, is the actual person mistaken, then, sure, you are correct: the person is mistaken; this follows trivially. But we can ask whether the theory is an appropriate one to apply to model defeasible reasoning too, and I am asking how you would defend the claim that your proposal is the correct standard for defeasible reasoning. Forgive me for pressing and apologies for my typing. I find the discussion very stimulating; thanks for your comments.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Jim Hawthorne</title>
		<link>http://certaindoubts.com/defeasible-consequences/#comment-1470</link>
		<dc:creator><![CDATA[Jim Hawthorne]]></dc:creator>
		<pubDate>Fri, 11 Mar 2005 19:40:46 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=244#comment-1470</guid>
		<description><![CDATA[Greg,

You say, &quot;I understand you to be committed to the view that a logic for defeasible consequence will provide a theory for ideal agents making defeasible consequences.&quot; Yes, that&#039;s the idea, I think. But I would put it somewhat differently. I would say instead, &quot;I am committed to the view that an important project in the development of the logic for defeasible consequence is to provide a theory of defeasible consequence relations that is not constrained by the limited cognitive abilities (particularly, the limited logical and computational abilities) of actual agents.&quot; To the extent that there are differences in your characterization and the one I just gave, I buy mine rather than yours.

You go on to say, &quot;Hence, I take it that if there is a disagreement in states between such an agent drawing a defeasible consequence and an actual person drawing a defeasible consequence, given the same inputs, you would say that the reason was due to some failing of the personâ??perhaps a nomologically necessary failingâ??to correctly draw a defeasible inference. Is that accurate?&quot; Yes, that is accurate. But I wouldn&#039;t call sub-ideal reasoning &quot;irrationality&quot;. The &quot;failing&quot; is that the actual person would be an even better reasoner if she were able to make the ideal inferences. (You certainly wouldn&#039;t say she was a worse reasoner than the actual reasoner, would you?) Perhaps it will make the point clearer to put it the other way around. What &quot;failings of reasoning&quot; would the &quot;ideal reasoner&quot; have if she failed to agree with the actual reasoner? Perhaps she would have failings, but they would be &quot;failings of modeling realistic abilities&quot;, not failures of reasoning.

In any case, I think we cannot do without the ideal model -- i.e. we cannot give a sufficiently rich account of &quot;the logic&quot; if we try only to develop a &quot;logic for realistic reasoners.&quot; I think we will always need this &quot;logically omniscient ideal&quot; for the following reason. No matter how good your &quot;logic of realistic reasoning&quot; is at describing norms for real reasoners, there will always be some cognitive differences among real people. Although no real agent will be logically omniscient, some will be more &quot;logically adept&quot; than others. And these people will (or should) count as better reasoners for it. I see no plausible way to draw a firm line for &quot;good enough reasoning&quot; -- i.e. I doubt that we can develop a &quot;logic of real reasoning&quot; that places us in a position to make the following claim (about that logic), &quot;Reasoning that reaches the logical depth this &#039;real logic&#039; articulates is as good as we can possibly want a real reasoner to be, and any actual reasoner who happens to possess an ability to reason more deeply than this just cannot count as any better at reasoning&quot;. The logical ideal may be an unattainable standard for real reasoners, but it may nevertheless be a least upper bound on reasoning ability -- an ideal limit that we cannot stop short of (in a non-arbitrary way) in our attempts spell out a sufficiently rich logic to capture all of what can count as &quot;good reasoning&quot;.

Let me make it clear that I don&#039;t intend this as some sort of knock down argument that a &quot;logic of realistic reasoning&quot; is impossible. I&#039;m just doubtful that any such logic, if we ever get one, can completely supercede the kind of normative role played by the stronger logic that is suitable to ideal agents (which I think we can develop). In any case, when you have such a &quot;realistic logic of defeasible belief&quot; to show me, then we&#039;ll see!!!]]></description>
		<content:encoded><![CDATA[<p>Greg,</p>
<p>You say, &#8220;I understand you to be committed to the view that a logic for defeasible consequence will provide a theory for ideal agents making defeasible consequences.&#8221; Yes, that&#8217;s the idea, I think. But I would put it somewhat differently. I would say instead, &#8220;I am committed to the view that an important project in the development of the logic for defeasible consequence is to provide a theory of defeasible consequence relations that is not constrained by the limited cognitive abilities (particularly, the limited logical and computational abilities) of actual agents.&#8221; To the extent that there are differences in your characterization and the one I just gave, I buy mine rather than yours.</p>
<p>You go on to say, &#8220;Hence, I take it that if there is a disagreement in states between such an agent drawing a defeasible consequence and an actual person drawing a defeasible consequence, given the same inputs, you would say that the reason was due to some failing of the personâ??perhaps a nomologically necessary failingâ??to correctly draw a defeasible inference. Is that accurate?&#8221; Yes, that is accurate. But I wouldn&#8217;t call sub-ideal reasoning &#8220;irrationality&#8221;. The &#8220;failing&#8221; is that the actual person would be an even better reasoner if she were able to make the ideal inferences. (You certainly wouldn&#8217;t say she was a worse reasoner than the actual reasoner, would you?) Perhaps it will make the point clearer to put it the other way around. What &#8220;failings of reasoning&#8221; would the &#8220;ideal reasoner&#8221; have if she failed to agree with the actual reasoner? Perhaps she would have failings, but they would be &#8220;failings of modeling realistic abilities&#8221;, not failures of reasoning.</p>
<p>In any case, I think we cannot do without the ideal model &#8212; i.e. we cannot give a sufficiently rich account of &#8220;the logic&#8221; if we try only to develop a &#8220;logic for realistic reasoners.&#8221; I think we will always need this &#8220;logically omniscient ideal&#8221; for the following reason. No matter how good your &#8220;logic of realistic reasoning&#8221; is at describing norms for real reasoners, there will always be some cognitive differences among real people. Although no real agent will be logically omniscient, some will be more &#8220;logically adept&#8221; than others. And these people will (or should) count as better reasoners for it. I see no plausible way to draw a firm line for &#8220;good enough reasoning&#8221; &#8212; i.e. I doubt that we can develop a &#8220;logic of real reasoning&#8221; that places us in a position to make the following claim (about that logic), &#8220;Reasoning that reaches the logical depth this &#8216;real logic&#8217; articulates is as good as we can possibly want a real reasoner to be, and any actual reasoner who happens to possess an ability to reason more deeply than this just cannot count as any better at reasoning&#8221;. The logical ideal may be an unattainable standard for real reasoners, but it may nevertheless be a least upper bound on reasoning ability &#8212; an ideal limit that we cannot stop short of (in a non-arbitrary way) in our attempts spell out a sufficiently rich logic to capture all of what can count as &#8220;good reasoning&#8221;.</p>
<p>Let me make it clear that I don&#8217;t intend this as some sort of knock down argument that a &#8220;logic of realistic reasoning&#8221; is impossible. I&#8217;m just doubtful that any such logic, if we ever get one, can completely supercede the kind of normative role played by the stronger logic that is suitable to ideal agents (which I think we can develop). In any case, when you have such a &#8220;realistic logic of defeasible belief&#8221; to show me, then we&#8217;ll see!!!</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/defeasible-consequences/#comment-1476</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Fri, 11 Mar 2005 16:47:45 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=244#comment-1476</guid>
		<description><![CDATA[Jim (and Trent, Stephen: very nice discussion. I enjoy reading all of your threads very much. My comments here are directed to the thread with Jim.)

 I am not committed to either semantics providing a better basis for defeasible consequence. Rather, I am very interested in how to evaluate whether a logic for defeasible consequence is a reasonable candidate for an ideal theory of defeasible consequence. I suppose one could simply call the notion so-defined &#039;defeasible consequence&#039;. However, I suggested in (ln 24) that viewing such a theory for ideal defeasible consequence commits one to providing a motivation for adopting this semantics, in so far as we&#039;re to think that studying the behavior of this ideal agent will give us prescriptive guidance or descriptive insight into defeasible reasoning. I&#039;m sincerely interested in this question.

 I understand our discussion to have taken the following form. I understood your initial reply to offer two reasons as motivation: (1) probabilistic semantics are the only game in town, and (2) the notion of degrees of belief and the exercise of constraining belief by the axioms of probability are intuitive and well understood, in so far as probability is a well understood theory. I offered the two semantics--preferential models for non-mon consequence, and the semantics for epistemic conditionals for non-mon conditionals--as examples directed against claim (1). I offered the analogy of preference revision to address (2) in the following sense: economic preference is an intuitive notion and well understood and economic decision is a well understood theory. However, we (most of us) don&#039;t view the theory as prescriptive without careful qualification. When the objects of choice are themselves preferences, decision theoretic treatments don&#039;t seem to be appropriate. (Jon Doyle has written on this recently in Computational intelligence 20(2) 2004, which is an expanded version of a Doyle and Rich Thomason piece appearing in AI Magazine, 1999.) The point is, we view this as a problem for the theory and not a problem of people being irrational.

 I offer this outline because I worry that we run a risk of talking past one another. I understand you to be committed to the view that a logic for defeasible consequence will provide a theory for ideal agents making defeasible consequences. Hence, I take it that if there is a disagreement in states between such an agent drawing a defeasible consequence and an actual person drawing a defeasible consequence, given the same inputs, you would say that the reason was due to some failing of the person---perhaps a nomologically necessary failing--to correctly draw a defeasible inference. Is that accurate?

 If so, why think this is the case? We don&#039;t think preference revision is irrational. Or would you think that preference revision is irrational?]]></description>
		<content:encoded><![CDATA[<p>Jim (and Trent, Stephen: very nice discussion. I enjoy reading all of your threads very much. My comments here are directed to the thread with Jim.)</p>
<p> I am not committed to either semantics providing a better basis for defeasible consequence. Rather, I am very interested in how to evaluate whether a logic for defeasible consequence is a reasonable candidate for an ideal theory of defeasible consequence. I suppose one could simply call the notion so-defined &#8216;defeasible consequence&#8217;. However, I suggested in (ln 24) that viewing such a theory for ideal defeasible consequence commits one to providing a motivation for adopting this semantics, in so far as we&#8217;re to think that studying the behavior of this ideal agent will give us prescriptive guidance or descriptive insight into defeasible reasoning. I&#8217;m sincerely interested in this question.</p>
<p> I understand our discussion to have taken the following form. I understood your initial reply to offer two reasons as motivation: (1) probabilistic semantics are the only game in town, and (2) the notion of degrees of belief and the exercise of constraining belief by the axioms of probability are intuitive and well understood, in so far as probability is a well understood theory. I offered the two semantics&#8211;preferential models for non-mon consequence, and the semantics for epistemic conditionals for non-mon conditionals&#8211;as examples directed against claim (1). I offered the analogy of preference revision to address (2) in the following sense: economic preference is an intuitive notion and well understood and economic decision is a well understood theory. However, we (most of us) don&#8217;t view the theory as prescriptive without careful qualification. When the objects of choice are themselves preferences, decision theoretic treatments don&#8217;t seem to be appropriate. (Jon Doyle has written on this recently in Computational intelligence 20(2) 2004, which is an expanded version of a Doyle and Rich Thomason piece appearing in AI Magazine, 1999.) The point is, we view this as a problem for the theory and not a problem of people being irrational.</p>
<p> I offer this outline because I worry that we run a risk of talking past one another. I understand you to be committed to the view that a logic for defeasible consequence will provide a theory for ideal agents making defeasible consequences. Hence, I take it that if there is a disagreement in states between such an agent drawing a defeasible consequence and an actual person drawing a defeasible consequence, given the same inputs, you would say that the reason was due to some failing of the person&#8212;perhaps a nomologically necessary failing&#8211;to correctly draw a defeasible inference. Is that accurate?</p>
<p> If so, why think this is the case? We don&#8217;t think preference revision is irrational. Or would you think that preference revision is irrational?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Trent Dougherty</title>
		<link>http://certaindoubts.com/defeasible-consequences/#comment-1486</link>
		<dc:creator><![CDATA[Trent Dougherty]]></dc:creator>
		<pubDate>Fri, 11 Mar 2005 15:07:16 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=244#comment-1486</guid>
		<description><![CDATA[Jim,

Very nice comments.  It is indeed the *uniqueness* that is the key.  The objective Bayesian wants to be able to say that the objective probabilities are just plain logical facts, logical relations between propositions just like deductive logical relations.  So if thereâ??s not uniqueness, thereâ??s nothing (which is why I didnâ??t take non-unique measures as being relevant to the question).  Your counterpart theoretic comparison was very interesting and Iâ??ll have to think about that.

The modal model example was very helpful.  Itâ??s ironic too because Iâ??m known for wanting to dispense with operators and just quantify over worlds (which I donâ??t mind doing as an actualist, since, after all, the do exist even if they are lies).  I just finished an article yesterday against the converse Barcan formula that depends on quantifying over worlds.  The key point in your analogy, as I understand it, was that there is no real competition between the probability calculus and the sorts of non-monotonic logics youâ??re looking at because the probability calculus generalizes the logic.  Itâ??s just that some particularizations are more usefully applied in certain situations than the general theory.  Do I have that right?  If so, that really helps a lot and is an apt analogy.

Trent]]></description>
		<content:encoded><![CDATA[<p>Jim,</p>
<p>Very nice comments.  It is indeed the *uniqueness* that is the key.  The objective Bayesian wants to be able to say that the objective probabilities are just plain logical facts, logical relations between propositions just like deductive logical relations.  So if thereâ??s not uniqueness, thereâ??s nothing (which is why I didnâ??t take non-unique measures as being relevant to the question).  Your counterpart theoretic comparison was very interesting and Iâ??ll have to think about that.</p>
<p>The modal model example was very helpful.  Itâ??s ironic too because Iâ??m known for wanting to dispense with operators and just quantify over worlds (which I donâ??t mind doing as an actualist, since, after all, the do exist even if they are lies).  I just finished an article yesterday against the converse Barcan formula that depends on quantifying over worlds.  The key point in your analogy, as I understand it, was that there is no real competition between the probability calculus and the sorts of non-monotonic logics youâ??re looking at because the probability calculus generalizes the logic.  Itâ??s just that some particularizations are more usefully applied in certain situations than the general theory.  Do I have that right?  If so, that really helps a lot and is an apt analogy.</p>
<p>Trent</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Stephen Fogdall</title>
		<link>http://certaindoubts.com/defeasible-consequences/#comment-1485</link>
		<dc:creator><![CDATA[Stephen Fogdall]]></dc:creator>
		<pubDate>Fri, 11 Mar 2005 14:40:13 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=244#comment-1485</guid>
		<description><![CDATA[Jim, on your post #35, granted you can define measures on infinite sets, but the problem Trent and I are worried just is the uniqueness problem.  It&#039;s hard to rest comfortably with the fact that there is no principled way to settle on a measure.  It suggests that the possible worlds framework can only be of very limited usefulness in explicating our pretheoretic conception of evidential support.  That being said, I do acknowledge that it is somewhat illuminating to say that if A is evidence for B then, generally speaking, B is true in most worlds in which A is true (&quot;genereally speaking,&quot; because I think you probably have to take account of relevance and other factors; not everything is evidence for a necessary truth; it is possible to have evidence for a necessary falsehood, etc.).

Trent, on your post #33, it&#039;s not so clear to me that the OED definition &quot;the quality or condition of being evident; clearness, evidentness&quot; is degree-theoretic.  In fact, on a certain reading it isn&#039;t.  I do believe you&#039;re right that our pretheoretic thinking recognizes degrees of belief, as well as full beliefs that are held more or less confidently than others.  But I think it also allows one to infer a full belief in certain circumstances where the probability of that belief&#039;s being true, given one&#039;s evidence, is quite a bit less than 1.]]></description>
		<content:encoded><![CDATA[<p>Jim, on your post #35, granted you can define measures on infinite sets, but the problem Trent and I are worried just is the uniqueness problem.  It&#8217;s hard to rest comfortably with the fact that there is no principled way to settle on a measure.  It suggests that the possible worlds framework can only be of very limited usefulness in explicating our pretheoretic conception of evidential support.  That being said, I do acknowledge that it is somewhat illuminating to say that if A is evidence for B then, generally speaking, B is true in most worlds in which A is true (&#8220;genereally speaking,&#8221; because I think you probably have to take account of relevance and other factors; not everything is evidence for a necessary truth; it is possible to have evidence for a necessary falsehood, etc.).</p>
<p>Trent, on your post #33, it&#8217;s not so clear to me that the OED definition &#8220;the quality or condition of being evident; clearness, evidentness&#8221; is degree-theoretic.  In fact, on a certain reading it isn&#8217;t.  I do believe you&#8217;re right that our pretheoretic thinking recognizes degrees of belief, as well as full beliefs that are held more or less confidently than others.  But I think it also allows one to infer a full belief in certain circumstances where the probability of that belief&#8217;s being true, given one&#8217;s evidence, is quite a bit less than 1.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Jim Hawthorne</title>
		<link>http://certaindoubts.com/defeasible-consequences/#comment-1484</link>
		<dc:creator><![CDATA[Jim Hawthorne]]></dc:creator>
		<pubDate>Fri, 11 Mar 2005 09:06:05 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=244#comment-1484</guid>
		<description><![CDATA[Trent, I agree with you that may learn a lot from Weirich&#039;s work about how to, as you say, find the sweet spot where we can expect normal adult human agents to perform. But I&#039;m not working at the level of adult human agents yet.

Trent and Stephen, one can define measures on infinite sets (even of uncountable cardinalities) of possible worlds -- no problem. It&#039;s just that such measures are not unique. Mathematicians define measures on uncountable sets all the time. But for any given uncountable set, there will be lots of different measures possible, and no one will necessarily be &quot;more natural&quot; than any other. When these measures are probabilities, they may be thought of as defined on &quot;proportions&quot;, relative to the specified measure. Thus, I think it makes perfectly good sense to think of probability functions as measures on possible worlds (and to think of each sentence as representing a set of possible worlds). The Popper functions, which take conditional probability as primitive, seem especially well suited to be interpreted this way. But there are many distinct probability functions -- many distinct ways of assigning a measure to possible worlds. So prior probabilities will not be &quot;uniquely given&quot;. Still, each Popper function may be thought of as a possible degree of entailment relation, relative to some way of assigning meanings to sentences and relative to some measure on worlds.

Similarly, there is no unique defeasible consequence relation. Formally the defeasible consequence relations are like probability functions (like Popper functions). There are many such consequence relations that satisfy the formal axioms. Each is relative to a way of assigning meanings to sentences and to a measure on the prominence of some possibilities (possible worlds) among all possibilities (possible worlds). The idea is somewhat analogous to Lewis&#039;s account of counterfactuals. Lewis doesn&#039;t think there is any single, &quot;uniquely given&quot; closeness measure, but that there are lots of different closeness measures among worlds, and that in appropriate contexts we may draw on an appropriate measure, and that counterfactuals are true or false relative to the measure appropriate to the context. The measures on worlds for defeasible consequence are not &quot;closeness measures&quot;. They are &quot;measures of proportionality&quot;. But they are not uniquely given by the sizes of the sets of worlds. They are &quot;imposed&quot; in much the way Lewis&#039;s closeness measures are &quot;imposed&quot;.

Trent, with regard to your third question, on why not just stick with Bayesian (probabilistic) degrees of confidence (or degrees of belief), here are several shots at it.

1. For some artificial intelligence systems it might be reasonable to build in a defeasible reasoning component that doesn&#039;t involve a complete Bayesian probability function. It may not be practical or feasible to build into the system a probability model that specifies unique degree of belief for each propositions in the systems vocabulary -- and doing so might be overkill, given what the system is designed to do. Still, one might think that the logic of the defeasible consequence relation should be a logic that can in principle be modeled probabilistically, because that may be how one thinks the logic of belief should work in general.

2. Your question is a bit like the following question. Once one has a possible worlds semantics for modal logic, and if one thinks that the semantics is a proper rendering of the modalities, and not a mere model theory for it, why not just translate all modal talk into possible worlds talk in the object language, and then throw away the modal logic? This is in effect what David Lewis does. He says, here is how to translate modal talk into possible worlds talk -- not let&#039;s just talk possible worlds talk in a language for first-order logic, and drop the modalities. After all, the first-order language that directly quantifies over worlds completely captures the modal logic, and is more expressive. (I only intend this as an anology -- there may be good reason not to buy Lewis&#039;s metaphysics, and so to want to maintain a language with modal operators and the corresponding modal logic.) I am quite sympathetic to this kind of move (or would be if I bought Lewis&#039;s metaphysics). But even Lewis recognizes that we use modal talk pretty naturally (we tend to think in those terms), and aren&#039;t likely to just through it away. So we can either continually translate into the possible worlds representation, and do all of our logic there, or we can have a modal logic that is faithful to the deeper possible worlds analysis, and often just do logic at that higher level. I think that much the same applies to the logic of defeasible belief that has a deeper underlying probabilistic semantics.

3. It may be that for some (or many) purposes in epistemology the deeper probabilistic analysis is overkill, and a logic of belief that works like probability at a threshold may provide a more digestable analysis. In any case, belief talk is common, and we often think in terms of belief, so we may want to see what its logic should be like if it rests on the deeper conception of degrees of confidence (or on a &quot;believes more strongly than&quot; relation). I think that such a logic, with a probabilistic semantics, gives us some insights into the preface and lottery &quot;paradoxes&quot;, for example.]]></description>
		<content:encoded><![CDATA[<p>Trent, I agree with you that may learn a lot from Weirich&#8217;s work about how to, as you say, find the sweet spot where we can expect normal adult human agents to perform. But I&#8217;m not working at the level of adult human agents yet.</p>
<p>Trent and Stephen, one can define measures on infinite sets (even of uncountable cardinalities) of possible worlds &#8212; no problem. It&#8217;s just that such measures are not unique. Mathematicians define measures on uncountable sets all the time. But for any given uncountable set, there will be lots of different measures possible, and no one will necessarily be &#8220;more natural&#8221; than any other. When these measures are probabilities, they may be thought of as defined on &#8220;proportions&#8221;, relative to the specified measure. Thus, I think it makes perfectly good sense to think of probability functions as measures on possible worlds (and to think of each sentence as representing a set of possible worlds). The Popper functions, which take conditional probability as primitive, seem especially well suited to be interpreted this way. But there are many distinct probability functions &#8212; many distinct ways of assigning a measure to possible worlds. So prior probabilities will not be &#8220;uniquely given&#8221;. Still, each Popper function may be thought of as a possible degree of entailment relation, relative to some way of assigning meanings to sentences and relative to some measure on worlds.</p>
<p>Similarly, there is no unique defeasible consequence relation. Formally the defeasible consequence relations are like probability functions (like Popper functions). There are many such consequence relations that satisfy the formal axioms. Each is relative to a way of assigning meanings to sentences and to a measure on the prominence of some possibilities (possible worlds) among all possibilities (possible worlds). The idea is somewhat analogous to Lewis&#8217;s account of counterfactuals. Lewis doesn&#8217;t think there is any single, &#8220;uniquely given&#8221; closeness measure, but that there are lots of different closeness measures among worlds, and that in appropriate contexts we may draw on an appropriate measure, and that counterfactuals are true or false relative to the measure appropriate to the context. The measures on worlds for defeasible consequence are not &#8220;closeness measures&#8221;. They are &#8220;measures of proportionality&#8221;. But they are not uniquely given by the sizes of the sets of worlds. They are &#8220;imposed&#8221; in much the way Lewis&#8217;s closeness measures are &#8220;imposed&#8221;.</p>
<p>Trent, with regard to your third question, on why not just stick with Bayesian (probabilistic) degrees of confidence (or degrees of belief), here are several shots at it.</p>
<p>1. For some artificial intelligence systems it might be reasonable to build in a defeasible reasoning component that doesn&#8217;t involve a complete Bayesian probability function. It may not be practical or feasible to build into the system a probability model that specifies unique degree of belief for each propositions in the systems vocabulary &#8212; and doing so might be overkill, given what the system is designed to do. Still, one might think that the logic of the defeasible consequence relation should be a logic that can in principle be modeled probabilistically, because that may be how one thinks the logic of belief should work in general.</p>
<p>2. Your question is a bit like the following question. Once one has a possible worlds semantics for modal logic, and if one thinks that the semantics is a proper rendering of the modalities, and not a mere model theory for it, why not just translate all modal talk into possible worlds talk in the object language, and then throw away the modal logic? This is in effect what David Lewis does. He says, here is how to translate modal talk into possible worlds talk &#8212; not let&#8217;s just talk possible worlds talk in a language for first-order logic, and drop the modalities. After all, the first-order language that directly quantifies over worlds completely captures the modal logic, and is more expressive. (I only intend this as an anology &#8212; there may be good reason not to buy Lewis&#8217;s metaphysics, and so to want to maintain a language with modal operators and the corresponding modal logic.) I am quite sympathetic to this kind of move (or would be if I bought Lewis&#8217;s metaphysics). But even Lewis recognizes that we use modal talk pretty naturally (we tend to think in those terms), and aren&#8217;t likely to just through it away. So we can either continually translate into the possible worlds representation, and do all of our logic there, or we can have a modal logic that is faithful to the deeper possible worlds analysis, and often just do logic at that higher level. I think that much the same applies to the logic of defeasible belief that has a deeper underlying probabilistic semantics.</p>
<p>3. It may be that for some (or many) purposes in epistemology the deeper probabilistic analysis is overkill, and a logic of belief that works like probability at a threshold may provide a more digestable analysis. In any case, belief talk is common, and we often think in terms of belief, so we may want to see what its logic should be like if it rests on the deeper conception of degrees of confidence (or on a &#8220;believes more strongly than&#8221; relation). I think that such a logic, with a probabilistic semantics, gives us some insights into the preface and lottery &#8220;paradoxes&#8221;, for example.</p>
]]></content:encoded>
	</item>
</channel>
</rss>
