<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	>
<channel>
	<title>Comments on: Why do we recognize induction as a category?</title>
	<atom:link href="http://certaindoubts.com/why-do-we-recognize-induction-as-a-category/feed/" rel="self" type="application/rss+xml" />
	<link>http://certaindoubts.com/why-do-we-recognize-induction-as-a-category/</link>
	<description>devoted to matters epistemic</description>
	<lastBuildDate>Wed, 10 Apr 2019 16:37:28 +0000</lastBuildDate>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.10</generator>
	<item>
		<title>By: Claudio</title>
		<link>http://certaindoubts.com/why-do-we-recognize-induction-as-a-category/#comment-30403</link>
		<dc:creator><![CDATA[Claudio]]></dc:creator>
		<pubDate>Sun, 03 Jun 2012 11:19:47 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=3461#comment-30403</guid>
		<description><![CDATA[“Case in point: some years ago my son asked me where the milk was. “Probably in the fridge,” I said. He opened the door to the fridge, saw the milk, and said, “yes, you were right.” Discovering the milk would not have supported his (correct!) judgment if my claim had been a probability statement. My rule of thumb: in most cases, `probably’ functions as a conclusion indicator in the context of an argument.”


Hi, Andrew,


I’m a bit mystified by what you wrote above. I’d say that you did make a probability claim: a claim of epistemic probability. You were telling your son (though maybe you didn’t mean to) that, for all you knew (given the evidence available to you), the milk was in the fridge. Upon finding the milk, he confirmed to you that your evidence was not misleading (though maybe that was not quite what he meant to do). The relevant fact in the exchange, however, was that he did find the milk – not quite the fact that you hadn’t been misled by the evidence. I don’t see how that contradicts the thought that what you did do was give him a piece of information that was ostensibly about your mental life. And what he did do was infer that the milk probably (as a matter of objective fact) was where you had evidence to believe it was. Or maybe he “jumped” to the conclusion that it was where you had evidence to believe it was. Still, what he did say, in reply, was that your evidence was leading to true belief. He was elliptical, though, in saying simply that your belief was true.


Is there something I’m missing here?]]></description>
		<content:encoded><![CDATA[<p>“Case in point: some years ago my son asked me where the milk was. “Probably in the fridge,” I said. He opened the door to the fridge, saw the milk, and said, “yes, you were right.” Discovering the milk would not have supported his (correct!) judgment if my claim had been a probability statement. My rule of thumb: in most cases, `probably’ functions as a conclusion indicator in the context of an argument.”</p>
<p>Hi, Andrew,</p>
<p>I’m a bit mystified by what you wrote above. I’d say that you did make a probability claim: a claim of epistemic probability. You were telling your son (though maybe you didn’t mean to) that, for all you knew (given the evidence available to you), the milk was in the fridge. Upon finding the milk, he confirmed to you that your evidence was not misleading (though maybe that was not quite what he meant to do). The relevant fact in the exchange, however, was that he did find the milk – not quite the fact that you hadn’t been misled by the evidence. I don’t see how that contradicts the thought that what you did do was give him a piece of information that was ostensibly about your mental life. And what he did do was infer that the milk probably (as a matter of objective fact) was where you had evidence to believe it was. Or maybe he “jumped” to the conclusion that it was where you had evidence to believe it was. Still, what he did say, in reply, was that your evidence was leading to true belief. He was elliptical, though, in saying simply that your belief was true.</p>
<p>Is there something I’m missing here?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Mark Nelson</title>
		<link>http://certaindoubts.com/why-do-we-recognize-induction-as-a-category/#comment-29826</link>
		<dc:creator><![CDATA[Mark Nelson]]></dc:creator>
		<pubDate>Thu, 17 May 2012 22:08:30 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=3461#comment-29826</guid>
		<description><![CDATA[Once again, folks, thanks for these comments.  I don&#039;t have enough expertise to improve on (let alone _contradict_) your comments, but I am finding them interesting and helpful all the same!]]></description>
		<content:encoded><![CDATA[<p>Once again, folks, thanks for these comments.  I don&#8217;t have enough expertise to improve on (let alone _contradict_) your comments, but I am finding them interesting and helpful all the same!</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/why-do-we-recognize-induction-as-a-category/#comment-29804</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Thu, 17 May 2012 13:06:33 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=3461#comment-29804</guid>
		<description><![CDATA[@Luis Rosa: Hempel remarks (in Philosophy of Natural Science, even, if memory serves) on the difference between uncertainty attaching itself to the proposition (e.g., P, r% of Ps are Q&#039;s, therefore Q is at least as likely as r%) and uncertainty attaching itself to the consequence operator (e.g., P, r% of Ps are Q&#039;s, therefore [to degree .r] Q). Keynes tried fleshing out the latter idea, which didn&#039;t work out that well. (See Ramsey, F. P.)  Kyburg worked out another version built on a scaffolding supplied by Fisher.  Statistical default logic is a distillation of this approach. 

Thus, @Jonathan Livengood, I am sympathetic to Norton&#039;s position. 

Another source of (non-probablistic) examples may be found in the computer science Knowledge Representation and Reasoning (KRR) literature exercising the distinction between &quot;open worlds&quot; and &quot;closed worlds&quot;, and arguments/motivations for treating &quot;negation as failure&quot; is another keyword source. Not everything there is compelling; the 80&#039;s and 90&#039;s literature on &quot;commonsense reasoning&quot; is naive. But, more recent work to do on implementations of systems for the semantic web or other inference engines for large databases provide more persuasive (even if dry) examples, and seeing instances of concrete worry provides room for looking back at the commonsense reasoning literature with a bit more charity.]]></description>
		<content:encoded><![CDATA[<p>@Luis Rosa: Hempel remarks (in Philosophy of Natural Science, even, if memory serves) on the difference between uncertainty attaching itself to the proposition (e.g., P, r% of Ps are Q&#8217;s, therefore Q is at least as likely as r%) and uncertainty attaching itself to the consequence operator (e.g., P, r% of Ps are Q&#8217;s, therefore [to degree .r] Q). Keynes tried fleshing out the latter idea, which didn&#8217;t work out that well. (See Ramsey, F. P.)  Kyburg worked out another version built on a scaffolding supplied by Fisher.  Statistical default logic is a distillation of this approach. </p>
<p>Thus, @Jonathan Livengood, I am sympathetic to Norton&#8217;s position. </p>
<p>Another source of (non-probablistic) examples may be found in the computer science Knowledge Representation and Reasoning (KRR) literature exercising the distinction between &#8220;open worlds&#8221; and &#8220;closed worlds&#8221;, and arguments/motivations for treating &#8220;negation as failure&#8221; is another keyword source. Not everything there is compelling; the 80&#8217;s and 90&#8217;s literature on &#8220;commonsense reasoning&#8221; is naive. But, more recent work to do on implementations of systems for the semantic web or other inference engines for large databases provide more persuasive (even if dry) examples, and seeing instances of concrete worry provides room for looking back at the commonsense reasoning literature with a bit more charity.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Priyedarshi Jetli</title>
		<link>http://certaindoubts.com/why-do-we-recognize-induction-as-a-category/#comment-29783</link>
		<dc:creator><![CDATA[Priyedarshi Jetli]]></dc:creator>
		<pubDate>Thu, 17 May 2012 04:56:26 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=3461#comment-29783</guid>
		<description><![CDATA[Despite agreement with the responses here I am quite sympathetic to the idea of thinking of inductive inferences as deductive (though this may just be a label).  I think perhaps a conditional statement could be built into the premises which assigns a quantitative probability and the conclusion has the same quantitative probability.  Taking Lewis&#039;s example:
1. The first 999 crows I saw from t1 to t999 were black.
2. The probability of all crows being black is .99999.
Therefore, 3. The crow I see at t1000 has a .99999 probability of being black.
Now, Lewis&#039;s addition does not seem to make this argument invalid:
1. The first 999 crows I saw from t1 to t999 were black.
2. The probability of all crows being black is .99999.
3. The next 1,000,000 crows I see from t1000 to t1,000,000 will be white
4. Therefore, 4. The crow I see at t1000 has .99999 probability of being black. 
I think there is nothing wrong with the deductive validity of this argument.  It is probably not sound.  2. is an independent premise, and this probability cannot be established deductively within the argument but outside.  The probability could even be one as in pulling out a black ball from an urn containing 5 black balls.  The probability could also be given in a range hence introducing vagueness, yet we could have a deductive inference.  I think bringing in a quantitative probability premise and tense predicates we may express traditional inductive inferences as deductive ones.

Priyedarshi Jetli]]></description>
		<content:encoded><![CDATA[<p>Despite agreement with the responses here I am quite sympathetic to the idea of thinking of inductive inferences as deductive (though this may just be a label).  I think perhaps a conditional statement could be built into the premises which assigns a quantitative probability and the conclusion has the same quantitative probability.  Taking Lewis&#8217;s example:<br />
1. The first 999 crows I saw from t1 to t999 were black.<br />
2. The probability of all crows being black is .99999.<br />
Therefore, 3. The crow I see at t1000 has a .99999 probability of being black.<br />
Now, Lewis&#8217;s addition does not seem to make this argument invalid:<br />
1. The first 999 crows I saw from t1 to t999 were black.<br />
2. The probability of all crows being black is .99999.<br />
3. The next 1,000,000 crows I see from t1000 to t1,000,000 will be white<br />
4. Therefore, 4. The crow I see at t1000 has .99999 probability of being black.<br />
I think there is nothing wrong with the deductive validity of this argument.  It is probably not sound.  2. is an independent premise, and this probability cannot be established deductively within the argument but outside.  The probability could even be one as in pulling out a black ball from an urn containing 5 black balls.  The probability could also be given in a range hence introducing vagueness, yet we could have a deductive inference.  I think bringing in a quantitative probability premise and tense predicates we may express traditional inductive inferences as deductive ones.</p>
<p>Priyedarshi Jetli</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Luis Rosa</title>
		<link>http://certaindoubts.com/why-do-we-recognize-induction-as-a-category/#comment-29769</link>
		<dc:creator><![CDATA[Luis Rosa]]></dc:creator>
		<pubDate>Wed, 16 May 2012 14:39:36 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=3461#comment-29769</guid>
		<description><![CDATA[One thought. Just as you can think that an argument of the form:

&lt;b&gt;99% of A&#039;s are B&#039;s 
Therefore
the next A is also a B&lt;/b&gt;

can be rewritten, with no loss of any formal property, as:

&lt;b&gt;99% of A&#039;s are B&#039;s 
therefore
&lt;i&gt;probably&lt;/i&gt; the next A is also a B,&lt;/b&gt;

you can also think that an argument of the form:

&lt;b&gt;P&#038;Q
therefore
P&lt;/b&gt;

can be rewritten as:

&lt;b&gt;P&#038;Q
therefore
&lt;i&gt;necessarily&lt;/i&gt; P&lt;/b&gt;

So, the probably/necessarily attribute indicates what kind of vero-functional support there is between premises and conclusion - and that could mean they imply two different kinds of inferential patterns.]]></description>
		<content:encoded><![CDATA[<p>One thought. Just as you can think that an argument of the form:</p>
<p><b>99% of A&#8217;s are B&#8217;s<br />
Therefore<br />
the next A is also a B</b></p>
<p>can be rewritten, with no loss of any formal property, as:</p>
<p><b>99% of A&#8217;s are B&#8217;s<br />
therefore<br />
<i>probably</i> the next A is also a B,</b></p>
<p>you can also think that an argument of the form:</p>
<p><b>P&amp;Q<br />
therefore<br />
P</b></p>
<p>can be rewritten as:</p>
<p><b>P&amp;Q<br />
therefore<br />
<i>necessarily</i> P</b></p>
<p>So, the probably/necessarily attribute indicates what kind of vero-functional support there is between premises and conclusion &#8211; and that could mean they imply two different kinds of inferential patterns.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Jonathan Livengood</title>
		<link>http://certaindoubts.com/why-do-we-recognize-induction-as-a-category/#comment-29735</link>
		<dc:creator><![CDATA[Jonathan Livengood]]></dc:creator>
		<pubDate>Tue, 15 May 2012 09:02:28 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=3461#comment-29735</guid>
		<description><![CDATA[Two thoughts:

1. Characteristically inductive arguments take us from samples to populations (as GRW suggests).  Often, we add two features to the conclusions of such inferences -- some vagueness and some probability.  For example, in the school-sampling case, we might look at a few children and then say that with probability p, the average height of the children is between h1 and h2.  But those two features need not be added in order to have the *form* of an induction.  You could simply conclude with a point estimate: &quot;The average height of the children is h.&quot;  (The case is parallel to singular statistical syllogisms, like: 90% of the balls in the bucket are red; this ball was drawn from the bucket; therefore, this ball is red.)

2. Some people have claimed that there are inductive arguments with conclusions that cannot be assigned any probability.  Peirce seems to have thought this about some inductive arguments with respect to infinite populations and with respect to extrapolations to the future.  John Norton gives a general argument against probability as the one true logic of induction.  (See &lt;a href=&quot;http://www.pitt.edu/~jdnorton/papers/IwoP.pdf&quot; rel=&quot;nofollow&quot;&gt;this paper (pdf)&lt;/a&gt;, for example.)  If their arguments are right, then induction is not likely to be replaceable with deduction plus a probability function.]]></description>
		<content:encoded><![CDATA[<p>Two thoughts:</p>
<p>1. Characteristically inductive arguments take us from samples to populations (as GRW suggests).  Often, we add two features to the conclusions of such inferences &#8212; some vagueness and some probability.  For example, in the school-sampling case, we might look at a few children and then say that with probability p, the average height of the children is between h1 and h2.  But those two features need not be added in order to have the *form* of an induction.  You could simply conclude with a point estimate: &#8220;The average height of the children is h.&#8221;  (The case is parallel to singular statistical syllogisms, like: 90% of the balls in the bucket are red; this ball was drawn from the bucket; therefore, this ball is red.)</p>
<p>2. Some people have claimed that there are inductive arguments with conclusions that cannot be assigned any probability.  Peirce seems to have thought this about some inductive arguments with respect to infinite populations and with respect to extrapolations to the future.  John Norton gives a general argument against probability as the one true logic of induction.  (See <a href="http://www.pitt.edu/~jdnorton/papers/IwoP.pdf" rel="nofollow">this paper (pdf)</a>, for example.)  If their arguments are right, then induction is not likely to be replaceable with deduction plus a probability function.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/why-do-we-recognize-induction-as-a-category/#comment-29654</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Sun, 13 May 2012 11:45:59 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=3461#comment-29654</guid>
		<description><![CDATA[Dear Mark,

The non-monotonic reasoning literature is a rich one, dating from the 1970s and christened in an AI journal special issue from 1980. But, for a (non-technical) reference which anticipates the idea of a logic for non-demonstrative inference, have a look at (Fisher 1922, 1936), referenced and discussed (briefly) in this &lt;a href=&quot;http://gregorywheeler.org/papers/LotteryReview.pdf&quot; rel=&quot;nofollow&quot;&gt;review of the lottery paradox&lt;/a&gt;.

Best, GRW]]></description>
		<content:encoded><![CDATA[<p>Dear Mark,</p>
<p>The non-monotonic reasoning literature is a rich one, dating from the 1970s and christened in an AI journal special issue from 1980. But, for a (non-technical) reference which anticipates the idea of a logic for non-demonstrative inference, have a look at (Fisher 1922, 1936), referenced and discussed (briefly) in this <a href="http://gregorywheeler.org/papers/LotteryReview.pdf" rel="nofollow">review of the lottery paradox</a>.</p>
<p>Best, GRW</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Andrew Cling</title>
		<link>http://certaindoubts.com/why-do-we-recognize-induction-as-a-category/#comment-29646</link>
		<dc:creator><![CDATA[Andrew Cling]]></dc:creator>
		<pubDate>Sun, 13 May 2012 01:52:03 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=3461#comment-29646</guid>
		<description><![CDATA[In most cases it seems that the conclusions of our inferences are about non-probabilistic states of affairs, not about the probabilities of propositions. Case in point: some years ago my son asked me where the milk was. &quot;Probably in the fridge,&quot; I said. He opened the door to the fridge, saw the milk, and said, &quot;yes, you were right.&quot; Discovering the milk would not have supported his (correct!) judgment if my claim had been a probability statement. My rule of thumb: in most cases, `probably&#039; functions as a conclusion indicator in the context of an argument.]]></description>
		<content:encoded><![CDATA[<p>In most cases it seems that the conclusions of our inferences are about non-probabilistic states of affairs, not about the probabilities of propositions. Case in point: some years ago my son asked me where the milk was. &#8220;Probably in the fridge,&#8221; I said. He opened the door to the fridge, saw the milk, and said, &#8220;yes, you were right.&#8221; Discovering the milk would not have supported his (correct!) judgment if my claim had been a probability statement. My rule of thumb: in most cases, `probably&#8217; functions as a conclusion indicator in the context of an argument.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: nelson</title>
		<link>http://certaindoubts.com/why-do-we-recognize-induction-as-a-category/#comment-29608</link>
		<dc:creator><![CDATA[nelson]]></dc:creator>
		<pubDate>Fri, 11 May 2012 17:51:37 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=3461#comment-29608</guid>
		<description><![CDATA[Richard, Lewis, Randy, Gregory, Colin &#038; Jon,

Thanks for those comments.   The overall point about non-monotonicity is helpful and (to me) convincing.  Just the sort of thing I was hoping for!

Mark]]></description>
		<content:encoded><![CDATA[<p>Richard, Lewis, Randy, Gregory, Colin &amp; Jon,</p>
<p>Thanks for those comments.   The overall point about non-monotonicity is helpful and (to me) convincing.  Just the sort of thing I was hoping for!</p>
<p>Mark</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Jon Kvanvig</title>
		<link>http://certaindoubts.com/why-do-we-recognize-induction-as-a-category/#comment-29598</link>
		<dc:creator><![CDATA[Jon Kvanvig]]></dc:creator>
		<pubDate>Fri, 11 May 2012 13:46:38 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=3461#comment-29598</guid>
		<description><![CDATA[Mark, I&#039;m with Lewis on this one.  To put it another way, a deductive system is monotonic:  add anything to the premises and the conclusion still follows if it did prior to the addition.  Inductive inferences are non-monotonic, as Lewis&#039;s example shows.  So the logics are quite different.]]></description>
		<content:encoded><![CDATA[<p>Mark, I&#8217;m with Lewis on this one.  To put it another way, a deductive system is monotonic:  add anything to the premises and the conclusion still follows if it did prior to the addition.  Inductive inferences are non-monotonic, as Lewis&#8217;s example shows.  So the logics are quite different.</p>
]]></content:encoded>
	</item>
</channel>
</rss>
