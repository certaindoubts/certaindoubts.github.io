<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	>
<channel>
	<title>Comments on: Rational Acceptance and Conjunctive/Disjunctive Absorption</title>
	<atom:link href="http://certaindoubts.com/rational-acceptance-and-conjunctivedisjunctive-absorption/feed/" rel="self" type="application/rss+xml" />
	<link>http://certaindoubts.com/rational-acceptance-and-conjunctivedisjunctive-absorption/</link>
	<description>devoted to matters epistemic</description>
	<lastBuildDate>Wed, 10 Apr 2019 16:37:28 +0000</lastBuildDate>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.10</generator>
	<item>
		<title>By: Stephen Fogdall</title>
		<link>http://certaindoubts.com/rational-acceptance-and-conjunctivedisjunctive-absorption/#comment-1581</link>
		<dc:creator><![CDATA[Stephen Fogdall]]></dc:creator>
		<pubDate>Wed, 06 Apr 2005 04:50:01 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=282#comment-1581</guid>
		<description><![CDATA[Greg,

I agree with much of your last post, assuming I&#039;ve understood it correctly.
(But I do wish my background allowed me to engage with your work more completely.)

I take your point re Carnap to be that the systems of inductive logic he explored were not &quot;structured&quot; in your sense.  I think this is right.  To put it in the simplest terms, measuring degree of confirmation in itself is not a theory of acceptance at all.  A true theory of acceptance, it seems, must be structured in your sense.  (Putting the point in the carnapian way as I did was not so much to endorse Carnap as simply to illustrate how probabilities might be objective, yet noncomparable.)

I actually think that your desire for a structured approach is related to the search for a &quot;rule of detachment&quot; that I mentioned earlier.  A system of inductive logic based on a rule of detachment would be structured in your sense because it would allow for the syntactic manipulation of accepted propositions that you want to capture.  But, as you point out, the rule of detachment in its strong form &quot;throws away perfectly good information to get a set of propositions&quot; and cannot account for depletion in confidence.  Thus, your bounded formula approach is a way of trying to capture the virtues of that rule of without its drawbacks.]]></description>
		<content:encoded><![CDATA[<p>Greg,</p>
<p>I agree with much of your last post, assuming I&#8217;ve understood it correctly.<br />
(But I do wish my background allowed me to engage with your work more completely.)</p>
<p>I take your point re Carnap to be that the systems of inductive logic he explored were not &#8220;structured&#8221; in your sense.  I think this is right.  To put it in the simplest terms, measuring degree of confirmation in itself is not a theory of acceptance at all.  A true theory of acceptance, it seems, must be structured in your sense.  (Putting the point in the carnapian way as I did was not so much to endorse Carnap as simply to illustrate how probabilities might be objective, yet noncomparable.)</p>
<p>I actually think that your desire for a structured approach is related to the search for a &#8220;rule of detachment&#8221; that I mentioned earlier.  A system of inductive logic based on a rule of detachment would be structured in your sense because it would allow for the syntactic manipulation of accepted propositions that you want to capture.  But, as you point out, the rule of detachment in its strong form &#8220;throws away perfectly good information to get a set of propositions&#8221; and cannot account for depletion in confidence.  Thus, your bounded formula approach is a way of trying to capture the virtues of that rule of without its drawbacks.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/rational-acceptance-and-conjunctivedisjunctive-absorption/#comment-1580</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Tue, 05 Apr 2005 16:46:25 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=282#comment-1580</guid>
		<description><![CDATA[Steve,

Thanks for your excellent comments. I&#039;m not sure how well I can address them, but I&#039;ll give it a try.

My thoughts on bounded formula are influenced by how basic inferential statistical reasoning treats hypothesis testing. And I&#039;ve extended this--building on work by Kyburg and Teng --to generalize basic forms of inferential statistical reasoning. Now I am working on extending this to model rational acceptance. My working conjecture is that statistical hypothesis testing is a special case of rational acceptance, namely the hyper-articulated case. The idea is that rough logical forms of statistical reasoning would give us a rough logical form of rational acceptance. One property of both is that confidence depletes. Let me look at hypothesis testing now to point out some other properties that might help in addressing your comments.

My favorite pedagogical example is Fisher&#039;s tea testing example: A woman claims to have the ability to determine, by taste alone, whether milk was added first or last to her tea. The hypothesis we wish to test (Lady has the ability) may be evaluated by comparing her performance of correct classification (pre- or post-lactated tea) for a sequence of trials (cups of tea) to the probability of a similar number of correct classifications by chance. Suppose we set a trial of 5 cups and we plan to `accept&#039; that she has the ability if she correctly classifies all 5 cups, since she stands a 1/32 odds of picking the correct sequence by chance and this chance of error is, observing convention, less than 0.05. Suppose she does it: that is, she correctly classifies all 5. Does she have the ability to discriminate pre-lactated tea from post-lactated tea?

Well, this depends on whether the experiment was properly controlled. If she sat watching you prepare the cups, her classification wouldn&#039;t be evidence for squat. So you might want to make sure that the samples are prepared out of her view, stirred properly, presented to her blindly by an assistant, that she wear a blindfold...this kind of thing. The point is, our confidence that she has the ability given her performance is based *not simply* on the distance of her performance (i.e., the sequence of 5 correct guesses) and the binomial probability of this sequence occurring (1/32), but that the experiment is sufficiently controlled so that this distance is reasonable enough evidence for us to think, yeah, she can do it. (I should note that &#039;sufficiently&#039; controlled is a misnomer: these conditions are defeasible, in practice. Which brings in non-monotonicity...but let&#039;s bracket this.)

I think that putting the matter in the Carnapian terms you suggested (para 3, post 7) masks this behavior that I&#039;ve spelled out in Fisher&#039;s example: For we would just start by saying that c(woman doesn&#039;t have ability, 5 correct guesses) =  1/32, and move from there.

I&#039;m reluctant to do this, since I think it completely fouls up the connection between probability and evidence. I think I understand the Tea-testing example and the role that my background knowledge plays in seeing that the experiment gives good evidence for my accepting the claim. I am inclined to see the confidence associated with rejecting the null hypothesis (one minus the probability of committing a type 2 error) as the lower-bound probability of accepting that, yes, the woman has the ability. I believe that a statistician would swallow the rough picture I&#039;ve laid out so far, except the very last part about accepting the hypothesis that she has the ability. But, the defeasibility discussion I&#039;ve bracketed is designed to address the thrust of this objection, as well. And, besides, I need some positive value to move around if I&#039;m going to talk about how it a formal language for rational accepted propositions is supposed to behave.

To some of your points:

(1) &lt;i&gt;Are the probabilities in the Tea testing example &#039;objective&#039;?&lt;/i&gt; We&#039;re comparing her performance to a theorem of the calculus of probabilities and what we know about the situation at hand to determine whether the salient properties of this theorem &#039;apply&#039; to the case at hand. We are not selecting probabilities out of thin air; our goal, our intent, and our confidence in our evidence is predicated on this experiment showing that it would be astonishing to see these results if she were guessing.

(2) &lt;i&gt;Are the probabilities in the Tea testing example &#039;objective&#039;?(!)&lt;/i&gt; I think that they are objective in this sense: if the experiment is properly controlled, then the confidence we assign to the claim that she can make the distinction is rooted in a fact about her performance and that we in fact stand a 1/32 chance of being wrong. Furthermore, if we do stand a chance greater than 1/32 of accepting that she can discriminate by taste but that she in fact cannot discriminate by taste, then the experiment is not well controlled or it is not well designed. I think there are facts to the matter that answer these questions, and that arguments over experimental results often are precisely about these factual matters. Some of these questions are settled in meetings; some by another experiment.

(3) &lt;i&gt;Are the probabilities in the Tea testing example &#039;objective&#039;? (!!)&lt;/i&gt; I don&#039;t know.

(4) &lt;i&gt;What&#039;s the use of this? Isn&#039;t it just turtles all the way down?&lt;/i&gt; I think that it is useful to understand the components that go into rationally accepting a proposition. That is, what I&#039;m after is a structure for rational accepted propositions that, when we&#039;re working with such items and we&#039;d like to manipulate them (conjoin, disjoin; close under consequence), we are provided with a better idea of how these various components are affected by those operations. That&#039;s what logic does, after all.

Background knowledge is very important to selecting a reference class and for determining whether or not a particular experiment is well designed or, if well designed, whether it is properly controlled. This observation is what, in part, makes so-called inductive logic so messy: it pollutes the clean distinction between logical vocabulary and non-logical vocabulary that make formal languages, well, formal.

I&#039;m realizing now, at the end of this, how rich your note is. I don&#039;t think that I&#039;ve even begun to address the substance of your points. It seems, rather, that what I&#039;ve done is to try to motivate how I hope to address them--or avoid them, as the case may be.

Greg]]></description>
		<content:encoded><![CDATA[<p>Steve,</p>
<p>Thanks for your excellent comments. I&#8217;m not sure how well I can address them, but I&#8217;ll give it a try.</p>
<p>My thoughts on bounded formula are influenced by how basic inferential statistical reasoning treats hypothesis testing. And I&#8217;ve extended this&#8211;building on work by Kyburg and Teng &#8211;to generalize basic forms of inferential statistical reasoning. Now I am working on extending this to model rational acceptance. My working conjecture is that statistical hypothesis testing is a special case of rational acceptance, namely the hyper-articulated case. The idea is that rough logical forms of statistical reasoning would give us a rough logical form of rational acceptance. One property of both is that confidence depletes. Let me look at hypothesis testing now to point out some other properties that might help in addressing your comments.</p>
<p>My favorite pedagogical example is Fisher&#8217;s tea testing example: A woman claims to have the ability to determine, by taste alone, whether milk was added first or last to her tea. The hypothesis we wish to test (Lady has the ability) may be evaluated by comparing her performance of correct classification (pre- or post-lactated tea) for a sequence of trials (cups of tea) to the probability of a similar number of correct classifications by chance. Suppose we set a trial of 5 cups and we plan to `accept&#8217; that she has the ability if she correctly classifies all 5 cups, since she stands a 1/32 odds of picking the correct sequence by chance and this chance of error is, observing convention, less than 0.05. Suppose she does it: that is, she correctly classifies all 5. Does she have the ability to discriminate pre-lactated tea from post-lactated tea?</p>
<p>Well, this depends on whether the experiment was properly controlled. If she sat watching you prepare the cups, her classification wouldn&#8217;t be evidence for squat. So you might want to make sure that the samples are prepared out of her view, stirred properly, presented to her blindly by an assistant, that she wear a blindfold&#8230;this kind of thing. The point is, our confidence that she has the ability given her performance is based *not simply* on the distance of her performance (i.e., the sequence of 5 correct guesses) and the binomial probability of this sequence occurring (1/32), but that the experiment is sufficiently controlled so that this distance is reasonable enough evidence for us to think, yeah, she can do it. (I should note that &#8216;sufficiently&#8217; controlled is a misnomer: these conditions are defeasible, in practice. Which brings in non-monotonicity&#8230;but let&#8217;s bracket this.)</p>
<p>I think that putting the matter in the Carnapian terms you suggested (para 3, post 7) masks this behavior that I&#8217;ve spelled out in Fisher&#8217;s example: For we would just start by saying that c(woman doesn&#8217;t have ability, 5 correct guesses) =  1/32, and move from there.</p>
<p>I&#8217;m reluctant to do this, since I think it completely fouls up the connection between probability and evidence. I think I understand the Tea-testing example and the role that my background knowledge plays in seeing that the experiment gives good evidence for my accepting the claim. I am inclined to see the confidence associated with rejecting the null hypothesis (one minus the probability of committing a type 2 error) as the lower-bound probability of accepting that, yes, the woman has the ability. I believe that a statistician would swallow the rough picture I&#8217;ve laid out so far, except the very last part about accepting the hypothesis that she has the ability. But, the defeasibility discussion I&#8217;ve bracketed is designed to address the thrust of this objection, as well. And, besides, I need some positive value to move around if I&#8217;m going to talk about how it a formal language for rational accepted propositions is supposed to behave.</p>
<p>To some of your points:</p>
<p>(1) <i>Are the probabilities in the Tea testing example &#8216;objective&#8217;?</i> We&#8217;re comparing her performance to a theorem of the calculus of probabilities and what we know about the situation at hand to determine whether the salient properties of this theorem &#8216;apply&#8217; to the case at hand. We are not selecting probabilities out of thin air; our goal, our intent, and our confidence in our evidence is predicated on this experiment showing that it would be astonishing to see these results if she were guessing.</p>
<p>(2) <i>Are the probabilities in the Tea testing example &#8216;objective&#8217;?(!)</i> I think that they are objective in this sense: if the experiment is properly controlled, then the confidence we assign to the claim that she can make the distinction is rooted in a fact about her performance and that we in fact stand a 1/32 chance of being wrong. Furthermore, if we do stand a chance greater than 1/32 of accepting that she can discriminate by taste but that she in fact cannot discriminate by taste, then the experiment is not well controlled or it is not well designed. I think there are facts to the matter that answer these questions, and that arguments over experimental results often are precisely about these factual matters. Some of these questions are settled in meetings; some by another experiment.</p>
<p>(3) <i>Are the probabilities in the Tea testing example &#8216;objective&#8217;? (!!)</i> I don&#8217;t know.</p>
<p>(4) <i>What&#8217;s the use of this? Isn&#8217;t it just turtles all the way down?</i> I think that it is useful to understand the components that go into rationally accepting a proposition. That is, what I&#8217;m after is a structure for rational accepted propositions that, when we&#8217;re working with such items and we&#8217;d like to manipulate them (conjoin, disjoin; close under consequence), we are provided with a better idea of how these various components are affected by those operations. That&#8217;s what logic does, after all.</p>
<p>Background knowledge is very important to selecting a reference class and for determining whether or not a particular experiment is well designed or, if well designed, whether it is properly controlled. This observation is what, in part, makes so-called inductive logic so messy: it pollutes the clean distinction between logical vocabulary and non-logical vocabulary that make formal languages, well, formal.</p>
<p>I&#8217;m realizing now, at the end of this, how rich your note is. I don&#8217;t think that I&#8217;ve even begun to address the substance of your points. It seems, rather, that what I&#8217;ve done is to try to motivate how I hope to address them&#8211;or avoid them, as the case may be.</p>
<p>Greg</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Stephen Fogdall</title>
		<link>http://certaindoubts.com/rational-acceptance-and-conjunctivedisjunctive-absorption/#comment-1579</link>
		<dc:creator><![CDATA[Stephen Fogdall]]></dc:creator>
		<pubDate>Sun, 03 Apr 2005 18:52:49 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=282#comment-1579</guid>
		<description><![CDATA[Hi Greg,

If I understand you correctly, in your view the source of noncomparability lies in the fact that if all one knows is the lower bound probability of two propositions -- (P,e) and (Q,e) -- then, given only that information, there is no way to say which of P or Q is the more probable.  All one can say is that each has a probability of at least e.  This seems right.

About Keynes:  it always seemed to me that there is a tension in his approach in that he marries objectivity with noncomparability.  I guess that is because I always just assumed (rashly) that objectivity should entail determinateness, which in turn should entail comparability.  My assumption was that if logicial probabilities are a matter of ratios among sets of possible worlds (not Keynes&#039; view, but a popular contemporary formulation), then intuitively these ratios ought to exist in every case, so that in principle one ought to be able to compare the probability of any two propositions on any given body of evidence.

On reflection, this does seem too rash an assumption.  For example, it might be that one has to use different measures for different types of proposition.  Thus, to assess the logical probability of P on E one might have to use measure M1, whereas measure M2 might be the &quot;objectively correct&quot; measure for assessing the probability of Q on E.  One would have Prob(P/E) = M1(P&amp;E)/M1(E) and Prob(Q/E) = M2(Q&amp;E)/M2(E).  In consequence, one couldn&#039;t compare the probability of P on E with the probability of Q on E, even though these probabilities are each defined for their &quot;correct&quot; measures.  Thus, as you suggest, while one has Prob(P/E) and Prob(Q/E), one doesn&#039;t have Prob(P&amp;Q/E).

However, this does raise a puzzle.  If there is one &quot;objectively correct&quot; measure for one type of proposition and another &quot;objectively correct&quot; measure for another, how useful are logical probabilities for analyzing rational belief?  What if you aren&#039;t sure which measure is the right one to use in a given case?  You&#039;re pretty sure it&#039;s measure M3 but it could also be M4 or M5.  This suggests that you&#039;d be forced to deal with epistemic probabilities about logical probabilities.  And in that case, what good are the logical probabilities?  Why not just use the epistemic probabilities if they&#039;re independently defined?  I realize this argument needs quite a bit of unpacking, but it think it does raise some interesting issues.

Steve]]></description>
		<content:encoded><![CDATA[<p>Hi Greg,</p>
<p>If I understand you correctly, in your view the source of noncomparability lies in the fact that if all one knows is the lower bound probability of two propositions &#8212; (P,e) and (Q,e) &#8212; then, given only that information, there is no way to say which of P or Q is the more probable.  All one can say is that each has a probability of at least e.  This seems right.</p>
<p>About Keynes:  it always seemed to me that there is a tension in his approach in that he marries objectivity with noncomparability.  I guess that is because I always just assumed (rashly) that objectivity should entail determinateness, which in turn should entail comparability.  My assumption was that if logicial probabilities are a matter of ratios among sets of possible worlds (not Keynes&#8217; view, but a popular contemporary formulation), then intuitively these ratios ought to exist in every case, so that in principle one ought to be able to compare the probability of any two propositions on any given body of evidence.</p>
<p>On reflection, this does seem too rash an assumption.  For example, it might be that one has to use different measures for different types of proposition.  Thus, to assess the logical probability of P on E one might have to use measure M1, whereas measure M2 might be the &#8220;objectively correct&#8221; measure for assessing the probability of Q on E.  One would have Prob(P/E) = M1(P&#038;E)/M1(E) and Prob(Q/E) = M2(Q&#038;E)/M2(E).  In consequence, one couldn&#8217;t compare the probability of P on E with the probability of Q on E, even though these probabilities are each defined for their &#8220;correct&#8221; measures.  Thus, as you suggest, while one has Prob(P/E) and Prob(Q/E), one doesn&#8217;t have Prob(P&#038;Q/E).</p>
<p>However, this does raise a puzzle.  If there is one &#8220;objectively correct&#8221; measure for one type of proposition and another &#8220;objectively correct&#8221; measure for another, how useful are logical probabilities for analyzing rational belief?  What if you aren&#8217;t sure which measure is the right one to use in a given case?  You&#8217;re pretty sure it&#8217;s measure M3 but it could also be M4 or M5.  This suggests that you&#8217;d be forced to deal with epistemic probabilities about logical probabilities.  And in that case, what good are the logical probabilities?  Why not just use the epistemic probabilities if they&#8217;re independently defined?  I realize this argument needs quite a bit of unpacking, but it think it does raise some interesting issues.</p>
<p>Steve</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/rational-acceptance-and-conjunctivedisjunctive-absorption/#comment-1578</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Fri, 01 Apr 2005 13:51:55 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=282#comment-1578</guid>
		<description><![CDATA[Hi Steve,

My previous comment was truncated: I still don&#039;t quite understand what fragment of HTML the comments script accepts, since it will preview HTML correctly but will not always post HTML correctly, and it makes a total mess of Word .doc files by the looks of how &#039; and &quot; are rendered in some posts. I had thought of putting together a guide for people composing replies with dreaded Word --the key thing is to save your document as unicode (.txt)--but I don&#039;t know what mark-up language we&#039;re working with here since I write everything here in HTML and still get buggy posts. (Although the truncation problem looks like it was my fault...)

Ah, right: Non-comparability. Let me try a historical run at it. I&#039;m cribbing here from a neat little paper of Kyburg&#039;s, &lt;i&gt;Journal of Applied Logic&lt;/i&gt; 1(2003): 139-149. Keynes had this idea that probability was an objective relation--objective in the sense that relations in a logical language are objective. Probability is not, he insisted &#039;subject to human caprice. A proposition is not probable because we think it so. When once the facts are given which determine our knowledge, what is probable or improbable in these circumstances has been fixed objectively, and is independent of our opinion...When we argue that Darwin gives valid grounds for our accepting his theory of natural selection, we do not simply mean that we are psychologically inclined to agree with  him; it is certain that we also intend to convey our belief that we are acting rationally in regarding his theory as probable. We believe that there is some real objective relation between Darwin&#039;s evidence and his conclusions...&#039;[Keynes &lt;i&gt;A Treatise on Probability&lt;/i&gt;, 4-5]

Now, Keynes does speak of &#039;degrees of rational belief&#039;, which invites one to question his commitment to objectivity. Nevertheless, he denies that all probabilities are capable of being compared--which would be a strange thing for a proto-subjectivist to hold. (cf. Ramsey). The example he cites is the insurance business and how an underwriter issues policies against almost any outcome. This practice, he writes, &#039;shows no more than that many probabilities are greater or less than some numerical measure, not that they themselves are numerically definite [p. 22]...there are some pairs of probabilities between the members of which no comparison of magnitude is possible...[we are not always in the position] to say that the degree of our rational belief in one conclusion is either equal to, greater than, or less than our degree of belief in another [p.34].

The short of it is that if you view evidential probability as an objective relation, then it is natural to view probabilities as not necessarily being comparable: just because you have a measure of an event P and a measure of an event Q doesn&#039;t mean that you&#039;ve always a measure of the joint event PQ. A Bayesian would deny this. I would suggest that if the Bayesian was right, the experimental drug business (for one) would be a whole lot cheaper. (Or if you want to insist on the metaphysics here, then the search problem becomes relevant and complexity issues are a real problem and not a minor technical issue.)

This is the underlying historical motivation for working with imprecise probabilities. What I&#039;m interested in doing with bounded formulas is to show that there are properties of the measure that we can exploit to characterize bounds for conjunctive/disjunctive expressions, even if we haven&#039;t a precise measure for each event.

Does that help?]]></description>
		<content:encoded><![CDATA[<p>Hi Steve,</p>
<p>My previous comment was truncated: I still don&#8217;t quite understand what fragment of HTML the comments script accepts, since it will preview HTML correctly but will not always post HTML correctly, and it makes a total mess of Word .doc files by the looks of how &#8216; and &#8221; are rendered in some posts. I had thought of putting together a guide for people composing replies with dreaded Word &#8211;the key thing is to save your document as unicode (.txt)&#8211;but I don&#8217;t know what mark-up language we&#8217;re working with here since I write everything here in HTML and still get buggy posts. (Although the truncation problem looks like it was my fault&#8230;)</p>
<p>Ah, right: Non-comparability. Let me try a historical run at it. I&#8217;m cribbing here from a neat little paper of Kyburg&#8217;s, <i>Journal of Applied Logic</i> 1(2003): 139-149. Keynes had this idea that probability was an objective relation&#8211;objective in the sense that relations in a logical language are objective. Probability is not, he insisted &#8216;subject to human caprice. A proposition is not probable because we think it so. When once the facts are given which determine our knowledge, what is probable or improbable in these circumstances has been fixed objectively, and is independent of our opinion&#8230;When we argue that Darwin gives valid grounds for our accepting his theory of natural selection, we do not simply mean that we are psychologically inclined to agree with  him; it is certain that we also intend to convey our belief that we are acting rationally in regarding his theory as probable. We believe that there is some real objective relation between Darwin&#8217;s evidence and his conclusions&#8230;'[Keynes <i>A Treatise on Probability</i>, 4-5]</p>
<p>Now, Keynes does speak of &#8216;degrees of rational belief&#8217;, which invites one to question his commitment to objectivity. Nevertheless, he denies that all probabilities are capable of being compared&#8211;which would be a strange thing for a proto-subjectivist to hold. (cf. Ramsey). The example he cites is the insurance business and how an underwriter issues policies against almost any outcome. This practice, he writes, &#8216;shows no more than that many probabilities are greater or less than some numerical measure, not that they themselves are numerically definite [p. 22]&#8230;there are some pairs of probabilities between the members of which no comparison of magnitude is possible&#8230;[we are not always in the position] to say that the degree of our rational belief in one conclusion is either equal to, greater than, or less than our degree of belief in another [p.34].</p>
<p>The short of it is that if you view evidential probability as an objective relation, then it is natural to view probabilities as not necessarily being comparable: just because you have a measure of an event P and a measure of an event Q doesn&#8217;t mean that you&#8217;ve always a measure of the joint event PQ. A Bayesian would deny this. I would suggest that if the Bayesian was right, the experimental drug business (for one) would be a whole lot cheaper. (Or if you want to insist on the metaphysics here, then the search problem becomes relevant and complexity issues are a real problem and not a minor technical issue.)</p>
<p>This is the underlying historical motivation for working with imprecise probabilities. What I&#8217;m interested in doing with bounded formulas is to show that there are properties of the measure that we can exploit to characterize bounds for conjunctive/disjunctive expressions, even if we haven&#8217;t a precise measure for each event.</p>
<p>Does that help?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Stephen Fogdall</title>
		<link>http://certaindoubts.com/rational-acceptance-and-conjunctivedisjunctive-absorption/#comment-1577</link>
		<dc:creator><![CDATA[Stephen Fogdall]]></dc:creator>
		<pubDate>Thu, 31 Mar 2005 19:50:11 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=282#comment-1577</guid>
		<description><![CDATA[I agree with you that the strong detachment thesis (accepting both the first and second properties) is dubious as an epistemic norm and implausible as a description of human inference.  But I do think this thesis tends to get taken for granted in many epistemological discussions.  Or rather, I think many epistemologists assume that the first property entails the second.  You are right to separate them.

I also agree with you that an adequate theory of human inference has to explain when (and why) it is appropriate to string together things we accept (if I may borrow your phrase), and when it isn&#039;t.  My own intuitive sense is that coherence or explanatory integration plays a role here (the permissible length of the string is related to its degree of integration, or something like that), but I confess I have made little effort to develop that idea.

I would be interested to hear more about how your bounded formula approach connects with the non-comparability issue.]]></description>
		<content:encoded><![CDATA[<p>I agree with you that the strong detachment thesis (accepting both the first and second properties) is dubious as an epistemic norm and implausible as a description of human inference.  But I do think this thesis tends to get taken for granted in many epistemological discussions.  Or rather, I think many epistemologists assume that the first property entails the second.  You are right to separate them.</p>
<p>I also agree with you that an adequate theory of human inference has to explain when (and why) it is appropriate to string together things we accept (if I may borrow your phrase), and when it isn&#8217;t.  My own intuitive sense is that coherence or explanatory integration plays a role here (the permissible length of the string is related to its degree of integration, or something like that), but I confess I have made little effort to develop that idea.</p>
<p>I would be interested to hear more about how your bounded formula approach connects with the non-comparability issue.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/rational-acceptance-and-conjunctivedisjunctive-absorption/#comment-1576</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Thu, 31 Mar 2005 16:41:37 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=282#comment-1576</guid>
		<description><![CDATA[Stephen,

Thank you very much for your comments. I am particularly interested in the epistemology underneath this proposal, so your observations are most welcome. My reply, in short, is that I would be inclined to reject the strong detachment thesis that you describe.

But I&#039;m not sure that doing so should be a depressing option for traditional epistemologists who might understandably be attracted to the thesis you describe. We can apply logic to beliefs that we accept with probabilistic support to get other beliefs that are supported as well. But, we should take caution when we start stringing together things we accept, leaving to one side &lt;i&gt;a priori&lt;/i&gt; warranted propositions for the sake of the argument. Roughly put, accepting p and accepting q is good but not conclusive reason for accepting p and q. A logic for rational acceptance should tell you those non-trivial cases in which acceptance of the conjunction is entailed by the acceptance of each conjunct and tell you about the cases where what you should do is go look for more evidence for the joint belief you&#039;d like to entertain. In the latter case, perhaps what one has is a hypothesis or conjecture that has some prima facie reason acceptance only. Note that both cases would nevertheless be defeasible (in the non-trivial case where theta is less than 1).

A second thought on detachment: I have played around with an idea that allowed one to treat a set of accepted statements as all on par in &#039;A resource bounded default logic&#039; (NMR 2004). But, since I start with bounded formulas it seems like I&#039;m throwing away perfectly good information in order to get a set of propositions. This might not be so bad if, for instance, you are just interested in whether a particular set of formulas is non-monotonically derivable above some threshold point. But tossing out this information completely seems to weaken iterative inference since, then, the only reasonable thing to do would be to assign the threshold value to all beliefs in that class. In most cases you would weaken your hand by doing this. (I should add, however, that the semantics for statistical default logic is flexible enough to select the &#039;best-bound&#039; for a proposition, if there happens to be more than one. Currently, we do this with Answer Set Semantics in an &#039;Implementation of SDL&#039; that Carlos Damasio and I did appearing in JELIA 2004.)

I am not too worried about the book keeping issue, either, although I can see how a critic might make mischief here. I&#039;m not claiming that we&#039;ve numbers in our heads; Indeed, I find the idea that we actually have degrees of belief as persuasive as thinking that planets are actually point masses: it is to confuse the map for the terrain. Rather, giving an account that tells us, more precisely, why and when justification *may* degrade offers us the prospect of providing a good reason for when  we-- ordinary but otherwise rational human beings--are warranted in being suspicious of a long but otherwise valid argument involving empirical premises that we accept and, perhaps more importantly, when we are not warranted in being suspicious of a complicated but otherwise valid empirical argument. It would seem that fallibilism, if we&#039;re to take the thesis seriously as a positive thesis, demands a positive account of how reasoning errs--not just how *we* err due to our psychological quirks of being homo sapiens, although I think that it is important also, but due to the more general problem of reasoning with uncertain representations of a domain.

I am committed to this rough picture. My project then is to explore the feasibility of modeling this conception of acceptance measure-theoretically, which is how I understand what it means to be committed to the first legislative principle.]]></description>
		<content:encoded><![CDATA[<p>Stephen,</p>
<p>Thank you very much for your comments. I am particularly interested in the epistemology underneath this proposal, so your observations are most welcome. My reply, in short, is that I would be inclined to reject the strong detachment thesis that you describe.</p>
<p>But I&#8217;m not sure that doing so should be a depressing option for traditional epistemologists who might understandably be attracted to the thesis you describe. We can apply logic to beliefs that we accept with probabilistic support to get other beliefs that are supported as well. But, we should take caution when we start stringing together things we accept, leaving to one side <i>a priori</i> warranted propositions for the sake of the argument. Roughly put, accepting p and accepting q is good but not conclusive reason for accepting p and q. A logic for rational acceptance should tell you those non-trivial cases in which acceptance of the conjunction is entailed by the acceptance of each conjunct and tell you about the cases where what you should do is go look for more evidence for the joint belief you&#8217;d like to entertain. In the latter case, perhaps what one has is a hypothesis or conjecture that has some prima facie reason acceptance only. Note that both cases would nevertheless be defeasible (in the non-trivial case where theta is less than 1).</p>
<p>A second thought on detachment: I have played around with an idea that allowed one to treat a set of accepted statements as all on par in &#8216;A resource bounded default logic&#8217; (NMR 2004). But, since I start with bounded formulas it seems like I&#8217;m throwing away perfectly good information in order to get a set of propositions. This might not be so bad if, for instance, you are just interested in whether a particular set of formulas is non-monotonically derivable above some threshold point. But tossing out this information completely seems to weaken iterative inference since, then, the only reasonable thing to do would be to assign the threshold value to all beliefs in that class. In most cases you would weaken your hand by doing this. (I should add, however, that the semantics for statistical default logic is flexible enough to select the &#8216;best-bound&#8217; for a proposition, if there happens to be more than one. Currently, we do this with Answer Set Semantics in an &#8216;Implementation of SDL&#8217; that Carlos Damasio and I did appearing in JELIA 2004.)</p>
<p>I am not too worried about the book keeping issue, either, although I can see how a critic might make mischief here. I&#8217;m not claiming that we&#8217;ve numbers in our heads; Indeed, I find the idea that we actually have degrees of belief as persuasive as thinking that planets are actually point masses: it is to confuse the map for the terrain. Rather, giving an account that tells us, more precisely, why and when justification *may* degrade offers us the prospect of providing a good reason for when  we&#8211; ordinary but otherwise rational human beings&#8211;are warranted in being suspicious of a long but otherwise valid argument involving empirical premises that we accept and, perhaps more importantly, when we are not warranted in being suspicious of a complicated but otherwise valid empirical argument. It would seem that fallibilism, if we&#8217;re to take the thesis seriously as a positive thesis, demands a positive account of how reasoning errs&#8211;not just how *we* err due to our psychological quirks of being homo sapiens, although I think that it is important also, but due to the more general problem of reasoning with uncertain representations of a domain.</p>
<p>I am committed to this rough picture. My project then is to explore the feasibility of modeling this conception of acceptance measure-theoretically, which is how I understand what it means to be committed to the first legislative principle.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Stephen Fogdall</title>
		<link>http://certaindoubts.com/rational-acceptance-and-conjunctivedisjunctive-absorption/#comment-1575</link>
		<dc:creator><![CDATA[Stephen Fogdall]]></dc:creator>
		<pubDate>Thu, 31 Mar 2005 03:57:13 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=282#comment-1575</guid>
		<description><![CDATA[Hi Greg,

This is going to be a short note that, I fear, will come at your last post somewhat obliquely and won&#039;t do it justice.

Many non-formal epistemologists want to understand your first legislative principle (that it is rational to accept a proposition that is very probably true) in terms of a &quot;rule of detachment&quot; that has two properties.  The first (less interesting) property is simply that the rule allows one to infer a &quot;full&quot; belief from probabilistic evidence.  The second (more interesting) property is that rule (as the name implies) &quot;detaches&quot; the inferred belief from that evidence in the sense that, once inferred, the belief carries no essential reference to the fact the evidence underlying it offered only probabilistic support.

These same epistemologists regard the third legislative principle as simply a reflection of this same &quot;detachment&quot; property.  If the belief, once inferred, carries no essential referrence to the probabilistic character of the evidence from which one  inferred it, then what would be the obstacle to conjoining it with a second belief simillarly inferred?

I take it that your approach in terms of bounded formulas rejects any such rule of detachment, since bounded formulas always are tagged with a lower bound probability.

I think this actually gets into a rather deep question about human reasoning -- whether each inference records the level of support that the conclusion receives from its evidence (which in turn constrains the use of that conclusion in later inferences), or whether instead each inferential stage is, at were, opaque to the degree of support that attaches to prior stages.]]></description>
		<content:encoded><![CDATA[<p>Hi Greg,</p>
<p>This is going to be a short note that, I fear, will come at your last post somewhat obliquely and won&#8217;t do it justice.</p>
<p>Many non-formal epistemologists want to understand your first legislative principle (that it is rational to accept a proposition that is very probably true) in terms of a &#8220;rule of detachment&#8221; that has two properties.  The first (less interesting) property is simply that the rule allows one to infer a &#8220;full&#8221; belief from probabilistic evidence.  The second (more interesting) property is that rule (as the name implies) &#8220;detaches&#8221; the inferred belief from that evidence in the sense that, once inferred, the belief carries no essential reference to the fact the evidence underlying it offered only probabilistic support.</p>
<p>These same epistemologists regard the third legislative principle as simply a reflection of this same &#8220;detachment&#8221; property.  If the belief, once inferred, carries no essential referrence to the probabilistic character of the evidence from which one  inferred it, then what would be the obstacle to conjoining it with a second belief simillarly inferred?</p>
<p>I take it that your approach in terms of bounded formulas rejects any such rule of detachment, since bounded formulas always are tagged with a lower bound probability.</p>
<p>I think this actually gets into a rather deep question about human reasoning &#8212; whether each inference records the level of support that the conclusion receives from its evidence (which in turn constrains the use of that conclusion in later inferences), or whether instead each inferential stage is, at were, opaque to the degree of support that attaches to prior stages.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/rational-acceptance-and-conjunctivedisjunctive-absorption/#comment-1574</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Tue, 29 Mar 2005 17:10:27 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=282#comment-1574</guid>
		<description><![CDATA[Hi Stephen,

Thanks for your comments. I do think that I&#039;m committed to your propositions 1, 2, and 3 in the following sense: if p is bounded above the threshold point for acceptance Î¸ and q is also bounded above the threshold point for acceptance Î¸, then p or q is bounded above the threshold point for acceptance Î¸, since, by DA, the min of either of these probabilities is, by hypothesis, above Î¸. This would not hold for conjunction, however: a conjunction of two accepted propositions need not yield a rationally acceptable conjunction.

W.r.t. the main idea of the approach: I&#039;m not sure that the disjunction rule is the core of the approach. I like the idea of performing resolution with absorbed clauses, which is why I included a disjunction rule that allows me to build clauses (since I don&#039;t have negation for bounded formulas). I haven&#039;t thought through this idea much beyond what I&#039;ve written in the paper, however. It might be completely crackers.

W.r.t. Cohen: I agree that the proposal you sketch, applied to my project, would invite the paradoxes.

W.r.t. complete reconciliation of principles for rational acceptance 1 and 3: There is more work to be done on the approach I&#039;ve sketched, certainly. For instance, I think there is an interesting difference between inner/outer measures and lower/upper probabilities that I&#039;d like to study. One difference appears to concern monotonicity properties: inner measures are induced on a single (gappy) probability space, and so have nice monotonicity properties, whereas upper/lower measures, being sets of probabilities, are aggregates of measures. Hence, viewing the matter in terms of sets of probability measures, you might come along with fantastic new evidence for a proposition (a new experiment, rooted in a different probabilistic model that puts your proposition way, way above threshold, say) that allows its conjunction with some other propositions you accept. This scenario appears to be blocked by the inner/outer measure idea as I&#039;ve used it here, since measures used to induce the inner probability of a statement is fixed.

Nevertheless, I am committed to the basic idea here for how we should expect conjunction and disjunction to behave for rationally accepted propositions. The comment I just mentioned about upper/lower probability seems to introduce a wrinkle in how to understand the downward monotonic preservation of lower probability, which I want[ed] to bracket and set aside for another time.

I should add, too, that I&#039;m not too troubled by not having negation for bounded formulas nor troubled by not allowing conditional bounded formulas. One reason is that a negated bounded formula is ambiguous: it could express that a proposition is not bounded by e, but rather by some other value that is (a) greater than e or (b) less than e; or it could express (c) that we&#039;ve no probabilistic data with respect to that proposition&#039;s bound whatsoever. I don&#039;t worry too much about not have a language to express conditional accepted propositions because it seems ill-advised to saddle the view with a particular semantics for conditionals at this stage. (Indeed, it might be nice to see what we can do while forgetting about conditionals altogether.)

Cheers, Greg]]></description>
		<content:encoded><![CDATA[<p>Hi Stephen,</p>
<p>Thanks for your comments. I do think that I&#8217;m committed to your propositions 1, 2, and 3 in the following sense: if p is bounded above the threshold point for acceptance Î¸ and q is also bounded above the threshold point for acceptance Î¸, then p or q is bounded above the threshold point for acceptance Î¸, since, by DA, the min of either of these probabilities is, by hypothesis, above Î¸. This would not hold for conjunction, however: a conjunction of two accepted propositions need not yield a rationally acceptable conjunction.</p>
<p>W.r.t. the main idea of the approach: I&#8217;m not sure that the disjunction rule is the core of the approach. I like the idea of performing resolution with absorbed clauses, which is why I included a disjunction rule that allows me to build clauses (since I don&#8217;t have negation for bounded formulas). I haven&#8217;t thought through this idea much beyond what I&#8217;ve written in the paper, however. It might be completely crackers.</p>
<p>W.r.t. Cohen: I agree that the proposal you sketch, applied to my project, would invite the paradoxes.</p>
<p>W.r.t. complete reconciliation of principles for rational acceptance 1 and 3: There is more work to be done on the approach I&#8217;ve sketched, certainly. For instance, I think there is an interesting difference between inner/outer measures and lower/upper probabilities that I&#8217;d like to study. One difference appears to concern monotonicity properties: inner measures are induced on a single (gappy) probability space, and so have nice monotonicity properties, whereas upper/lower measures, being sets of probabilities, are aggregates of measures. Hence, viewing the matter in terms of sets of probability measures, you might come along with fantastic new evidence for a proposition (a new experiment, rooted in a different probabilistic model that puts your proposition way, way above threshold, say) that allows its conjunction with some other propositions you accept. This scenario appears to be blocked by the inner/outer measure idea as I&#8217;ve used it here, since measures used to induce the inner probability of a statement is fixed.</p>
<p>Nevertheless, I am committed to the basic idea here for how we should expect conjunction and disjunction to behave for rationally accepted propositions. The comment I just mentioned about upper/lower probability seems to introduce a wrinkle in how to understand the downward monotonic preservation of lower probability, which I want[ed] to bracket and set aside for another time.</p>
<p>I should add, too, that I&#8217;m not too troubled by not having negation for bounded formulas nor troubled by not allowing conditional bounded formulas. One reason is that a negated bounded formula is ambiguous: it could express that a proposition is not bounded by e, but rather by some other value that is (a) greater than e or (b) less than e; or it could express (c) that we&#8217;ve no probabilistic data with respect to that proposition&#8217;s bound whatsoever. I don&#8217;t worry too much about not have a language to express conditional accepted propositions because it seems ill-advised to saddle the view with a particular semantics for conditionals at this stage. (Indeed, it might be nice to see what we can do while forgetting about conditionals altogether.)</p>
<p>Cheers, Greg</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Stephen Fogdall</title>
		<link>http://certaindoubts.com/rational-acceptance-and-conjunctivedisjunctive-absorption/#comment-1573</link>
		<dc:creator><![CDATA[Stephen Fogdall]]></dc:creator>
		<pubDate>Sun, 27 Mar 2005 17:59:00 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=282#comment-1573</guid>
		<description><![CDATA[Greg,

Pretty interesting paper (though I confess I&#039;m still getting my head around it).  Is it fair to say that your core suggestion on solving the paradoxes is that there&#039;s no inconsistency in asserting the following three propositions simultaneously: 1) &quot;the probabilty that -p is true is above theta,&quot;; 2) &quot;the probability that -q is true is above theta,&quot; and 3) &quot;the probability that p or q is true is above theta&quot; (where theta is the threshold for rational acceptance)?

Is it fair to say also that your proposal still does not completely reconcile the first and third legislative principles you identify for rational acceptance?

Are you familiar at all with Cohen&#039;s book, the Probable and the Provable?  He develps a nonstandard probability calculus in which the probabiilty of a conjunction is equal to its least probable conjunct.  This obviosuly isn&#039;t what you&#039;re doing (and his approach seems rather to invite than to solve the paradoxes).  Still there is some overlap in the spirit of your approaches in the sense that (to the extent I understand you correctly) you recognize that the first and third legislative principles can be reconciled only to the extent that there is some way of controlling the probability bleed off that results when two propositions are conjoined.]]></description>
		<content:encoded><![CDATA[<p>Greg,</p>
<p>Pretty interesting paper (though I confess I&#8217;m still getting my head around it).  Is it fair to say that your core suggestion on solving the paradoxes is that there&#8217;s no inconsistency in asserting the following three propositions simultaneously: 1) &#8220;the probabilty that -p is true is above theta,&#8221;; 2) &#8220;the probability that -q is true is above theta,&#8221; and 3) &#8220;the probability that p or q is true is above theta&#8221; (where theta is the threshold for rational acceptance)?</p>
<p>Is it fair to say also that your proposal still does not completely reconcile the first and third legislative principles you identify for rational acceptance?</p>
<p>Are you familiar at all with Cohen&#8217;s book, the Probable and the Provable?  He develps a nonstandard probability calculus in which the probabiilty of a conjunction is equal to its least probable conjunct.  This obviosuly isn&#8217;t what you&#8217;re doing (and his approach seems rather to invite than to solve the paradoxes).  Still there is some overlap in the spirit of your approaches in the sense that (to the extent I understand you correctly) you recognize that the first and third legislative principles can be reconciled only to the extent that there is some way of controlling the probability bleed off that results when two propositions are conjoined.</p>
]]></content:encoded>
	</item>
</channel>
</rss>
