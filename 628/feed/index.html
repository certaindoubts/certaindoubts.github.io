<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	>
<channel>
	<title>Comments on: Knowing the Semantic Web</title>
	<atom:link href="http://certaindoubts.com/628/feed/" rel="self" type="application/rss+xml" />
	<link>http://certaindoubts.com/628/</link>
	<description>devoted to matters epistemic</description>
	<lastBuildDate>Wed, 10 Apr 2019 16:37:28 +0000</lastBuildDate>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.10</generator>
	<item>
		<title>By: CrunchyLogic  &#187; Blog Archive   &#187; AI, Semantic Web and Epistemology</title>
		<link>http://certaindoubts.com/628/#comment-4200</link>
		<dc:creator><![CDATA[CrunchyLogic  &#187; Blog Archive   &#187; AI, Semantic Web and Epistemology]]></dc:creator>
		<pubDate>Thu, 16 Nov 2006 21:30:23 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=628#comment-4200</guid>
		<description><![CDATA[[...]  on November 16th, 2006 &#8212; Posted in Artificial Intelligence  	Gregory Wheeler has an interesting post about the Semantic Web over at Certain Doubts. I tried to sketch some ways in  [...]]]></description>
		<content:encoded><![CDATA[<p>[&#8230;]  on November 16th, 2006 &#8212; Posted in Artificial Intelligence  	Gregory Wheeler has an interesting post about the Semantic Web over at Certain Doubts. I tried to sketch some ways in  [&#8230;]</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Yarden Katz</title>
		<link>http://certaindoubts.com/628/#comment-4199</link>
		<dc:creator><![CDATA[Yarden Katz]]></dc:creator>
		<pubDate>Tue, 14 Nov 2006 21:23:15 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=628#comment-4199</guid>
		<description><![CDATA[More detailed reply in email (sorry for the delay, I seem to have failed passing the &quot;CAPTCHA&quot; for your address on your homepage the first time :)).

A relevant reference is:

Axel Polleres, Cristina Feier, and Andreas Harth. Rules with contextually scoped negation. In Proceedings of the 3rd European Semantic Web Conference (ESWC2006), volume 4011 of Lecture Notes in Computer Science, Budva, Montenegro, June 2006. Springer.]]></description>
		<content:encoded><![CDATA[<p>More detailed reply in email (sorry for the delay, I seem to have failed passing the &#8220;CAPTCHA&#8221; for your address on your homepage the first time :)).</p>
<p>A relevant reference is:</p>
<p>Axel Polleres, Cristina Feier, and Andreas Harth. Rules with contextually scoped negation. In Proceedings of the 3rd European Semantic Web Conference (ESWC2006), volume 4011 of Lecture Notes in Computer Science, Budva, Montenegro, June 2006. Springer.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/628/#comment-4198</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Tue, 14 Nov 2006 20:52:12 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=628#comment-4198</guid>
		<description><![CDATA[PS: Yarden, do you know what is going on with Cycorp lately? I&#039;ve lost touch with news on them in the last few years. [Perhaps that is an email question.] Also, do you have a reference for a survey paper for scoped negation as failure?]]></description>
		<content:encoded><![CDATA[<p>PS: Yarden, do you know what is going on with Cycorp lately? I&#8217;ve lost touch with news on them in the last few years. [Perhaps that is an email question.] Also, do you have a reference for a survey paper for scoped negation as failure?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/628/#comment-4197</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Tue, 14 Nov 2006 12:29:01 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=628#comment-4197</guid>
		<description><![CDATA[Your comment is very rich, Yarden. I&#039;ll try to be brief.

I think the contribution that several (but not all) branches of AI can make to epistemology is by offering very clean and clear modeling tools. There is a comment Robert Stalnaker has made about modal logic. I think it is in one of the Blackwell handbooks, perhaps the one on philosophical logic. The comment is that what modal logic allows us to do is to articulate problems involving modalities much clearer than we are able to manage without it, and a consequence of having this very clean and clear tool for representing modalities (relational structures) opens new philosophical ground. I think this is exactly right.

Does AI truly offer improved methods? I think it does, but it has a marketing problem: it still has a snake-oil salesman&#039;s reputation in philosophy. This is mostly due to sociological noise &quot;progressives versus the hand-wavy old guard&quot;, or &quot;technological know-nothings tossing computers at problems that have withstood resolution by giants standing on the shoulders of &lt;i&gt;the&lt;/i&gt; giants of Western Civilization&quot;. (Sigh).

But, if you were to look at the similarity of structure of many problems in both fields, the clarity of &lt;i&gt;constraints&lt;/i&gt; demanded of solutions in many AI problems, and the results that are coming out of AI, I think there are signs of progress on methods that are relevant to epistemology.

Let me try a very general example. It concerns &quot;systematic approaches&quot; to problems. AI researchers are much less willing to collapse a diachronic system to a static model, and will (often) insist on dynamic models from the start, study how the other attributes behave through time and with each other. Right? And we do this because enough experience has taught us that building up a model and then tossing time in later rarely goes well.

Now turn back to philosophy. You&#039;ve always had &quot;systems&quot; philosophers making more or less similar points about analogous epistemological issues, but they have tended to take a back seat in analytic philosophy in the last 50 years or so. The reason, I suspect, is that the field&#039;s standards for clarity are keyed to a few basic tools (logic, probability, and basic modal logic) which are essentially methods for static models. One person&#039;s &lt;i&gt;modus ponens&lt;/i&gt; is another&#039;s &lt;i&gt;modus tollens&lt;/i&gt;, to be sure, but you&#039;ve got the upper hand if your negated consequent is necessarily false in the field&#039;s Style Guide. And I note some continued resistance in the field to natural extensions of the later two to allow them to behave more dynamically.

AI, despite its enthusiasm, or maybe because of it, doesn&#039;t have this institutional habit of tying its hands behind its back regarding methods. It will try just about anything as a representation language, study that representational language &lt;i&gt;qua&lt;/i&gt; language, and tell you what it is and is not good for.]]></description>
		<content:encoded><![CDATA[<p>Your comment is very rich, Yarden. I&#8217;ll try to be brief.</p>
<p>I think the contribution that several (but not all) branches of AI can make to epistemology is by offering very clean and clear modeling tools. There is a comment Robert Stalnaker has made about modal logic. I think it is in one of the Blackwell handbooks, perhaps the one on philosophical logic. The comment is that what modal logic allows us to do is to articulate problems involving modalities much clearer than we are able to manage without it, and a consequence of having this very clean and clear tool for representing modalities (relational structures) opens new philosophical ground. I think this is exactly right.</p>
<p>Does AI truly offer improved methods? I think it does, but it has a marketing problem: it still has a snake-oil salesman&#8217;s reputation in philosophy. This is mostly due to sociological noise &#8220;progressives versus the hand-wavy old guard&#8221;, or &#8220;technological know-nothings tossing computers at problems that have withstood resolution by giants standing on the shoulders of <i>the</i> giants of Western Civilization&#8221;. (Sigh).</p>
<p>But, if you were to look at the similarity of structure of many problems in both fields, the clarity of <i>constraints</i> demanded of solutions in many AI problems, and the results that are coming out of AI, I think there are signs of progress on methods that are relevant to epistemology.</p>
<p>Let me try a very general example. It concerns &#8220;systematic approaches&#8221; to problems. AI researchers are much less willing to collapse a diachronic system to a static model, and will (often) insist on dynamic models from the start, study how the other attributes behave through time and with each other. Right? And we do this because enough experience has taught us that building up a model and then tossing time in later rarely goes well.</p>
<p>Now turn back to philosophy. You&#8217;ve always had &#8220;systems&#8221; philosophers making more or less similar points about analogous epistemological issues, but they have tended to take a back seat in analytic philosophy in the last 50 years or so. The reason, I suspect, is that the field&#8217;s standards for clarity are keyed to a few basic tools (logic, probability, and basic modal logic) which are essentially methods for static models. One person&#8217;s <i>modus ponens</i> is another&#8217;s <i>modus tollens</i>, to be sure, but you&#8217;ve got the upper hand if your negated consequent is necessarily false in the field&#8217;s Style Guide. And I note some continued resistance in the field to natural extensions of the later two to allow them to behave more dynamically.</p>
<p>AI, despite its enthusiasm, or maybe because of it, doesn&#8217;t have this institutional habit of tying its hands behind its back regarding methods. It will try just about anything as a representation language, study that representational language <i>qua</i> language, and tell you what it is and is not good for.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Yarden Katz</title>
		<link>http://certaindoubts.com/628/#comment-4196</link>
		<dc:creator><![CDATA[Yarden Katz]]></dc:creator>
		<pubDate>Tue, 14 Nov 2006 04:43:28 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=628#comment-4196</guid>
		<description><![CDATA[Nice post.  I think that it is very clear that there is a tight connection between epistemology and traditional AI.  The Semantic Web (I&#039;m refraining from the fairly new Web 3.0 buzzword) can be seen as an application of traditional AI to the Web.  The interesting question for me is whether the Semantic Web raises any new philosophical issues -- whether in epistemology or not -- that did not come up in traditional AI.  In my view, the answer is No, with some exceptions.

A lot of the work by researchers described in the article you link to is different from what is the mainstream approach in the Semantic Web community.  A primary goal of the research described in the article is to &quot;data mine&quot; the Web, and infer the structures of web pages.  This is certainly relevant to Semantic Web, but I think the much more dominant approach to achieving the same results (a more intelligent web, an understanding of content, etc.) in the SW community is logic-based knowledge representation.  The Web Ontology Language (OWL) has become a W3C standard, and I think the common view is that people will create ontologies in this language for modelling their domains of interest, that users will mark up data using these ontologies, and tools will support certain inference services for this marked up data.  OWL is a KR language, and two of its three &quot;flavors&quot; happen to be decidable subsets of first-order logic.

It is hard to find agreement in the community on how exactly the above goals will be met.  This is not surprising, since many of these are regurgitated disagreements from previous AI debates.  What is the right knowledge representation?  Do we want to represent commonsense knowledge, and is representing our knowledge the right path to intelligent systems at all?  If it is, do we want to use (first-order?) logic as the main tool for doing so? Etc.

So far these are unoriginal questions.  What makes the Web different in a way that may raise new problems?  I can think of two things: (i) sheer size, and (ii) the &#039;free-for-all&#039; aspects of it.  (i) is obvious.  By (ii) I mean that right now, nothing prevents anyone from linking to any document and commenting on it, where as traditional KR systems seemed to be much more controlled.  Currently my comments, posted on my web page, about one of your documents will merely be text in some natural language.  But what if instead of linking to your document, I borrowed a concept from your ontology?  For example, from a project like CYC, which attempts to build a huge &quot;commonsense&quot; ontology, I may import the concept &quot;Cat&quot;.  Naturally, Cat is defined in terms of other concepts, like Animal and Mammal.  Does my importing of this concept imply acceptance of CYC&#039;s view on these other concepts?  (And what if these concepts are inconsistent with other things I have already defined?) If CYC changes the definitions of concepts -- just like you can change text on your web page -- am I still committed to them?  The intuition is that the answers to both should be No, but it is hard to work out the techical details to make this happen.

Many of these issues have been labelled as &quot;Social Meaning&quot; problems, see http://www.xml.com/pub/a/2003/03/05/social.html for a nice discussion.

There are other issues that came up in traditional logic-based KR but that get a new &quot;twist&quot; in a Web context.  One is old Open v. Closed world assumptions.    In a database, closed world makes sense, and we can usually employ negation-as-failure.  Does it make sense to think of the Web as a database?  Again, the common answer is that it doesn&#039;t, since OWL has open-world semantics, but this is actively contested.  Some proposed a &quot;local&quot; (or &quot;scoped&quot;) negation-as-failure, which is relativized to documents, but I have not seen the technical details worked out on this either.  In fact, anything having to do with scoping and negation in this context is really hard to get right..]]></description>
		<content:encoded><![CDATA[<p>Nice post.  I think that it is very clear that there is a tight connection between epistemology and traditional AI.  The Semantic Web (I&#8217;m refraining from the fairly new Web 3.0 buzzword) can be seen as an application of traditional AI to the Web.  The interesting question for me is whether the Semantic Web raises any new philosophical issues &#8212; whether in epistemology or not &#8212; that did not come up in traditional AI.  In my view, the answer is No, with some exceptions.</p>
<p>A lot of the work by researchers described in the article you link to is different from what is the mainstream approach in the Semantic Web community.  A primary goal of the research described in the article is to &#8220;data mine&#8221; the Web, and infer the structures of web pages.  This is certainly relevant to Semantic Web, but I think the much more dominant approach to achieving the same results (a more intelligent web, an understanding of content, etc.) in the SW community is logic-based knowledge representation.  The Web Ontology Language (OWL) has become a W3C standard, and I think the common view is that people will create ontologies in this language for modelling their domains of interest, that users will mark up data using these ontologies, and tools will support certain inference services for this marked up data.  OWL is a KR language, and two of its three &#8220;flavors&#8221; happen to be decidable subsets of first-order logic.</p>
<p>It is hard to find agreement in the community on how exactly the above goals will be met.  This is not surprising, since many of these are regurgitated disagreements from previous AI debates.  What is the right knowledge representation?  Do we want to represent commonsense knowledge, and is representing our knowledge the right path to intelligent systems at all?  If it is, do we want to use (first-order?) logic as the main tool for doing so? Etc.</p>
<p>So far these are unoriginal questions.  What makes the Web different in a way that may raise new problems?  I can think of two things: (i) sheer size, and (ii) the &#8216;free-for-all&#8217; aspects of it.  (i) is obvious.  By (ii) I mean that right now, nothing prevents anyone from linking to any document and commenting on it, where as traditional KR systems seemed to be much more controlled.  Currently my comments, posted on my web page, about one of your documents will merely be text in some natural language.  But what if instead of linking to your document, I borrowed a concept from your ontology?  For example, from a project like CYC, which attempts to build a huge &#8220;commonsense&#8221; ontology, I may import the concept &#8220;Cat&#8221;.  Naturally, Cat is defined in terms of other concepts, like Animal and Mammal.  Does my importing of this concept imply acceptance of CYC&#8217;s view on these other concepts?  (And what if these concepts are inconsistent with other things I have already defined?) If CYC changes the definitions of concepts &#8212; just like you can change text on your web page &#8212; am I still committed to them?  The intuition is that the answers to both should be No, but it is hard to work out the techical details to make this happen.</p>
<p>Many of these issues have been labelled as &#8220;Social Meaning&#8221; problems, see <a href="http://www.xml.com/pub/a/2003/03/05/social.html" rel="nofollow">http://www.xml.com/pub/a/2003/03/05/social.html</a> for a nice discussion.</p>
<p>There are other issues that came up in traditional logic-based KR but that get a new &#8220;twist&#8221; in a Web context.  One is old Open v. Closed world assumptions.    In a database, closed world makes sense, and we can usually employ negation-as-failure.  Does it make sense to think of the Web as a database?  Again, the common answer is that it doesn&#8217;t, since OWL has open-world semantics, but this is actively contested.  Some proposed a &#8220;local&#8221; (or &#8220;scoped&#8221;) negation-as-failure, which is relativized to documents, but I have not seen the technical details worked out on this either.  In fact, anything having to do with scoping and negation in this context is really hard to get right..</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/628/#comment-4195</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Mon, 13 Nov 2006 12:47:23 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=628#comment-4195</guid>
		<description><![CDATA[Hi Joe,

Thanks for your comments. The short of my reply is that I think there is a closer relationship between epistemology and AI than there is between the philosophy of mind and  AI. So, we might be focusing on two different sets of questions.

My point about the semantic web is that there are a cluster of problems standing between where we are now and what the WC3 wants to achieve that are structurally similar to a cluster of problems in epistemology. And there is good reason to suggest that the methods and results that are generated in attacking these problems will have bearing on their epistemological cousins. I think it will have a direct bearing on our model building.

So, to answer your first question, the progress will come in having a wider variety of methods to model and articulate problems that exercise epistemologists. I don&#039;t know how to measure that.

My answer to your second question is similar to Davidson&#039;s but without making any commitments about the workings of human brains. I think that we do not clearly understand the problems that still stand between us and answer(s) to how we actually reason/plan/change view/change preferences/understand natural language/use formal languages... .

The results of AI  do not necessarily amount to empirical conjectures about the actual working of human minds, despite (often) enthusiastic claims you&#039;ll find in abstracts and motivating examples. At bottom they are modeling proposals, which can be evaluated both empirically and formally. And now the field is poised to turn its sights on a class of problems that bear remarkable similarity to core concerns of traditional epistemology. This is where we stand to learn a great deal.]]></description>
		<content:encoded><![CDATA[<p>Hi Joe,</p>
<p>Thanks for your comments. The short of my reply is that I think there is a closer relationship between epistemology and AI than there is between the philosophy of mind and  AI. So, we might be focusing on two different sets of questions.</p>
<p>My point about the semantic web is that there are a cluster of problems standing between where we are now and what the WC3 wants to achieve that are structurally similar to a cluster of problems in epistemology. And there is good reason to suggest that the methods and results that are generated in attacking these problems will have bearing on their epistemological cousins. I think it will have a direct bearing on our model building.</p>
<p>So, to answer your first question, the progress will come in having a wider variety of methods to model and articulate problems that exercise epistemologists. I don&#8217;t know how to measure that.</p>
<p>My answer to your second question is similar to Davidson&#8217;s but without making any commitments about the workings of human brains. I think that we do not clearly understand the problems that still stand between us and answer(s) to how we actually reason/plan/change view/change preferences/understand natural language/use formal languages&#8230; .</p>
<p>The results of AI  do not necessarily amount to empirical conjectures about the actual working of human minds, despite (often) enthusiastic claims you&#8217;ll find in abstracts and motivating examples. At bottom they are modeling proposals, which can be evaluated both empirically and formally. And now the field is poised to turn its sights on a class of problems that bear remarkable similarity to core concerns of traditional epistemology. This is where we stand to learn a great deal.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Joe Hatfield</title>
		<link>http://certaindoubts.com/628/#comment-4194</link>
		<dc:creator><![CDATA[Joe Hatfield]]></dc:creator>
		<pubDate>Mon, 13 Nov 2006 04:49:52 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=628#comment-4194</guid>
		<description><![CDATA[Reaction #1:  I agree that the successes of the W3C project will be less than &quot;complete&quot; (although it is interesting to note that even with ones fellow human language users there will not be &quot;complete&quot; success either).  This brings up an interesting question: what standard or metric would one use to judge progress in this research?  Wouldn&#039;t this depend upon what you are interested in?  Google may be playing a different &quot;language-game&quot; here and bringing in an over-arching word like &quot;understanding&quot; to try to link what Google desires to do and what philosophers are interested in may be blurring two issues.  Let&#039;s say that not only did Google &quot;pass the Turing test&quot; with respect to questions like the one about Spanish logicians but let&#039;s say that it was statistically more likely to generate &quot;correct&quot; (speaking loosly) responses than a large sample of fully linguistic adults.  This would be complete success for the purposes that Google wanted for this technology. But I can still hear Searle or somebody like that saying that &quot;it is only behaving as if it understands...etc.&quot;.

Reaction #2: Assuming that the &quot;blurring&quot; charge is not true from reaction #1...Donald Davidson was tireless of emphasizing that his proposal for using the Tarskian truth theory machinary to explicate the type of recursive pattern that must exist (in our brains lets say) in order to explain some key characteristics of our linguistic abilities was simply ONE POSSIBLE way that we might ACTUALLY perform such recursive functions.  In other words, as he often put it, &quot;if I can show that here is one possible way that it can be done then this might shed some light on how we actually do it&quot; (I think that is a direct quote from the Davidson/McDowell &quot;In Conversation&quot; video).  This strikes me as similar to the Google W3C work and the philosopher.  If Google can map out some possible way in which it could work it may very well shed some light into how it is that we actually do this type of thing.  And if we take a Davidsonian or Dennett view about how content is derived then it doesn&#039;t really matter that much anyway if Google&#039;s algorithms aren&#039;t &quot;locatable&quot; somewhere within our &quot;hardware&quot;.  Ok, I&#039;ll stop because now I&#039;m just thinking out loud.  I hope this helps.]]></description>
		<content:encoded><![CDATA[<p>Reaction #1:  I agree that the successes of the W3C project will be less than &#8220;complete&#8221; (although it is interesting to note that even with ones fellow human language users there will not be &#8220;complete&#8221; success either).  This brings up an interesting question: what standard or metric would one use to judge progress in this research?  Wouldn&#8217;t this depend upon what you are interested in?  Google may be playing a different &#8220;language-game&#8221; here and bringing in an over-arching word like &#8220;understanding&#8221; to try to link what Google desires to do and what philosophers are interested in may be blurring two issues.  Let&#8217;s say that not only did Google &#8220;pass the Turing test&#8221; with respect to questions like the one about Spanish logicians but let&#8217;s say that it was statistically more likely to generate &#8220;correct&#8221; (speaking loosly) responses than a large sample of fully linguistic adults.  This would be complete success for the purposes that Google wanted for this technology. But I can still hear Searle or somebody like that saying that &#8220;it is only behaving as if it understands&#8230;etc.&#8221;.</p>
<p>Reaction #2: Assuming that the &#8220;blurring&#8221; charge is not true from reaction #1&#8230;Donald Davidson was tireless of emphasizing that his proposal for using the Tarskian truth theory machinary to explicate the type of recursive pattern that must exist (in our brains lets say) in order to explain some key characteristics of our linguistic abilities was simply ONE POSSIBLE way that we might ACTUALLY perform such recursive functions.  In other words, as he often put it, &#8220;if I can show that here is one possible way that it can be done then this might shed some light on how we actually do it&#8221; (I think that is a direct quote from the Davidson/McDowell &#8220;In Conversation&#8221; video).  This strikes me as similar to the Google W3C work and the philosopher.  If Google can map out some possible way in which it could work it may very well shed some light into how it is that we actually do this type of thing.  And if we take a Davidsonian or Dennett view about how content is derived then it doesn&#8217;t really matter that much anyway if Google&#8217;s algorithms aren&#8217;t &#8220;locatable&#8221; somewhere within our &#8220;hardware&#8221;.  Ok, I&#8217;ll stop because now I&#8217;m just thinking out loud.  I hope this helps.</p>
]]></content:encoded>
	</item>
</channel>
</rss>
