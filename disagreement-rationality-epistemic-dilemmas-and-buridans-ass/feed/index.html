<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	>
<channel>
	<title>Comments on: Disagreement, Rationality, Epistemic Dilemmas, and Buridan&#8217;s Ass</title>
	<atom:link href="http://certaindoubts.com/disagreement-rationality-epistemic-dilemmas-and-buridans-ass/feed/" rel="self" type="application/rss+xml" />
	<link>http://certaindoubts.com/disagreement-rationality-epistemic-dilemmas-and-buridans-ass/</link>
	<description>devoted to matters epistemic</description>
	<lastBuildDate>Wed, 10 Apr 2019 16:37:28 +0000</lastBuildDate>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.10</generator>
	<item>
		<title>By: jon kvanvig</title>
		<link>http://certaindoubts.com/disagreement-rationality-epistemic-dilemmas-and-buridans-ass/#comment-629</link>
		<dc:creator><![CDATA[jon kvanvig]]></dc:creator>
		<pubDate>Fri, 03 Sep 2004 21:11:41 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=116#comment-629</guid>
		<description><![CDATA[Rich, good to see you here!  And the example has just the right flavor for me--the evidentialists are perfectly rational...

The self-trust issue is a hard one for me, too, but I think the key has to be that it is  not a matter of some general trust, though that plays a role too.  I think of it as very specific trust in (some) cases of disagreement, since if I lose trust in myself in the specific case in which I disagree, say, with you, then I shouldn&#039;t hold my opinion prior to splitting any difference with those whom I disagree.  The first thing I must do is find an attitude that I do trust myself about.  And once I achieve that, I don&#039;t attribute the same specific trustworthiness to you:  after all, I take you to be mistaken on the issue.  I do grant that you may be entitled to the same trust for yourself, but that&#039;s just part of the egocentric aspect of rationality.

So I think the self-trust issue is, in a way, unaffected by whether other people agree or disagree.  If I take an attitude toward p and lack self-trust on the specific issue, then there is something incoherent in my total epistemic condition.  When others disagree, it brings to consciousness the issue of whether I should have such trust, in a way that doesn&#039;t arise for me when others agree with me.  My inclination, though, is to treat both cases the same:  if you agree with me, that doesn&#039;t legitimate my self-trust on the issue nor is it evidence for the claim in question; and if you disagree, that doesn&#039;t by itself make my self-trust illegitimate.  It doesn&#039;t introduce an explanatory gap in my overall understanding of things, since if I&#039;m right, you&#039;ve made a blunder of some sort, and that raise the question of why an ordinarily reliable cognizer has done so.  So to complete my understanding of the issue, I&#039;ll need to come to see why you&#039;ve blundered.  But lack of full understanding doesn&#039;t undermine justification, or so it seems to me.]]></description>
		<content:encoded><![CDATA[<p>Rich, good to see you here!  And the example has just the right flavor for me&#8211;the evidentialists are perfectly rational&#8230;</p>
<p>The self-trust issue is a hard one for me, too, but I think the key has to be that it is  not a matter of some general trust, though that plays a role too.  I think of it as very specific trust in (some) cases of disagreement, since if I lose trust in myself in the specific case in which I disagree, say, with you, then I shouldn&#8217;t hold my opinion prior to splitting any difference with those whom I disagree.  The first thing I must do is find an attitude that I do trust myself about.  And once I achieve that, I don&#8217;t attribute the same specific trustworthiness to you:  after all, I take you to be mistaken on the issue.  I do grant that you may be entitled to the same trust for yourself, but that&#8217;s just part of the egocentric aspect of rationality.</p>
<p>So I think the self-trust issue is, in a way, unaffected by whether other people agree or disagree.  If I take an attitude toward p and lack self-trust on the specific issue, then there is something incoherent in my total epistemic condition.  When others disagree, it brings to consciousness the issue of whether I should have such trust, in a way that doesn&#8217;t arise for me when others agree with me.  My inclination, though, is to treat both cases the same:  if you agree with me, that doesn&#8217;t legitimate my self-trust on the issue nor is it evidence for the claim in question; and if you disagree, that doesn&#8217;t by itself make my self-trust illegitimate.  It doesn&#8217;t introduce an explanatory gap in my overall understanding of things, since if I&#8217;m right, you&#8217;ve made a blunder of some sort, and that raise the question of why an ordinarily reliable cognizer has done so.  So to complete my understanding of the issue, I&#8217;ll need to come to see why you&#8217;ve blundered.  But lack of full understanding doesn&#8217;t undermine justification, or so it seems to me.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Rich Feldman</title>
		<link>http://certaindoubts.com/disagreement-rationality-epistemic-dilemmas-and-buridans-ass/#comment-628</link>
		<dc:creator><![CDATA[Rich Feldman]]></dc:creator>
		<pubDate>Fri, 03 Sep 2004 18:12:59 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=116#comment-628</guid>
		<description><![CDATA[This is my first contribution here (or to any blog, for that matter). I hope I&#039;m doing it right.

First, Jon - thanks very much for the kind words at the beginning of this thread. I really do appeciate them, even if what (you think) you learned in this case is roughly &quot;Don&#039;t think &lt;i&gt;that&lt;/i&gt;.&quot;

On the issue itself, if some kind of &quot;split the difference&quot; principle is true, then we will have to resort to degrees of belief or some set of ever finer-grained attitudes. Of course, any such principle will have to be carefully crafted. I don&#039;t think the following idea made it into the paper that was online, but it is in an another version. Imagine an apparent &quot;reasonable disagreement&quot; in which there are two points of view. For laughs - reliabilism and evidentialism. All the evidentialists respond to the disagreement by being reasonable and they suspend judgment. The reliabilists stick to their guns. Now there are only reliabilists and suspenders of judgment. So the reliabilists win. That can&#039;t be right.

The point about self-trust is an interesting one. I have no worked out views. Here&#039;s one thought. It&#039;s one thing to think that self-trust plays some role in the justification of beliefs generally, and maybe that it plays some role in dealing with skepticism. I&#039;m no fan of that view, but I won&#039;t argue about it. But, if so, I think I should agree that others, e.g., you, are entitled to the same sort of self-trust. For the kinds of cases we typically worry about in epistemology - perceptual knowledge, etc. - this doesn&#039;t really lead to problems since there is so much agreement. But the cases I&#039;m interested in seem to me to be quite different, for just that reason. I don&#039;t think the self-trust amounts to a claim to the effect that I&#039;m more trustworthy than you, just that I am in some sense trustworthy. This is not very clear. But maybe that&#039;s ok in this setting.]]></description>
		<content:encoded><![CDATA[<p>This is my first contribution here (or to any blog, for that matter). I hope I&#8217;m doing it right.</p>
<p>First, Jon &#8211; thanks very much for the kind words at the beginning of this thread. I really do appeciate them, even if what (you think) you learned in this case is roughly &#8220;Don&#8217;t think <i>that</i>.&#8221;</p>
<p>On the issue itself, if some kind of &#8220;split the difference&#8221; principle is true, then we will have to resort to degrees of belief or some set of ever finer-grained attitudes. Of course, any such principle will have to be carefully crafted. I don&#8217;t think the following idea made it into the paper that was online, but it is in an another version. Imagine an apparent &#8220;reasonable disagreement&#8221; in which there are two points of view. For laughs &#8211; reliabilism and evidentialism. All the evidentialists respond to the disagreement by being reasonable and they suspend judgment. The reliabilists stick to their guns. Now there are only reliabilists and suspenders of judgment. So the reliabilists win. That can&#8217;t be right.</p>
<p>The point about self-trust is an interesting one. I have no worked out views. Here&#8217;s one thought. It&#8217;s one thing to think that self-trust plays some role in the justification of beliefs generally, and maybe that it plays some role in dealing with skepticism. I&#8217;m no fan of that view, but I won&#8217;t argue about it. But, if so, I think I should agree that others, e.g., you, are entitled to the same sort of self-trust. For the kinds of cases we typically worry about in epistemology &#8211; perceptual knowledge, etc. &#8211; this doesn&#8217;t really lead to problems since there is so much agreement. But the cases I&#8217;m interested in seem to me to be quite different, for just that reason. I don&#8217;t think the self-trust amounts to a claim to the effect that I&#8217;m more trustworthy than you, just that I am in some sense trustworthy. This is not very clear. But maybe that&#8217;s ok in this setting.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: jon kvanvig</title>
		<link>http://certaindoubts.com/disagreement-rationality-epistemic-dilemmas-and-buridans-ass/#comment-627</link>
		<dc:creator><![CDATA[jon kvanvig]]></dc:creator>
		<pubDate>Tue, 31 Aug 2004 19:32:17 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=116#comment-627</guid>
		<description><![CDATA[Matt, yes I think the key here is some kind of self-trust that makes the first/third-person cases much different from a third/third-person evaluation.

I think it&#039;s a mistake to think of noting the disagree-ers attitude toward p as evidence regarding p.  If I add that you&#039;re a peer (or superior), and that we&#039;ve shared all our evidence, then the most that I could learn in addition is something about how you assess the evidence.  But evidence about how you assess the evidence for p is not itself evidence for p.]]></description>
		<content:encoded><![CDATA[<p>Matt, yes I think the key here is some kind of self-trust that makes the first/third-person cases much different from a third/third-person evaluation.</p>
<p>I think it&#8217;s a mistake to think of noting the disagree-ers attitude toward p as evidence regarding p.  If I add that you&#8217;re a peer (or superior), and that we&#8217;ve shared all our evidence, then the most that I could learn in addition is something about how you assess the evidence.  But evidence about how you assess the evidence for p is not itself evidence for p.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Matt Weiner</title>
		<link>http://certaindoubts.com/disagreement-rationality-epistemic-dilemmas-and-buridans-ass/#comment-626</link>
		<dc:creator><![CDATA[Matt Weiner]]></dc:creator>
		<pubDate>Tue, 31 Aug 2004 16:59:53 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=116#comment-626</guid>
		<description><![CDATA[Well, the rule I was envisioning was something like this: The evidence S and S&#039; share must include the fact that they&#039;re epistemic peers and the fact that they each have certain degrees of belief. (This&#039;ll have to be put in the third person rather than the first for the evidence to be sharable--each can believe &quot;S is rational and believes p,&quot; &quot;S&#039; is rational and withholds judgment.&quot;) My idea was that this evidence requires S and S&#039; to adopt a new degree of belief midway between their old ones. So that requires adopting the attitude that any degree of belief other than the new midway belief is mistaken--in light of the new evidence each person acquires concerning the other&#039;s degree of belief.* Since learning about the other person&#039;s attitude provides new evidence, it&#039;s not surprising that it requires dropping old attitudes concerning the appropriate degree of belief on the evidence.

Suppose S and S&#039; both do have the attitude &quot;splitting the difference between our two attitudes would do nothing to calibrate either of our attitudes more closely to the truth.&quot; I think Rich would say that that attitude just is irrational and should be dropped--his argument is meant to show that it&#039;s irrational! And, if S adopts the midway attitude and S&#039; does not, S isn&#039;t obliged to meet S&#039; halfway again; since S&#039; isn&#039;t responding appropriately to the evidence about the attitude S has**, S&#039; isn&#039;t an epistemic peer any more.

I hope that answers your questions in the second paragraph. I should say that I am almost certain that Rich&#039;s position is wrong, though I&#039;m not sure where the flaw in his argument is! It seems to require a total lack of self-trust; once you&#039;ve evaluated the evidence, your degree of belief is blown about willy-nilly by every epistemic peer who (you become convinced) has evaluated the same evidence and come to different conclusions. Perhaps the moral should be that it&#039;s really hard to become convinced that someone&#039;s really has rationally considered all your evidence, and that disagreeing with such a person is very disturbing.

*An insight I had while typing this sentence: S&#039; does not have a possessive form.
**S doesn&#039;t have a possessive form either, in this context.]]></description>
		<content:encoded><![CDATA[<p>Well, the rule I was envisioning was something like this: The evidence S and S&#8217; share must include the fact that they&#8217;re epistemic peers and the fact that they each have certain degrees of belief. (This&#8217;ll have to be put in the third person rather than the first for the evidence to be sharable&#8211;each can believe &#8220;S is rational and believes p,&#8221; &#8220;S&#8217; is rational and withholds judgment.&#8221;) My idea was that this evidence requires S and S&#8217; to adopt a new degree of belief midway between their old ones. So that requires adopting the attitude that any degree of belief other than the new midway belief is mistaken&#8211;in light of the new evidence each person acquires concerning the other&#8217;s degree of belief.* Since learning about the other person&#8217;s attitude provides new evidence, it&#8217;s not surprising that it requires dropping old attitudes concerning the appropriate degree of belief on the evidence.</p>
<p>Suppose S and S&#8217; both do have the attitude &#8220;splitting the difference between our two attitudes would do nothing to calibrate either of our attitudes more closely to the truth.&#8221; I think Rich would say that that attitude just is irrational and should be dropped&#8211;his argument is meant to show that it&#8217;s irrational! And, if S adopts the midway attitude and S&#8217; does not, S isn&#8217;t obliged to meet S&#8217; halfway again; since S&#8217; isn&#8217;t responding appropriately to the evidence about the attitude S has**, S&#8217; isn&#8217;t an epistemic peer any more.</p>
<p>I hope that answers your questions in the second paragraph. I should say that I am almost certain that Rich&#8217;s position is wrong, though I&#8217;m not sure where the flaw in his argument is! It seems to require a total lack of self-trust; once you&#8217;ve evaluated the evidence, your degree of belief is blown about willy-nilly by every epistemic peer who (you become convinced) has evaluated the same evidence and come to different conclusions. Perhaps the moral should be that it&#8217;s really hard to become convinced that someone&#8217;s really has rationally considered all your evidence, and that disagreeing with such a person is very disturbing.</p>
<p>*An insight I had while typing this sentence: S&#8217; does not have a possessive form.<br />
**S doesn&#8217;t have a possessive form either, in this context.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: jon kvanvig</title>
		<link>http://certaindoubts.com/disagreement-rationality-epistemic-dilemmas-and-buridans-ass/#comment-625</link>
		<dc:creator><![CDATA[jon kvanvig]]></dc:creator>
		<pubDate>Mon, 30 Aug 2004 20:59:00 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=116#comment-625</guid>
		<description><![CDATA[Matt, good points here.  I take it your revision of Rich would require, first, an appeal to degrees of belief, and second, that given the antecedent for the principles I formulate, a degree of belief is rational for S iff it is rational for S&#039;.

To disagree in a transparent way, however, is going to cause problems for the revised account as well.  If you and I disagree in a situation where we both know the degree of belief of the other, then we will also be convinced that the other person&#039;s degree of belief is mistaken (and that any degree of belief other than the one we hold is also mistaken).  If we then acquiesce and split the difference, then we&#039;ll have an incoherent belief set.  So suppose we also require that these additional attitudes be dropped (or lowered significantly).  And attitudes such as, &quot;splitting the difference between our two attitudes would do nothing to calibrate either of our attitudes more closely to the truth.&quot;  But I don&#039;t see why dropping any such attitude would be rational; in fact, it will be one of the few things we do agree on!

Here&#039;s another point.  Suppose degree of belief n is required for justified belief.  If I don&#039;t like having unjustified beliefs, then if I&#039;m short of n with respect to some proposition, I can go look for a peer that I regard as overconfident.  Once we share our information, and if I still think of them as a peer, I&#039;ve got it made.  Their overconfidence makes it rational for me to adopt a higher degree of belief.  Suppose I manage to do so, in spite of still viewing my peer as overconfident.  I&#039;ve now got a justified belief; what a relief!]]></description>
		<content:encoded><![CDATA[<p>Matt, good points here.  I take it your revision of Rich would require, first, an appeal to degrees of belief, and second, that given the antecedent for the principles I formulate, a degree of belief is rational for S iff it is rational for S&#8217;.</p>
<p>To disagree in a transparent way, however, is going to cause problems for the revised account as well.  If you and I disagree in a situation where we both know the degree of belief of the other, then we will also be convinced that the other person&#8217;s degree of belief is mistaken (and that any degree of belief other than the one we hold is also mistaken).  If we then acquiesce and split the difference, then we&#8217;ll have an incoherent belief set.  So suppose we also require that these additional attitudes be dropped (or lowered significantly).  And attitudes such as, &#8220;splitting the difference between our two attitudes would do nothing to calibrate either of our attitudes more closely to the truth.&#8221;  But I don&#8217;t see why dropping any such attitude would be rational; in fact, it will be one of the few things we do agree on!</p>
<p>Here&#8217;s another point.  Suppose degree of belief n is required for justified belief.  If I don&#8217;t like having unjustified beliefs, then if I&#8217;m short of n with respect to some proposition, I can go look for a peer that I regard as overconfident.  Once we share our information, and if I still think of them as a peer, I&#8217;ve got it made.  Their overconfidence makes it rational for me to adopt a higher degree of belief.  Suppose I manage to do so, in spite of still viewing my peer as overconfident.  I&#8217;ve now got a justified belief; what a relief!</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Matt Weiner</title>
		<link>http://certaindoubts.com/disagreement-rationality-epistemic-dilemmas-and-buridans-ass/#comment-624</link>
		<dc:creator><![CDATA[Matt Weiner]]></dc:creator>
		<pubDate>Mon, 30 Aug 2004 20:34:07 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=116#comment-624</guid>
		<description><![CDATA[On Rich&#039;s behalf, though he may be committed to the principle you&#039;ve bolded, it needn&#039;t generate a paradox.  Note that Rich says:
&quot;another person, every bit as sensible and serious as I, has an &lt;b&gt;opposing&lt;/b&gt; reaction.&quot;

It seems to me that the bolded principle, combined with the things Rich says, yield that the correct thing for S and S&#039; to do is split the difference in their attitudes. If S believes, and S&#039; withholds judgment, then both should end up at a belief that&#039;s half as strong as S&#039;s original belief. Now, in Rich&#039;s example, S and S&#039; have opposing reactions--so when you split the difference you wind up with withholding judgment. But in general, splitting the difference won&#039;t yield that.

And this does seem like the right thing to do if you accept Rich&#039;s line of thinking here--if I withhold with respect to p, and I discover that one of my peers thinks that our evidence &lt;i&gt;does&lt;/i&gt; prove that p, then I should worry that p may be true. (In the extreme, if all my peers think it&#039;s proof that p, I am certainly being dogmatically timid to continue withholding judgment.)

Of course, this raises the problem of whether these differences can always be split....]]></description>
		<content:encoded><![CDATA[<p>On Rich&#8217;s behalf, though he may be committed to the principle you&#8217;ve bolded, it needn&#8217;t generate a paradox.  Note that Rich says:<br />
&#8220;another person, every bit as sensible and serious as I, has an <b>opposing</b> reaction.&#8221;</p>
<p>It seems to me that the bolded principle, combined with the things Rich says, yield that the correct thing for S and S&#8217; to do is split the difference in their attitudes. If S believes, and S&#8217; withholds judgment, then both should end up at a belief that&#8217;s half as strong as S&#8217;s original belief. Now, in Rich&#8217;s example, S and S&#8217; have opposing reactions&#8211;so when you split the difference you wind up with withholding judgment. But in general, splitting the difference won&#8217;t yield that.</p>
<p>And this does seem like the right thing to do if you accept Rich&#8217;s line of thinking here&#8211;if I withhold with respect to p, and I discover that one of my peers thinks that our evidence <i>does</i> prove that p, then I should worry that p may be true. (In the extreme, if all my peers think it&#8217;s proof that p, I am certainly being dogmatically timid to continue withholding judgment.)</p>
<p>Of course, this raises the problem of whether these differences can always be split&#8230;.</p>
]]></content:encoded>
	</item>
</channel>
</rss>
