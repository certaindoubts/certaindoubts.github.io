<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	>
<channel>
	<title>Comments on: An Epistemic Newcomb Problem</title>
	<atom:link href="http://certaindoubts.com/an-epistemic-newcomb-problem/feed/" rel="self" type="application/rss+xml" />
	<link>http://certaindoubts.com/an-epistemic-newcomb-problem/</link>
	<description>devoted to matters epistemic</description>
	<lastBuildDate>Wed, 10 Apr 2019 16:37:28 +0000</lastBuildDate>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.10</generator>
	<item>
		<title>By: kvanvig</title>
		<link>http://certaindoubts.com/an-epistemic-newcomb-problem/#comment-7141</link>
		<dc:creator><![CDATA[kvanvig]]></dc:creator>
		<pubDate>Tue, 19 Dec 2006 18:57:28 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=641#comment-7141</guid>
		<description><![CDATA[Ralph, I&#039;m not really after two distinctions, I don&#039;t believe.  My reason for objecting to using the phrase &#039;rational to believe&#039; as you did earlier is that it is ambiguous between propositional and doxastic rationality.  The latter is the only distinction I&#039;m after, and the cases I described were only meant as counterexamples to the proposal you suggested.  The math counterexample presupposes, of course, that propositional rationality for a claim isn&#039;t implied by having evidence that entails that claim.  In short, propositional rationality isn&#039;t closed under entailment.

We need a notion of propositional rationality because it functions explanatorily.  A person sometimes believes contrary to his/her evidence, and thus believes irrationally; a person&#039;s withholding is sometimes irrational because he or she has evidence adequately supporting the claim or its denial.  The argument for taking propositional rationality as basic is basically a lesson learned from the conditional fallacy:  getting a belief into the picture when there isn&#039;t one encounters the problem that doing so can change the totality of evidence available and thus subvert the explanatory role that propositional rationality plays.]]></description>
		<content:encoded><![CDATA[<p>Ralph, I&#8217;m not really after two distinctions, I don&#8217;t believe.  My reason for objecting to using the phrase &#8216;rational to believe&#8217; as you did earlier is that it is ambiguous between propositional and doxastic rationality.  The latter is the only distinction I&#8217;m after, and the cases I described were only meant as counterexamples to the proposal you suggested.  The math counterexample presupposes, of course, that propositional rationality for a claim isn&#8217;t implied by having evidence that entails that claim.  In short, propositional rationality isn&#8217;t closed under entailment.</p>
<p>We need a notion of propositional rationality because it functions explanatorily.  A person sometimes believes contrary to his/her evidence, and thus believes irrationally; a person&#8217;s withholding is sometimes irrational because he or she has evidence adequately supporting the claim or its denial.  The argument for taking propositional rationality as basic is basically a lesson learned from the conditional fallacy:  getting a belief into the picture when there isn&#8217;t one encounters the problem that doing so can change the totality of evidence available and thus subvert the explanatory role that propositional rationality plays.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Ralph Wedgwood</title>
		<link>http://certaindoubts.com/an-epistemic-newcomb-problem/#comment-7140</link>
		<dc:creator><![CDATA[Ralph Wedgwood]]></dc:creator>
		<pubDate>Tue, 19 Dec 2006 11:05:59 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=641#comment-7140</guid>
		<description><![CDATA[I&#039;ll try to think some more about the points that have been made here by Steve and Gregory. But here&#039;s a quick response to the latest comments by Jonathan and Mark.

1. Jonathan insists on distinguishing between what one&#039;s current evidence supports and what a possible process of reasoning (starting out from one&#039;s current evidence) could lead one rationally to believe. He also seems to think that there is a second distinction between what a possible process of reasoning of this sort could lead one rationally to believe, and what it is rational for one to believe (since in his view, there are complex mathematical truths that it is not rational for him to believe, but which he could come rationally to believe via a possible course of reasoning).

Now, I can certainly recognize a distinction of the first sort. I was supposing that a proposition p is supported by one&#039;s evidence precisely to the extent that p is probable on that evidence, and then as I was claiming, there are cases in which one can rationally come to believe something even though it is not antecedently supported by one&#039;s evidence. Jonathan and I probably disagree about whether I should analyse the notion of what my evidence supports in probabilistic terms; but I don&#039;t think we disagree about the intelligibility of this distinction.


I could recognize *something* like Jonathan&#039;s second distinction as well. At least, there is a distinction between a proposition that a *short and easy* course of reasoning could lead one rationally to believe, and a proposition that some (possibly fantastically complex) course of reasoning could lead one rationally to believe. I&#039;m not convinced that any further distinction is needed to capture any real difference in this area.

2. Mark&#039;s objection, I think, overlooks the point that the relevant conditional probabilities are one&#039;s *current* probabilities, which must reflect one&#039;s total current evidence. E.g. my total current evidence includes an experience as of looking out over Christ Church meadow towards the river, and a complete lack any reasons to suspect my current experiences of being unreliable. Given this total evidence, the conditional probability that I am directly in front of a table, on the assumption that I believe that I am directly in front of a table, is still pretty low. If given all this evidence, I still believe that I am directly in front of a table, it is more probable that I have gone insane than that I am directly in front of a (presumably invisible) table!]]></description>
		<content:encoded><![CDATA[<p>I&#8217;ll try to think some more about the points that have been made here by Steve and Gregory. But here&#8217;s a quick response to the latest comments by Jonathan and Mark.</p>
<p>1. Jonathan insists on distinguishing between what one&#8217;s current evidence supports and what a possible process of reasoning (starting out from one&#8217;s current evidence) could lead one rationally to believe. He also seems to think that there is a second distinction between what a possible process of reasoning of this sort could lead one rationally to believe, and what it is rational for one to believe (since in his view, there are complex mathematical truths that it is not rational for him to believe, but which he could come rationally to believe via a possible course of reasoning).</p>
<p>Now, I can certainly recognize a distinction of the first sort. I was supposing that a proposition p is supported by one&#8217;s evidence precisely to the extent that p is probable on that evidence, and then as I was claiming, there are cases in which one can rationally come to believe something even though it is not antecedently supported by one&#8217;s evidence. Jonathan and I probably disagree about whether I should analyse the notion of what my evidence supports in probabilistic terms; but I don&#8217;t think we disagree about the intelligibility of this distinction.</p>
<p>I could recognize *something* like Jonathan&#8217;s second distinction as well. At least, there is a distinction between a proposition that a *short and easy* course of reasoning could lead one rationally to believe, and a proposition that some (possibly fantastically complex) course of reasoning could lead one rationally to believe. I&#8217;m not convinced that any further distinction is needed to capture any real difference in this area.</p>
<p>2. Mark&#8217;s objection, I think, overlooks the point that the relevant conditional probabilities are one&#8217;s *current* probabilities, which must reflect one&#8217;s total current evidence. E.g. my total current evidence includes an experience as of looking out over Christ Church meadow towards the river, and a complete lack any reasons to suspect my current experiences of being unreliable. Given this total evidence, the conditional probability that I am directly in front of a table, on the assumption that I believe that I am directly in front of a table, is still pretty low. If given all this evidence, I still believe that I am directly in front of a table, it is more probable that I have gone insane than that I am directly in front of a (presumably invisible) table!</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Mark van Roojen</title>
		<link>http://certaindoubts.com/an-epistemic-newcomb-problem/#comment-7139</link>
		<dc:creator><![CDATA[Mark van Roojen]]></dc:creator>
		<pubDate>Sat, 16 Dec 2006 20:58:06 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=641#comment-7139</guid>
		<description><![CDATA[Or better, I should have asked my question in #24 above in this form: Does it follow that I&#039;d be rational if I believed there is a table in front of me?

The self-correction seems needed since the judgement that there is no table in front of me is symmetrically situated - there would likely be no table in front of me if I judged there was not one. So it seems like the verdict here would be similar to that in the original story - that I&#039;d be rational whatever I believed.]]></description>
		<content:encoded><![CDATA[<p>Or better, I should have asked my question in #24 above in this form: Does it follow that I&#8217;d be rational if I believed there is a table in front of me?</p>
<p>The self-correction seems needed since the judgement that there is no table in front of me is symmetrically situated &#8211; there would likely be no table in front of me if I judged there was not one. So it seems like the verdict here would be similar to that in the original story &#8211; that I&#8217;d be rational whatever I believed.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Mark van Roojen</title>
		<link>http://certaindoubts.com/an-epistemic-newcomb-problem/#comment-7138</link>
		<dc:creator><![CDATA[Mark van Roojen]]></dc:creator>
		<pubDate>Sat, 16 Dec 2006 19:31:53 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=641#comment-7138</guid>
		<description><![CDATA[Ralph,

You wrote:

&quot;The relevant probability that should guide one in rationally forming one’s beliefs with respect to a given proposition p is not the unconditional probability of p given the evidence, but the conditional probability that p has on the assumption that one believes it.&quot;

Suppose I know that I&#039;m a pretty good tracker of truth in forming and holding my beliefs and that I&#039;m especially good at making judgements about medium sized physical objects.  If I believe there is a table in front of me I am almost certain to be right. Does it follow from your idea that I should believe that there is a table in front of me?  For, on the hypothesis that I believe it, there is almost certainly a table in front of me.

Or am I reading what you say in the quoted bit incorrectly?  I&#039;m probably missing something.]]></description>
		<content:encoded><![CDATA[<p>Ralph,</p>
<p>You wrote:</p>
<p>&#8220;The relevant probability that should guide one in rationally forming one’s beliefs with respect to a given proposition p is not the unconditional probability of p given the evidence, but the conditional probability that p has on the assumption that one believes it.&#8221;</p>
<p>Suppose I know that I&#8217;m a pretty good tracker of truth in forming and holding my beliefs and that I&#8217;m especially good at making judgements about medium sized physical objects.  If I believe there is a table in front of me I am almost certain to be right. Does it follow from your idea that I should believe that there is a table in front of me?  For, on the hypothesis that I believe it, there is almost certainly a table in front of me.</p>
<p>Or am I reading what you say in the quoted bit incorrectly?  I&#8217;m probably missing something.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: kvanvig</title>
		<link>http://certaindoubts.com/an-epistemic-newcomb-problem/#comment-7137</link>
		<dc:creator><![CDATA[kvanvig]]></dc:creator>
		<pubDate>Sat, 16 Dec 2006 13:28:47 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=641#comment-7137</guid>
		<description><![CDATA[The question isn&#039;t whether it is rational to believe the claim, but whether there is propositional justification or rational grounds for the claim in question.  That&#039;s the notion of ex ante justification in play here, and one surely can have grounds for the claim in question.  Maybe I just started squaring and I know I&#039;ve never thought of squaring anything bigger than a single-digit number, and I know that 12 isn&#039;t a single digit number.  That&#039;s excellent evidence for the claim in question.

There are other examples as well.  For one, take mathematical truths.  Some are rational for me to accept, some are rational for me to be agnostic about, and some I rationally disbelieve.  Sad to say, but true!  In every case, there is a line of reasoning that gets me to the truth, however, and using it would make the resulting belief rational.  Even so, rational false beliefs are possible in this domain.

The point is the same as I started with.  The line of reasoning you use may create or destroy evidence, and hence can&#039;t be relied on to get a good picture of what your actual evidence supports.  It&#039;s just another case of the conditional fallacy.]]></description>
		<content:encoded><![CDATA[<p>The question isn&#8217;t whether it is rational to believe the claim, but whether there is propositional justification or rational grounds for the claim in question.  That&#8217;s the notion of ex ante justification in play here, and one surely can have grounds for the claim in question.  Maybe I just started squaring and I know I&#8217;ve never thought of squaring anything bigger than a single-digit number, and I know that 12 isn&#8217;t a single digit number.  That&#8217;s excellent evidence for the claim in question.</p>
<p>There are other examples as well.  For one, take mathematical truths.  Some are rational for me to accept, some are rational for me to be agnostic about, and some I rationally disbelieve.  Sad to say, but true!  In every case, there is a line of reasoning that gets me to the truth, however, and using it would make the resulting belief rational.  Even so, rational false beliefs are possible in this domain.</p>
<p>The point is the same as I started with.  The line of reasoning you use may create or destroy evidence, and hence can&#8217;t be relied on to get a good picture of what your actual evidence supports.  It&#8217;s just another case of the conditional fallacy.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Ralph Wedgwood</title>
		<link>http://certaindoubts.com/an-epistemic-newcomb-problem/#comment-7136</link>
		<dc:creator><![CDATA[Ralph Wedgwood]]></dc:creator>
		<pubDate>Sat, 16 Dec 2006 11:01:15 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=641#comment-7136</guid>
		<description><![CDATA[Loads of interesting points here. For now, I&#039;ll just respond to Jonathan&#039;s alleged counterexample. I just deny that it is a counterexample. In my view, it would never be rational for me to believe that I have never considered the proposition that twelve squared is 144. Of course, it might be highly probable given my evidence that I have never considered this proposition; but according to me, that needn&#039;t make it true that it is rational for me to believe that I&#039;ve never considered this proposition.]]></description>
		<content:encoded><![CDATA[<p>Loads of interesting points here. For now, I&#8217;ll just respond to Jonathan&#8217;s alleged counterexample. I just deny that it is a counterexample. In my view, it would never be rational for me to believe that I have never considered the proposition that twelve squared is 144. Of course, it might be highly probable given my evidence that I have never considered this proposition; but according to me, that needn&#8217;t make it true that it is rational for me to believe that I&#8217;ve never considered this proposition.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/an-epistemic-newcomb-problem/#comment-7135</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Sat, 16 Dec 2006 04:28:27 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=641#comment-7135</guid>
		<description><![CDATA[Ralph: Let me try expanding my remark. You wrote that:

&lt;blockquote&gt;

The relevant probability that should guide one in rationally forming one’s beliefs with respect to a given proposition p is not the unconditional probability of p given the evidence, but the conditional probability that p has &lt;i&gt;on the assumption that one believes it.&lt;/i&gt;
&lt;/blockquote&gt;

I&#039;m not sure that the additional assumption, that the agent believes p, is telling. Rather, the agent may be viewed to be in an experimental setting whereby his having a belief about a parameter, the parity of number of coins, intervenes on the outcome of p. This cuts through quite a lot of the philosophy, but it seems to me that this is the basic mechanism underlying the agent&#039;s assessment of p.

Now, the event of believing p is important only because the agent also accepts that he is in a situation where so-believing has an effect on the outcome of p. His believing p is an intervention on the assignment of the parity of p. There doesn&#039;t seem to be anything special about the conditional probability of p given that he believes p; he accepts that this probability is 1 in virtue of accepting the story about Peter. Probability isn&#039;t doing any special work here; this is just one way to represent this piece of evidence about the uncertainty mechanism underlying assessments of p.

Notice that this point creates some problems for the claim that each of the possible beliefs about p are rational, that it doesn&#039;t matter whether the agent (i) believed p = odd, (ii) believed p = even, or (iii) suspended judgment about p.

I don&#039;t think the antecedent of (iii) would be a reasonable option for the agent, given the back story, and assuming that the agent accepts the part of the story about Peter. The agent accepts that his forming a belief about p will fix the value of p. So, why would he then suspend judgment about p? I thought this was the idea behind Jon&#039;s point; and it seems to go to the heart of the matter.]]></description>
		<content:encoded><![CDATA[<p>Ralph: Let me try expanding my remark. You wrote that:</p>
<blockquote>
<p>The relevant probability that should guide one in rationally forming one’s beliefs with respect to a given proposition p is not the unconditional probability of p given the evidence, but the conditional probability that p has <i>on the assumption that one believes it.</i>
</p></blockquote>
<p>I&#8217;m not sure that the additional assumption, that the agent believes p, is telling. Rather, the agent may be viewed to be in an experimental setting whereby his having a belief about a parameter, the parity of number of coins, intervenes on the outcome of p. This cuts through quite a lot of the philosophy, but it seems to me that this is the basic mechanism underlying the agent&#8217;s assessment of p.</p>
<p>Now, the event of believing p is important only because the agent also accepts that he is in a situation where so-believing has an effect on the outcome of p. His believing p is an intervention on the assignment of the parity of p. There doesn&#8217;t seem to be anything special about the conditional probability of p given that he believes p; he accepts that this probability is 1 in virtue of accepting the story about Peter. Probability isn&#8217;t doing any special work here; this is just one way to represent this piece of evidence about the uncertainty mechanism underlying assessments of p.</p>
<p>Notice that this point creates some problems for the claim that each of the possible beliefs about p are rational, that it doesn&#8217;t matter whether the agent (i) believed p = odd, (ii) believed p = even, or (iii) suspended judgment about p.</p>
<p>I don&#8217;t think the antecedent of (iii) would be a reasonable option for the agent, given the back story, and assuming that the agent accepts the part of the story about Peter. The agent accepts that his forming a belief about p will fix the value of p. So, why would he then suspend judgment about p? I thought this was the idea behind Jon&#8217;s point; and it seems to go to the heart of the matter.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/an-epistemic-newcomb-problem/#comment-7134</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Sat, 16 Dec 2006 03:44:07 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=641#comment-7134</guid>
		<description><![CDATA[Hi Carrie

You are right about symmetry in Ralph&#039;s example: in this case, it would seem that the agent&#039;s expectations for p and for not-p should be symmetric. (hmm; so much for my reading back to front...) It is not always the case, however, which would be a common sense point  for a traditional epistemologist, but a bone of contention for an orthodox Bayesian: the calculus throws off structural properties on &quot;rational belief&quot; that are neither reasonable nor have much to do with belief, which we&#039;ve known for a long time, but there is no longer a good reason to put up with these defects given the theory of imprecise probabilities. I mean, there are &lt;i&gt;new&lt;/i&gt; defects for us to chart and complain about!

But one of the things the theory of imprecise probabilities gets right, I think, is that it pulls distinguishes between uncertainty and ignorance. And this turns out to make the framework much more supple. There are cases where the agent may have a belief that p, Bel(p), which is neither equal to the belief that 1-Bel(not-p), nor symmetric, and the theory can represent this. For example, if your degree of ignorance about p is 0.2, it doesn&#039;t follow that your degree of ignorance about not-p must also be 0.2: you could have better evidence for your assessment of p, and have almost no evidence at all for not-p. Imprecise probabilities allow you to represent this difference in your evidence w.r.t. p and its complement.

Perhaps a more familiar example is a wish to distinguish between the case where there is a lot of evidence that a coin is fair, and so a lot of evidence for the frequency of tosses to land heads being 0.5, and the case where we don&#039;t know anything at all about the coin, and so also assign 0.5. On IP, you&#039;d assign [0,1] to the second case to represent your maximum degree of ignorance about the coin.]]></description>
		<content:encoded><![CDATA[<p>Hi Carrie</p>
<p>You are right about symmetry in Ralph&#8217;s example: in this case, it would seem that the agent&#8217;s expectations for p and for not-p should be symmetric. (hmm; so much for my reading back to front&#8230;) It is not always the case, however, which would be a common sense point  for a traditional epistemologist, but a bone of contention for an orthodox Bayesian: the calculus throws off structural properties on &#8220;rational belief&#8221; that are neither reasonable nor have much to do with belief, which we&#8217;ve known for a long time, but there is no longer a good reason to put up with these defects given the theory of imprecise probabilities. I mean, there are <i>new</i> defects for us to chart and complain about!</p>
<p>But one of the things the theory of imprecise probabilities gets right, I think, is that it pulls distinguishes between uncertainty and ignorance. And this turns out to make the framework much more supple. There are cases where the agent may have a belief that p, Bel(p), which is neither equal to the belief that 1-Bel(not-p), nor symmetric, and the theory can represent this. For example, if your degree of ignorance about p is 0.2, it doesn&#8217;t follow that your degree of ignorance about not-p must also be 0.2: you could have better evidence for your assessment of p, and have almost no evidence at all for not-p. Imprecise probabilities allow you to represent this difference in your evidence w.r.t. p and its complement.</p>
<p>Perhaps a more familiar example is a wish to distinguish between the case where there is a lot of evidence that a coin is fair, and so a lot of evidence for the frequency of tosses to land heads being 0.5, and the case where we don&#8217;t know anything at all about the coin, and so also assign 0.5. On IP, you&#8217;d assign [0,1] to the second case to represent your maximum degree of ignorance about the coin.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Carrie Jenkins</title>
		<link>http://certaindoubts.com/an-epistemic-newcomb-problem/#comment-7133</link>
		<dc:creator><![CDATA[Carrie Jenkins]]></dc:creator>
		<pubDate>Fri, 15 Dec 2006 23:51:26 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=641#comment-7133</guid>
		<description><![CDATA[Hi Greg,

Thanks for that.  Can you tell me a bit more about what you mean by cases where &quot;the agent is partially ignorant of the parameter, p&quot;.  Is Ralph&#039;s case like this?

(For clarity, I wasn&#039;t suggesting that we should generally have the same credence in p as not-p; just that in Ralph&#039;s case each is equally well-supported for us so I don&#039;t see how it could be rational to assign different values.)]]></description>
		<content:encoded><![CDATA[<p>Hi Greg,</p>
<p>Thanks for that.  Can you tell me a bit more about what you mean by cases where &#8220;the agent is partially ignorant of the parameter, p&#8221;.  Is Ralph&#8217;s case like this?</p>
<p>(For clarity, I wasn&#8217;t suggesting that we should generally have the same credence in p as not-p; just that in Ralph&#8217;s case each is equally well-supported for us so I don&#8217;t see how it could be rational to assign different values.)</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: kvanvig</title>
		<link>http://certaindoubts.com/an-epistemic-newcomb-problem/#comment-7132</link>
		<dc:creator><![CDATA[kvanvig]]></dc:creator>
		<pubDate>Fri, 15 Dec 2006 18:38:21 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=641#comment-7132</guid>
		<description><![CDATA[Ralph, there are lots of counterexamples to what you propose in response to my comment, but I&#039;ll just give one here.  Let p be the claim that I have never considered the claim that twelve squared is 144.  The totality of my evidence could yield justification that claim, but of course not if I believe it already.  And if I reason to this claim from the evidence I have for it, the addition of the belief will undermine the evidence for it.]]></description>
		<content:encoded><![CDATA[<p>Ralph, there are lots of counterexamples to what you propose in response to my comment, but I&#8217;ll just give one here.  Let p be the claim that I have never considered the claim that twelve squared is 144.  The totality of my evidence could yield justification that claim, but of course not if I believe it already.  And if I reason to this claim from the evidence I have for it, the addition of the belief will undermine the evidence for it.</p>
]]></content:encoded>
	</item>
</channel>
</rss>
