<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	>
<channel>
	<title>Comments on: Aptness entails safety</title>
	<atom:link href="http://certaindoubts.com/aptness-entails-safety/feed/" rel="self" type="application/rss+xml" />
	<link>http://certaindoubts.com/aptness-entails-safety/</link>
	<description>devoted to matters epistemic</description>
	<lastBuildDate>Wed, 10 Apr 2019 16:37:28 +0000</lastBuildDate>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.10</generator>
	<item>
		<title>By: Ralph Wedgwood</title>
		<link>http://certaindoubts.com/aptness-entails-safety/#comment-55922</link>
		<dc:creator><![CDATA[Ralph Wedgwood]]></dc:creator>
		<pubDate>Tue, 18 Feb 2014 03:22:33 +0000</pubDate>
		<guid isPermaLink="false">http://certaindoubts.com/?p=4283#comment-55922</guid>
		<description><![CDATA[Thanks again, Baron! You&#039;re right that I need to be careful about how I express my idea about &quot;normality&quot;. Indeed, I should have been more careful than I actually was, since ticket number &lt;em&gt;n&lt;/em&gt;&#039;s winning the lottery is undeniably a low-chance event, and one that is &quot;relevant to the truth value&quot; of my true belief that my ticket hasn&#039;t win. So perhaps I should just have said that in &quot;highly normal&quot; conditions, no low-chance events that &lt;em&gt;falsify&lt;/em&gt; any of my beliefs occur? (I will have to think about this some more.)

Of course, if I ever give a complete account of knowledge on the basis of these suggestions, I will have to base it on an account of &quot;rationality&quot;. Obviously I can&#039;t give an account of rationality now... But perhaps it would be helpful for me to make one comment. In fact, unlike Sosa, I&#039;m an &lt;em&gt;internalist&lt;/em&gt; about rationality, and so I don&#039;t think that rationality itself is necessarily a &quot;repeatably successful ... way of acquiring true beliefs&quot;: after all, evil demons can ensure that their victims&#039; rationality does &lt;em&gt;not&lt;/em&gt; function as a successful way of acquiring true beliefs.

Indeed, I wouldn&#039;t characterize rationality in terms of safety at all: after all, a whimsical demon might set out to Gettierize all of an agent&#039;s rational beliefs, in which case the agent&#039;s beliefs would be rational, but would not meet any standard of safety -- except for the very lowest standard of all. As I conceive of things, there is a completely independent norm of rationality, which can be invoked in giving an account of knowledge.]]></description>
		<content:encoded><![CDATA[<p>Thanks again, Baron! You&#8217;re right that I need to be careful about how I express my idea about &#8220;normality&#8221;. Indeed, I should have been more careful than I actually was, since ticket number <em>n</em>&#8216;s winning the lottery is undeniably a low-chance event, and one that is &#8220;relevant to the truth value&#8221; of my true belief that my ticket hasn&#8217;t win. So perhaps I should just have said that in &#8220;highly normal&#8221; conditions, no low-chance events that <em>falsify</em> any of my beliefs occur? (I will have to think about this some more.)</p>
<p>Of course, if I ever give a complete account of knowledge on the basis of these suggestions, I will have to base it on an account of &#8220;rationality&#8221;. Obviously I can&#8217;t give an account of rationality now&#8230; But perhaps it would be helpful for me to make one comment. In fact, unlike Sosa, I&#8217;m an <em>internalist</em> about rationality, and so I don&#8217;t think that rationality itself is necessarily a &#8220;repeatably successful &#8230; way of acquiring true beliefs&#8221;: after all, evil demons can ensure that their victims&#8217; rationality does <em>not</em> function as a successful way of acquiring true beliefs.</p>
<p>Indeed, I wouldn&#8217;t characterize rationality in terms of safety at all: after all, a whimsical demon might set out to Gettierize all of an agent&#8217;s rational beliefs, in which case the agent&#8217;s beliefs would be rational, but would not meet any standard of safety &#8212; except for the very lowest standard of all. As I conceive of things, there is a completely independent norm of rationality, which can be invoked in giving an account of knowledge.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Baron</title>
		<link>http://certaindoubts.com/aptness-entails-safety/#comment-55896</link>
		<dc:creator><![CDATA[Baron]]></dc:creator>
		<pubDate>Mon, 17 Feb 2014 19:59:02 +0000</pubDate>
		<guid isPermaLink="false">http://certaindoubts.com/?p=4283#comment-55896</guid>
		<description><![CDATA[Thanks again, Ralph! If I understand you correctly, it sounds like you would be willing to accept that I can know my lottery ticket will lose (when it will lose). This is so because it would be abnormal for my ticket to win. That’s fine with me, but you will need to be careful about how you express this point: “In what we could call &#039;highly normal&#039; conditions, no low-chance events that are relevant to the truth value of the agent’s beliefs occur.” No matter which ticket wins, it is a low-chance event. Similarly, no matter which crow I see next, it is a low-chance event that it has the genetic profile it does. Low-chance events happen all the time, even in highly normal conditions.

Regarding your suggestion at the end, that in super-low-contexts “safety collapses into mere true belief; and knowledge effectively collapses into rational true belief,” I think the collapse is going to go a little further than you are allowing. In your initial post, you proposed an explanation of what it means to say that a belief is correct because it is rational. That explanation, you said, implies a kind of safety, which you then characterized in the RN way. If safety collapses into mere true belief, though, then it is hard to see how to go about explaining why a belief is correct because rational. I take it part of the motivation for your initial post is that rationality should be a repeatably successful (in some sense) way of acquiring true beliefs. If we drop the repeatability part of it, then it will be difficult to make the case that rationality is really what explains the success. So, if safety collapses into mere true belief, the danger will be that knowledge likewise collapses into mere true belief. That&#039;s what seems absurd to me. But perhaps you could avoid this consequence by saying more about what rationality is (apart from our ability to characterize it in terms of safety).]]></description>
		<content:encoded><![CDATA[<p>Thanks again, Ralph! If I understand you correctly, it sounds like you would be willing to accept that I can know my lottery ticket will lose (when it will lose). This is so because it would be abnormal for my ticket to win. That’s fine with me, but you will need to be careful about how you express this point: “In what we could call &#8216;highly normal&#8217; conditions, no low-chance events that are relevant to the truth value of the agent’s beliefs occur.” No matter which ticket wins, it is a low-chance event. Similarly, no matter which crow I see next, it is a low-chance event that it has the genetic profile it does. Low-chance events happen all the time, even in highly normal conditions.</p>
<p>Regarding your suggestion at the end, that in super-low-contexts “safety collapses into mere true belief; and knowledge effectively collapses into rational true belief,” I think the collapse is going to go a little further than you are allowing. In your initial post, you proposed an explanation of what it means to say that a belief is correct because it is rational. That explanation, you said, implies a kind of safety, which you then characterized in the RN way. If safety collapses into mere true belief, though, then it is hard to see how to go about explaining why a belief is correct because rational. I take it part of the motivation for your initial post is that rationality should be a repeatably successful (in some sense) way of acquiring true beliefs. If we drop the repeatability part of it, then it will be difficult to make the case that rationality is really what explains the success. So, if safety collapses into mere true belief, the danger will be that knowledge likewise collapses into mere true belief. That&#8217;s what seems absurd to me. But perhaps you could avoid this consequence by saying more about what rationality is (apart from our ability to characterize it in terms of safety).</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Ralph Wedgwood</title>
		<link>http://certaindoubts.com/aptness-entails-safety/#comment-55852</link>
		<dc:creator><![CDATA[Ralph Wedgwood]]></dc:creator>
		<pubDate>Sun, 16 Feb 2014 23:43:12 +0000</pubDate>
		<guid isPermaLink="false">http://certaindoubts.com/?p=4283#comment-55852</guid>
		<description><![CDATA[Thanks, Baron! You say:
&lt;blockquote&gt;The only reason I can see for denying that the white crow case is just as similar is that the crow’s color is different from the color of the crow I actually see next.&lt;/blockquote&gt;
But in my previous comment, I tried to indicate a different reason for denying this:	


	1. The relevant dimension of similarity between cases is in what I called &quot;normality&quot; (specifically, both in the cases&#039; degree of normality and in the way in which the cases are normal to that degree).

	2. In what we could call &quot;highly normal&quot; conditions, no low-chance events that are relevant to the truth value of the agent&#039;s beliefs occur. So if the actual case is highly normal in this way, then no similar case will involve the occurrence of any such low-chance events.


The case that you describe, in which you irrationally believe that the next crow to be observed will be white, and -- amazingly -- it actually is white, is clearly not &quot;highly normal&quot; in this sense; on the contrary, it is highly abnormal. So the cases that are &quot;similar&quot; to this case in the relevant respects will include other cases that are abnormal in similar ways -- and in most contexts, these cases will include cases in which the next crow is not white.

That said, I am willing to allow that in principle there could be super-low-standards contexts in which the range of &quot;similar&quot; cases in the &quot;nearby&quot; possible worlds includes only the actual case. In these contexts, safety collapses into mere true belief; and knowledge effectively collapses into rational true belief. I suspect that we do sometimes use the word &#039;know&#039; like this -- although I also suspect that usually we require some non-trivial degree of safety instead. I don&#039;t see why this is &quot;absurd&quot; (as you put it)!]]></description>
		<content:encoded><![CDATA[<p>Thanks, Baron! You say:</p>
<blockquote><p>The only reason I can see for denying that the white crow case is just as similar is that the crow’s color is different from the color of the crow I actually see next.</p></blockquote>
<p>But in my previous comment, I tried to indicate a different reason for denying this:	</p>
<p>	1. The relevant dimension of similarity between cases is in what I called &#8220;normality&#8221; (specifically, both in the cases&#8217; degree of normality and in the way in which the cases are normal to that degree).</p>
<p>	2. In what we could call &#8220;highly normal&#8221; conditions, no low-chance events that are relevant to the truth value of the agent&#8217;s beliefs occur. So if the actual case is highly normal in this way, then no similar case will involve the occurrence of any such low-chance events.</p>
<p>The case that you describe, in which you irrationally believe that the next crow to be observed will be white, and &#8212; amazingly &#8212; it actually is white, is clearly not &#8220;highly normal&#8221; in this sense; on the contrary, it is highly abnormal. So the cases that are &#8220;similar&#8221; to this case in the relevant respects will include other cases that are abnormal in similar ways &#8212; and in most contexts, these cases will include cases in which the next crow is not white.</p>
<p>That said, I am willing to allow that in principle there could be super-low-standards contexts in which the range of &#8220;similar&#8221; cases in the &#8220;nearby&#8221; possible worlds includes only the actual case. In these contexts, safety collapses into mere true belief; and knowledge effectively collapses into rational true belief. I suspect that we do sometimes use the word &#8216;know&#8217; like this &#8212; although I also suspect that usually we require some non-trivial degree of safety instead. I don&#8217;t see why this is &#8220;absurd&#8221; (as you put it)!</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Baron</title>
		<link>http://certaindoubts.com/aptness-entails-safety/#comment-55836</link>
		<dc:creator><![CDATA[Baron]]></dc:creator>
		<pubDate>Sun, 16 Feb 2014 18:33:45 +0000</pubDate>
		<guid isPermaLink="false">http://certaindoubts.com/?p=4283#comment-55836</guid>
		<description><![CDATA[Thanks, Ralph. I think I have a better idea of what your view is now, though I think more will need to be said about how similarity is determined. In the case of a lottery, aren’t all of the cases in which various tickets win similar to the case that actually occurs? (Assuming the lottery is fair, of course.) If so, then the case in which I will see a white crow is just as similar to the actual case as the cases in which I will see black crows.

The only reason I can see for denying that the white crow case is just as similar is that the crow’s color is different from the color of the crow I actually see next. But if you tie similarity to the content of the belief in this way, safety is trivialized. Here’s an example that shows why. Suppose I see a white crow and come to believe that the next crow I’ll see is also white. Suppose further that this is actually true; despite being tremendously improbable, the next crow I will see actually is white. Is this belief safe? If this is a “low” context and we look only at the most similar cases, where similarity is determined by matching the content of the belief, then the relevant cases will be those nearby worlds in which the next crow I see is white. Hence, the belief counts as safe. But this seems absurd—if this belief is safe, then any true belief will be safe (at least in “low” contexts).]]></description>
		<content:encoded><![CDATA[<p>Thanks, Ralph. I think I have a better idea of what your view is now, though I think more will need to be said about how similarity is determined. In the case of a lottery, aren’t all of the cases in which various tickets win similar to the case that actually occurs? (Assuming the lottery is fair, of course.) If so, then the case in which I will see a white crow is just as similar to the actual case as the cases in which I will see black crows.</p>
<p>The only reason I can see for denying that the white crow case is just as similar is that the crow’s color is different from the color of the crow I actually see next. But if you tie similarity to the content of the belief in this way, safety is trivialized. Here’s an example that shows why. Suppose I see a white crow and come to believe that the next crow I’ll see is also white. Suppose further that this is actually true; despite being tremendously improbable, the next crow I will see actually is white. Is this belief safe? If this is a “low” context and we look only at the most similar cases, where similarity is determined by matching the content of the belief, then the relevant cases will be those nearby worlds in which the next crow I see is white. Hence, the belief counts as safe. But this seems absurd—if this belief is safe, then any true belief will be safe (at least in “low” contexts).</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Ralph Wedgwood</title>
		<link>http://certaindoubts.com/aptness-entails-safety/#comment-55787</link>
		<dc:creator><![CDATA[Ralph Wedgwood]]></dc:creator>
		<pubDate>Sat, 15 Feb 2014 22:24:29 +0000</pubDate>
		<guid isPermaLink="false">http://certaindoubts.com/?p=4283#comment-55787</guid>
		<description><![CDATA[Thanks Baron! You’re right that I should have said more about how the kind of safety that I’m proposing (“RN-safety” as I called it) can be combined with a contextualist approach. However, I suspect that you may be overlooking a detail in the way in which I articulated this kind of safety.

Specifically, as I articulated it, RN-safety involves, not just quantification over “nearby worlds”, but also quantification over “similar cases” within those nearby worlds. So, even if the &lt;em&gt;world&lt;/em&gt; where the next crow that I observe is an albino is just as “nearby” as the world where the next crow that I observe is black, it does not follow that the &lt;em&gt;case&lt;/em&gt; in which the next crow is an albino will count in the relevant context as a “similar” case.

In general, suppose that there is a case &lt;em&gt;C&lt;/em&gt;1 in which you believe &lt;em&gt;p&lt;/em&gt;, and in &lt;em&gt;C&lt;/em&gt;1 no very low-chance events that are relevant to the truth-value of your beliefs occur. (Of course, every case involves the occurrence of &lt;em&gt;some&lt;/em&gt; very low-chance events, but these events will typically be irrelevant to the truth value of your beliefs.) Then it seems that in some contexts, possible cases in which such highly-improbable events relevant to the truth value of your beliefs &lt;em&gt;do&lt;/em&gt; occur will not count as “similar” to &lt;em&gt;C&lt;/em&gt;1 with respect to the degree to which (and the way in which) the conditions are normal. Obviously, these will be the relaxed low-standards contexts, where many beliefs count as “safe”.

There will also be other contexts where cases in which low-chance events relevant to the truth value of your beliefs occur do count as “similar”; these will be strict high-standards contexts, where far fewer beliefs count as “safe”.]]></description>
		<content:encoded><![CDATA[<p>Thanks Baron! You’re right that I should have said more about how the kind of safety that I’m proposing (“RN-safety” as I called it) can be combined with a contextualist approach. However, I suspect that you may be overlooking a detail in the way in which I articulated this kind of safety.</p>
<p>Specifically, as I articulated it, RN-safety involves, not just quantification over “nearby worlds”, but also quantification over “similar cases” within those nearby worlds. So, even if the <em>world</em> where the next crow that I observe is an albino is just as “nearby” as the world where the next crow that I observe is black, it does not follow that the <em>case</em> in which the next crow is an albino will count in the relevant context as a “similar” case.</p>
<p>In general, suppose that there is a case <em>C</em>1 in which you believe <em>p</em>, and in <em>C</em>1 no very low-chance events that are relevant to the truth-value of your beliefs occur. (Of course, every case involves the occurrence of <em>some</em> very low-chance events, but these events will typically be irrelevant to the truth value of your beliefs.) Then it seems that in some contexts, possible cases in which such highly-improbable events relevant to the truth value of your beliefs <em>do</em> occur will not count as “similar” to <em>C</em>1 with respect to the degree to which (and the way in which) the conditions are normal. Obviously, these will be the relaxed low-standards contexts, where many beliefs count as “safe”.</p>
<p>There will also be other contexts where cases in which low-chance events relevant to the truth value of your beliefs occur do count as “similar”; these will be strict high-standards contexts, where far fewer beliefs count as “safe”.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Baron</title>
		<link>http://certaindoubts.com/aptness-entails-safety/#comment-55722</link>
		<dc:creator><![CDATA[Baron]]></dc:creator>
		<pubDate>Sat, 15 Feb 2014 01:24:25 +0000</pubDate>
		<guid isPermaLink="false">http://certaindoubts.com/?p=4283#comment-55722</guid>
		<description><![CDATA[Thanks for your reply, Ralph. I think contextualism by itself is not enough to give a satisfactory reply here; more would need to be said about proximity of worlds, too. As I was thinking of the crow case, the genetic pool of nearby crows allows for a low probability of there being an albino. Every genetic combination, whether it produces albinism or not, is individually improbable, but they are each equidistant from the actual world. In this sense, each combination is like a ticket in a large, fair lottery. Each ticket has the same, small chance of winning. Consequently, the various worlds in which these tickets win are equidistant.

My impression is that the most natural way to construe the stringency of standards in different contexts is that they govern different distances from the actual world. But if the world with a white crow is no more distant from the actual world than the worlds with black crows, lowering the epistemic standards so that more distant worlds are irrelevant is not going to prevent it from being relevant to my actual belief. It isn’t a more distant world.

I think you would need to have some way of connecting stringency of standards to something like preponderance of nearby worlds, though I imagine that would be a difficult thing to characterize.]]></description>
		<content:encoded><![CDATA[<p>Thanks for your reply, Ralph. I think contextualism by itself is not enough to give a satisfactory reply here; more would need to be said about proximity of worlds, too. As I was thinking of the crow case, the genetic pool of nearby crows allows for a low probability of there being an albino. Every genetic combination, whether it produces albinism or not, is individually improbable, but they are each equidistant from the actual world. In this sense, each combination is like a ticket in a large, fair lottery. Each ticket has the same, small chance of winning. Consequently, the various worlds in which these tickets win are equidistant.</p>
<p>My impression is that the most natural way to construe the stringency of standards in different contexts is that they govern different distances from the actual world. But if the world with a white crow is no more distant from the actual world than the worlds with black crows, lowering the epistemic standards so that more distant worlds are irrelevant is not going to prevent it from being relevant to my actual belief. It isn’t a more distant world.</p>
<p>I think you would need to have some way of connecting stringency of standards to something like preponderance of nearby worlds, though I imagine that would be a difficult thing to characterize.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Ralph Wedgwood</title>
		<link>http://certaindoubts.com/aptness-entails-safety/#comment-55700</link>
		<dc:creator><![CDATA[Ralph Wedgwood]]></dc:creator>
		<pubDate>Fri, 14 Feb 2014 16:38:13 +0000</pubDate>
		<guid isPermaLink="false">http://certaindoubts.com/?p=4283#comment-55700</guid>
		<description><![CDATA[Thanks, Baron! Let me reply, all too briefly, to your interesting comment.

It seems to me that the phenomena that you point to call for a broadly &lt;em&gt;contextualist&lt;/em&gt; treatment. In some contexts, it will be true to say that you &quot;know&quot; that the next crow to be observed will be black (just as it is normally true for you to say that you &quot;know&quot; where your car is). These will be &quot;low standards&quot; contexts, where it would also be true for physicists to say (as they doubtless did in informal conversations in the bar, etc.), &quot;We know that the Higgs boson exists&quot;, even before the team at CERN met the 5-sigma standard.

In other contexts, however, it would not have been true to say that you &quot;know&quot; that the next crow will be black, or that the scientists at CERN &quot;knew&quot; that the Higgs boson exists. These will be &quot;high standards&quot; contexts, where it is also not true for you to say that you &quot;know&quot; where your car is.

The difference between &quot;high standards&quot; and &quot;low standards&quot; cases depends on how inclusive the relevant class of cases is in which the believer must have a correct belief. The larger this class of cases, the higher the standard of safety becomes; the smaller this class, the lower the standard of safety becomes.

As I have formulated RN-safety, the class of cases in question depends &lt;em&gt;both&lt;/em&gt; on which worlds count as &quot;sufficiently nearby&quot; &lt;em&gt;and&lt;/em&gt; on which cases in those worlds count as &quot;similar&quot;, &lt;em&gt;both&lt;/em&gt; with respect to what makes the actual case a case of &quot;rational belief&quot; &lt;em&gt;and&lt;/em&gt; with respect to the degree and kind of &quot;normality&quot; that is exhibited by the actual case.

In the &quot;high standards&quot; contexts, a lot of worlds count as &quot;sufficiently nearby&quot; and a lot of cases count as &quot;similar&quot; in these respects; in the &quot;low standards&quot; contexts, many fewer worlds count as &quot;nearby&quot;, and fewer cases count as &quot;similar&quot;.]]></description>
		<content:encoded><![CDATA[<p>Thanks, Baron! Let me reply, all too briefly, to your interesting comment.</p>
<p>It seems to me that the phenomena that you point to call for a broadly <em>contextualist</em> treatment. In some contexts, it will be true to say that you &#8220;know&#8221; that the next crow to be observed will be black (just as it is normally true for you to say that you &#8220;know&#8221; where your car is). These will be &#8220;low standards&#8221; contexts, where it would also be true for physicists to say (as they doubtless did in informal conversations in the bar, etc.), &#8220;We know that the Higgs boson exists&#8221;, even before the team at CERN met the 5-sigma standard.</p>
<p>In other contexts, however, it would not have been true to say that you &#8220;know&#8221; that the next crow will be black, or that the scientists at CERN &#8220;knew&#8221; that the Higgs boson exists. These will be &#8220;high standards&#8221; contexts, where it is also not true for you to say that you &#8220;know&#8221; where your car is.</p>
<p>The difference between &#8220;high standards&#8221; and &#8220;low standards&#8221; cases depends on how inclusive the relevant class of cases is in which the believer must have a correct belief. The larger this class of cases, the higher the standard of safety becomes; the smaller this class, the lower the standard of safety becomes.</p>
<p>As I have formulated RN-safety, the class of cases in question depends <em>both</em> on which worlds count as &#8220;sufficiently nearby&#8221; <em>and</em> on which cases in those worlds count as &#8220;similar&#8221;, <em>both</em> with respect to what makes the actual case a case of &#8220;rational belief&#8221; <em>and</em> with respect to the degree and kind of &#8220;normality&#8221; that is exhibited by the actual case.</p>
<p>In the &#8220;high standards&#8221; contexts, a lot of worlds count as &#8220;sufficiently nearby&#8221; and a lot of cases count as &#8220;similar&#8221; in these respects; in the &#8220;low standards&#8221; contexts, many fewer worlds count as &#8220;nearby&#8221;, and fewer cases count as &#8220;similar&#8221;.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Baron</title>
		<link>http://certaindoubts.com/aptness-entails-safety/#comment-55678</link>
		<dc:creator><![CDATA[Baron]]></dc:creator>
		<pubDate>Fri, 14 Feb 2014 08:13:51 +0000</pubDate>
		<guid isPermaLink="false">http://certaindoubts.com/?p=4283#comment-55678</guid>
		<description><![CDATA[This is an interesting argument, Ralph. What would you say about the following objection?

RN-safety, as you have characterized it, is either too hard to satisfy or too easy. It depends on what you mean by “normal conditions.”

Suppose I have been observing crows for several years and have seen only black ones. I inductively infer that the next crow I will see will be black, too. Is this belief RN-safe? Albinism in crows is a rare but natural phenomenon. Let us suppose that the crows in my surrounding area constitute a genetic pool such that there is a low probability that a fully white albino crow has been hatched in recent years. If that is enough for the existence of a white crow to be normal in your sense, then there are worlds that are sufficiently nearby and sufficiently normal in which I could easily have had a false belief that the next crow I will see will be black. (I am assuming that this belief would be rational in just the way my actual belief is rational.) If so, my actual belief is not RN-safe. I take it this sort of case is not unusual, in the sense that many common phenomena have low-probability natural alternatives similar to albinism. So, this lack of RN-safety would generalize to a wide variety of empirical beliefs.

On the other hand, if you want to exclude low-probability natural alternatives like albinism from counting as normal conditions, RN-safety becomes too easy to satisfy. Part of what scientists want to do is to rule out such possibilities. To take one extreme example, physicists adopted a 5-sigma standard for claiming that the Higgs boson had been detected. The point in doing so was to rule out the possibility that they were observing low-probability phenomena that looked like Higgs bosons but were in fact something else.  Here’s a more prosaic example: in the thousands of times I have parked my car, it has been stolen only once. In this more stringent sense of normal conditions, it is not normal for my car to be stolen. Still, when I want to know where my car is, I do not carve out an exception for its having been stolen. That is, part of what I want to know is that it has not been stolen. Paying attention to RN-safety will not tell me that. So, knowledge cannot be understood in terms of RN-safety (where normal conditions exclude low-probability natural alternatives).]]></description>
		<content:encoded><![CDATA[<p>This is an interesting argument, Ralph. What would you say about the following objection?</p>
<p>RN-safety, as you have characterized it, is either too hard to satisfy or too easy. It depends on what you mean by “normal conditions.”</p>
<p>Suppose I have been observing crows for several years and have seen only black ones. I inductively infer that the next crow I will see will be black, too. Is this belief RN-safe? Albinism in crows is a rare but natural phenomenon. Let us suppose that the crows in my surrounding area constitute a genetic pool such that there is a low probability that a fully white albino crow has been hatched in recent years. If that is enough for the existence of a white crow to be normal in your sense, then there are worlds that are sufficiently nearby and sufficiently normal in which I could easily have had a false belief that the next crow I will see will be black. (I am assuming that this belief would be rational in just the way my actual belief is rational.) If so, my actual belief is not RN-safe. I take it this sort of case is not unusual, in the sense that many common phenomena have low-probability natural alternatives similar to albinism. So, this lack of RN-safety would generalize to a wide variety of empirical beliefs.</p>
<p>On the other hand, if you want to exclude low-probability natural alternatives like albinism from counting as normal conditions, RN-safety becomes too easy to satisfy. Part of what scientists want to do is to rule out such possibilities. To take one extreme example, physicists adopted a 5-sigma standard for claiming that the Higgs boson had been detected. The point in doing so was to rule out the possibility that they were observing low-probability phenomena that looked like Higgs bosons but were in fact something else.  Here’s a more prosaic example: in the thousands of times I have parked my car, it has been stolen only once. In this more stringent sense of normal conditions, it is not normal for my car to be stolen. Still, when I want to know where my car is, I do not carve out an exception for its having been stolen. That is, part of what I want to know is that it has not been stolen. Paying attention to RN-safety will not tell me that. So, knowledge cannot be understood in terms of RN-safety (where normal conditions exclude low-probability natural alternatives).</p>
]]></content:encoded>
	</item>
</channel>
</rss>
