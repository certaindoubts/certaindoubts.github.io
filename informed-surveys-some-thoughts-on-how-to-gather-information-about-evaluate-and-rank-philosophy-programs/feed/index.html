<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	>
<channel>
	<title>Comments on: Informed Surveys: Some Thoughts on How to Gather Information About, Evaluate, and Rank Philosophy Programs</title>
	<atom:link href="http://certaindoubts.com/informed-surveys-some-thoughts-on-how-to-gather-information-about-evaluate-and-rank-philosophy-programs/feed/" rel="self" type="application/rss+xml" />
	<link>http://certaindoubts.com/informed-surveys-some-thoughts-on-how-to-gather-information-about-evaluate-and-rank-philosophy-programs/</link>
	<description>devoted to matters epistemic</description>
	<lastBuildDate>Wed, 10 Apr 2019 16:37:28 +0000</lastBuildDate>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.10</generator>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/informed-surveys-some-thoughts-on-how-to-gather-information-about-evaluate-and-rank-philosophy-programs/#comment-7821</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Wed, 23 Jan 2008 13:28:48 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=775#comment-7821</guid>
		<description><![CDATA[It is the &#039;&lt;i&gt;put[ting] the departments back together and determine the departmental score as a function of the individual scores of the philosophers&lt;/i&gt;&#039; part that is the rub: you&#039;d need to say what function should be used and then explain/defend why.]]></description>
		<content:encoded><![CDATA[<p>It is the &#8216;<i>put[ting] the departments back together and determine the departmental score as a function of the individual scores of the philosophers</i>&#8216; part that is the rub: you&#8217;d need to say what function should be used and then explain/defend why.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Matt McAdam</title>
		<link>http://certaindoubts.com/informed-surveys-some-thoughts-on-how-to-gather-information-about-evaluate-and-rank-philosophy-programs/#comment-7820</link>
		<dc:creator><![CDATA[Matt McAdam]]></dc:creator>
		<pubDate>Tue, 22 Jan 2008 21:58:43 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=775#comment-7820</guid>
		<description><![CDATA[Why not for the first stage, instead of grouping philosophers by department, just list philosophers by AOS?  The idea would be to detemine, per philosopher, what kind of positive contribution he or she would make to a department, given his or her reputation.  People would only be required to evaluate people in their fields, or people with whose work they are familiar from any field.  Then Leiter or someone else could, as it were, put the departments back together and determine the departmental score as a function of the individual scores of the philosophers.]]></description>
		<content:encoded><![CDATA[<p>Why not for the first stage, instead of grouping philosophers by department, just list philosophers by AOS?  The idea would be to detemine, per philosopher, what kind of positive contribution he or she would make to a department, given his or her reputation.  People would only be required to evaluate people in their fields, or people with whose work they are familiar from any field.  Then Leiter or someone else could, as it were, put the departments back together and determine the departmental score as a function of the individual scores of the philosophers.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/informed-surveys-some-thoughts-on-how-to-gather-information-about-evaluate-and-rank-philosophy-programs/#comment-7819</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Thu, 27 Dec 2007 12:25:01 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=775#comment-7819</guid>
		<description><![CDATA[One candidate for a second stage is placement data. There might be terrific people at U State but lousy placement results. From a student&#039;s point of view, that would count against U State. From the profession&#039;s point of view, that misalignment might count against any number of things: halo effects on hiring practices, vagaries of fashion, the number of PhD programs, a preference for caste systems....

A solution to Academic Analytics is to be found in the kind of thing Jon&#039;s doing. If you find a basket of metrics that are positively correlated, and you find the groupings that result useful&#8212;for professional mobility, for guiding students to graduate programs, for settling needling questions of purity&#8212;then either the z-score will be in this basket or it won&#039;t.

And if it isn&#039;t? Then it might point to features that the profession should consider. But, more likely it will disagree because the metric is too coarse for measuring anything of interest in philosophy. Again, a case for embracing best practices: If the data is open and the methods transparent, then you can have this kind of discussion and be in a much better position for defending your interests.]]></description>
		<content:encoded><![CDATA[<p>One candidate for a second stage is placement data. There might be terrific people at U State but lousy placement results. From a student&#8217;s point of view, that would count against U State. From the profession&#8217;s point of view, that misalignment might count against any number of things: halo effects on hiring practices, vagaries of fashion, the number of PhD programs, a preference for caste systems&#8230;.</p>
<p>A solution to Academic Analytics is to be found in the kind of thing Jon&#8217;s doing. If you find a basket of metrics that are positively correlated, and you find the groupings that result useful&mdash;for professional mobility, for guiding students to graduate programs, for settling needling questions of purity&mdash;then either the z-score will be in this basket or it won&#8217;t.</p>
<p>And if it isn&#8217;t? Then it might point to features that the profession should consider. But, more likely it will disagree because the metric is too coarse for measuring anything of interest in philosophy. Again, a case for embracing best practices: If the data is open and the methods transparent, then you can have this kind of discussion and be in a much better position for defending your interests.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: kvanvig</title>
		<link>http://certaindoubts.com/informed-surveys-some-thoughts-on-how-to-gather-information-about-evaluate-and-rank-philosophy-programs/#comment-7818</link>
		<dc:creator><![CDATA[kvanvig]]></dc:creator>
		<pubDate>Thu, 27 Dec 2007 00:09:13 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=775#comment-7818</guid>
		<description><![CDATA[Greg, I had suggested above that maybe only subfield rankings by specialists in the field should be gathered, leaving it to some other group or program to combine the data into overall rankings.  There is a problem with this idea, however.  Think of students looking at the subfield rankings and trying to decide what to make of them.  To make an informed decision, they&#039;d want to know what the dominant view in the profession is about which areas are most important.  Keith&#039;s second stage of evaluation addresses that concern, even though it also raises the concerns you have.  But, if I were a student trying to decide where to apply, I&#039;d want the information in Keith&#039;s second stage, even knowing the methodological issues it raises.

I had thought that one might use a different type of second stage, where one asks raters simply to rate subfields in terms of their importance to the discipline, but that won&#039;t work.  MIT is a good example of why.  I&#039;d never discourage a student from going to MIT if their interests involved MIT&#039;s strengths, even though I doubt they would do well in terms of the distribution of subfields that would come out of a survey of the sort I imagine.

Keith and I are both concerned about data sources, but if you want to see something even more worrisome, look at the data being used by Academic Analytics.  There&#039;s some info in the Chronicle of Higher Ed about it, and they are gathering clients slowly at present, but (I predict) only slowly because the NRC rankings are about to come out.  As soon as they do and universities are looking for more readily available data, Academic Analytics will be a stronger force in academia, and the z-score that they generate gives some rather, errr, unusual results...]]></description>
		<content:encoded><![CDATA[<p>Greg, I had suggested above that maybe only subfield rankings by specialists in the field should be gathered, leaving it to some other group or program to combine the data into overall rankings.  There is a problem with this idea, however.  Think of students looking at the subfield rankings and trying to decide what to make of them.  To make an informed decision, they&#8217;d want to know what the dominant view in the profession is about which areas are most important.  Keith&#8217;s second stage of evaluation addresses that concern, even though it also raises the concerns you have.  But, if I were a student trying to decide where to apply, I&#8217;d want the information in Keith&#8217;s second stage, even knowing the methodological issues it raises.</p>
<p>I had thought that one might use a different type of second stage, where one asks raters simply to rate subfields in terms of their importance to the discipline, but that won&#8217;t work.  MIT is a good example of why.  I&#8217;d never discourage a student from going to MIT if their interests involved MIT&#8217;s strengths, even though I doubt they would do well in terms of the distribution of subfields that would come out of a survey of the sort I imagine.</p>
<p>Keith and I are both concerned about data sources, but if you want to see something even more worrisome, look at the data being used by Academic Analytics.  There&#8217;s some info in the Chronicle of Higher Ed about it, and they are gathering clients slowly at present, but (I predict) only slowly because the NRC rankings are about to come out.  As soon as they do and universities are looking for more readily available data, Academic Analytics will be a stronger force in academia, and the z-score that they generate gives some rather, errr, unusual results&#8230;</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/informed-surveys-some-thoughts-on-how-to-gather-information-about-evaluate-and-rank-philosophy-programs/#comment-7817</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Wed, 26 Dec 2007 23:32:08 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=775#comment-7817</guid>
		<description><![CDATA[Keith wrote about the problem of assessing the strength of people in fields outside his own: &lt;blockquote&gt; One thing I often do to address this problem is consult the PGR area rankings when deciding on what overall score to give the programs. I don’t know how legitimate that procedure will seem to people.&lt;/blockquote&gt;

If his practice is representative, then it would go some way toward explaining why the Leiter data records a &lt;a href=&quot;http://crookedtimber.org/2005/01/27/specialization-and-status-in-philosophy&quot; rel=&quot;nofollow&quot;&gt;high degree of consensus&lt;/a&gt;.

I think it is reasonable to complain a bit about this procedure: the task is for each evaluator to rank the departments, not for the evaluators to collude in assigning ranks. Looking at the specialty ranking information is (a form of) collusion.

Push-polling is another form of consensus-building; the hand-selection of evaluation panels is another one still.

I hope it is clear that I am complaining about methodology. The kind of variability in Kvanvig&#039;s data is more in line with what I would expect in a discipline like philosophy, where people spend an a lot  of time at conferences talking and disagreeing about (other people&#039;s) social position in the field. I don&#039;t think philosophers would do this if they actually were in a &quot;high consensus&quot; field. (Otherwise, why bother?) The Leiter data look more like reports of bankers or financiers assessing each other rather than philosophers.

I don&#039;t want to suggest that there is no point to a sociological study of the field. But I think scorn and derision should be heaped upon ranking methods that resist complete, transparent, data-driven assessments of the field.]]></description>
		<content:encoded><![CDATA[<p>Keith wrote about the problem of assessing the strength of people in fields outside his own: </p>
<blockquote><p> One thing I often do to address this problem is consult the PGR area rankings when deciding on what overall score to give the programs. I don’t know how legitimate that procedure will seem to people.</p></blockquote>
<p>If his practice is representative, then it would go some way toward explaining why the Leiter data records a <a href="http://crookedtimber.org/2005/01/27/specialization-and-status-in-philosophy" rel="nofollow">high degree of consensus</a>.</p>
<p>I think it is reasonable to complain a bit about this procedure: the task is for each evaluator to rank the departments, not for the evaluators to collude in assigning ranks. Looking at the specialty ranking information is (a form of) collusion.</p>
<p>Push-polling is another form of consensus-building; the hand-selection of evaluation panels is another one still.</p>
<p>I hope it is clear that I am complaining about methodology. The kind of variability in Kvanvig&#8217;s data is more in line with what I would expect in a discipline like philosophy, where people spend an a lot  of time at conferences talking and disagreeing about (other people&#8217;s) social position in the field. I don&#8217;t think philosophers would do this if they actually were in a &#8220;high consensus&#8221; field. (Otherwise, why bother?) The Leiter data look more like reports of bankers or financiers assessing each other rather than philosophers.</p>
<p>I don&#8217;t want to suggest that there is no point to a sociological study of the field. But I think scorn and derision should be heaped upon ranking methods that resist complete, transparent, data-driven assessments of the field.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: kvanvig</title>
		<link>http://certaindoubts.com/informed-surveys-some-thoughts-on-how-to-gather-information-about-evaluate-and-rank-philosophy-programs/#comment-7816</link>
		<dc:creator><![CDATA[kvanvig]]></dc:creator>
		<pubDate>Wed, 26 Dec 2007 13:31:23 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=775#comment-7816</guid>
		<description><![CDATA[Mike, the worry is that face-to-face experiences have disproportionate influence compared to actual significance.  There is data to this effect regarding interviews in comparison with more reliable, on-paper, data about job candidates, and I think the same phenomenon occurs regarding conferences in philosophy as opposed to written work.  Citation-based information is a surrogate for actually reading all the work in question, and not a completely reliable one even when the database is excellent, but it can correct for a disproportionate assessment of a person or department based on personal contact.]]></description>
		<content:encoded><![CDATA[<p>Mike, the worry is that face-to-face experiences have disproportionate influence compared to actual significance.  There is data to this effect regarding interviews in comparison with more reliable, on-paper, data about job candidates, and I think the same phenomenon occurs regarding conferences in philosophy as opposed to written work.  Citation-based information is a surrogate for actually reading all the work in question, and not a completely reliable one even when the database is excellent, but it can correct for a disproportionate assessment of a person or department based on personal contact.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: jon kvanvig</title>
		<link>http://certaindoubts.com/informed-surveys-some-thoughts-on-how-to-gather-information-about-evaluate-and-rank-philosophy-programs/#comment-7811</link>
		<dc:creator><![CDATA[jon kvanvig]]></dc:creator>
		<pubDate>Mon, 24 Dec 2007 21:20:49 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=775#comment-7811</guid>
		<description><![CDATA[Mike I had a glitch in the software:  below is the original reply I wrote to your question and tried to post, and just noticed this morning, after posting the next reply below, that it was routed so that administrator approval was required before it would show up.  That&#039;s really strange!  Reminds me of John Perry&#039;s story at the beginning of the paper about essential indexicals...

Mike, the point is that face-to-face contact tends to trump more reliable information about quality (such as actually reading a person&#039;s work), and some departments have better travel budgets than others, and some people like to travel more than others.  So these places and people secure an advantage in the ratings because of conference participation.  The point isn&#039;t that such participation is not a good thing and shouldn&#039;t count at all, but that it&#039;s value is likely to be disproportionate to its actual significance.  The allusion I made was, I think, to the studies that show that interviewing candidates tends to swamp more reliable, on paper, information about the quality of the candidates.  These studies have led Princeton, among others, to discontinue APA interviews, and we followed their lead when I was at Missouri.]]></description>
		<content:encoded><![CDATA[<p>Mike I had a glitch in the software:  below is the original reply I wrote to your question and tried to post, and just noticed this morning, after posting the next reply below, that it was routed so that administrator approval was required before it would show up.  That&#8217;s really strange!  Reminds me of John Perry&#8217;s story at the beginning of the paper about essential indexicals&#8230;</p>
<p>Mike, the point is that face-to-face contact tends to trump more reliable information about quality (such as actually reading a person&#8217;s work), and some departments have better travel budgets than others, and some people like to travel more than others.  So these places and people secure an advantage in the ratings because of conference participation.  The point isn&#8217;t that such participation is not a good thing and shouldn&#8217;t count at all, but that it&#8217;s value is likely to be disproportionate to its actual significance.  The allusion I made was, I think, to the studies that show that interviewing candidates tends to swamp more reliable, on paper, information about the quality of the candidates.  These studies have led Princeton, among others, to discontinue APA interviews, and we followed their lead when I was at Missouri.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Keith DeRose</title>
		<link>http://certaindoubts.com/informed-surveys-some-thoughts-on-how-to-gather-information-about-evaluate-and-rank-philosophy-programs/#comment-7813</link>
		<dc:creator><![CDATA[Keith DeRose]]></dc:creator>
		<pubDate>Mon, 24 Dec 2007 18:41:38 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=775#comment-7813</guid>
		<description><![CDATA[In response to K (comment #2): I myself would not advocate extending the PGR, as it&#039;s currently done, to cover all or even more of the graduate programs.  I believe that the rankings (&#038; here I&#039;m thinking mostly of the overall rankings) become less and less trustworthy as you move down the list to lower ranked programs.  (So, even among the limited number of programs currently ranked, I believe less confidence is appropriate in the bottom of the rankings than in the top.)  This is because the rankings are based on reputational surveys, and evaluators tend to have more of a basis for the scores they assign to the more highly regarded programs.  At what point, as you move down the list, you should stop doing the rankings is a very tricky call, and I admit that I have little confidence in my own inclinations as to where to draw that line.  (The worries of some skeptics about the PGR can be seen as a limiting case of this kind of worry I&#039;m expressing here.  Like me, they worry that evaluators don&#039;t have enough basis for the scores they assign, and like me, they may think this problem intensifies as one moves down the list of programs, but, unlike me, they think the problem is already bad enough at the top that we should stop doing these rankings before they begin.)  But my own inclination is not to think that the PGR should be extended to cover significantly more programs in the overall rankings.

However, I do believe that if a reputational survey were done in the way I&#039;m here suggesting, it would be more reliable further down the list, and in fact, even if it included all PhD programs, it would be more trustworthy at the bottom of that long list than the PGR currently is toward the bottom of its much shorter list.  This would be a very significant advantage because, as you point out, evaluations of the programs currently excluded from the PGR would be a very valuable service to many prospective students.]]></description>
		<content:encoded><![CDATA[<p>In response to K (comment #2): I myself would not advocate extending the PGR, as it&#8217;s currently done, to cover all or even more of the graduate programs.  I believe that the rankings (&amp; here I&#8217;m thinking mostly of the overall rankings) become less and less trustworthy as you move down the list to lower ranked programs.  (So, even among the limited number of programs currently ranked, I believe less confidence is appropriate in the bottom of the rankings than in the top.)  This is because the rankings are based on reputational surveys, and evaluators tend to have more of a basis for the scores they assign to the more highly regarded programs.  At what point, as you move down the list, you should stop doing the rankings is a very tricky call, and I admit that I have little confidence in my own inclinations as to where to draw that line.  (The worries of some skeptics about the PGR can be seen as a limiting case of this kind of worry I&#8217;m expressing here.  Like me, they worry that evaluators don&#8217;t have enough basis for the scores they assign, and like me, they may think this problem intensifies as one moves down the list of programs, but, unlike me, they think the problem is already bad enough at the top that we should stop doing these rankings before they begin.)  But my own inclination is not to think that the PGR should be extended to cover significantly more programs in the overall rankings.</p>
<p>However, I do believe that if a reputational survey were done in the way I&#8217;m here suggesting, it would be more reliable further down the list, and in fact, even if it included all PhD programs, it would be more trustworthy at the bottom of that long list than the PGR currently is toward the bottom of its much shorter list.  This would be a very significant advantage because, as you point out, evaluations of the programs currently excluded from the PGR would be a very valuable service to many prospective students.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Mike</title>
		<link>http://certaindoubts.com/informed-surveys-some-thoughts-on-how-to-gather-information-about-evaluate-and-rank-philosophy-programs/#comment-7812</link>
		<dc:creator><![CDATA[Mike]]></dc:creator>
		<pubDate>Mon, 24 Dec 2007 18:36:57 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=775#comment-7812</guid>
		<description><![CDATA[&lt;i&gt;part of what underlies the halo effect and the way in which conference participation and presentation has an effect on the rankings that citation information can help correct for&lt;/i&gt;

Jon,
I&#039;m missing the allusion here. I guess you&#039;re referring to the favorable bias produced by participation in (sponsoring of?) high profile (national?) conferences on a department&#039;s ranking. Is that it? I&#039;m not sure why that shouldn&#039;t count in favor of a department, so I&#039;m probably missing something.]]></description>
		<content:encoded><![CDATA[<p><i>part of what underlies the halo effect and the way in which conference participation and presentation has an effect on the rankings that citation information can help correct for</i></p>
<p>Jon,<br />
I&#8217;m missing the allusion here. I guess you&#8217;re referring to the favorable bias produced by participation in (sponsoring of?) high profile (national?) conferences on a department&#8217;s ranking. Is that it? I&#8217;m not sure why that shouldn&#8217;t count in favor of a department, so I&#8217;m probably missing something.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: K</title>
		<link>http://certaindoubts.com/informed-surveys-some-thoughts-on-how-to-gather-information-about-evaluate-and-rank-philosophy-programs/#comment-7814</link>
		<dc:creator><![CDATA[K]]></dc:creator>
		<pubDate>Mon, 24 Dec 2007 15:00:58 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=775#comment-7814</guid>
		<description><![CDATA[One of Jon’s comments in another post suggests that numerous folks are interested in having a ranking system that is more inclusive or maybe more refined.  Jon lists 99 PhD granting departments, 49 of which are not mentioned in the PGR.  So, one might say they are not included in the PGR or that they belong in an “unranked&quot; category in the PGR.  In either case, those 49 fall into a broad category.  Essentially half the departments fall in that category.  If rankings are in fact a service for potential grad students and for the profession, wouldn’t a more inclusive or refined ranking be a better service for potential grad students and the profession?  Issues of tractability aside, might this not be a way to improve the quality and usefulness of the PGR?]]></description>
		<content:encoded><![CDATA[<p>One of Jon’s comments in another post suggests that numerous folks are interested in having a ranking system that is more inclusive or maybe more refined.  Jon lists 99 PhD granting departments, 49 of which are not mentioned in the PGR.  So, one might say they are not included in the PGR or that they belong in an “unranked&#8221; category in the PGR.  In either case, those 49 fall into a broad category.  Essentially half the departments fall in that category.  If rankings are in fact a service for potential grad students and for the profession, wouldn’t a more inclusive or refined ranking be a better service for potential grad students and the profession?  Issues of tractability aside, might this not be a way to improve the quality and usefulness of the PGR?</p>
]]></content:encoded>
	</item>
</channel>
</rss>
