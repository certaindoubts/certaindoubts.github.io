<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	>
<channel>
	<title>Comments on: Modal Theories of Knowledge and Defeat (BLEG)</title>
	<atom:link href="http://certaindoubts.com/modal-theories-of-knowledge-and-defeat-bleg/feed/" rel="self" type="application/rss+xml" />
	<link>http://certaindoubts.com/modal-theories-of-knowledge-and-defeat-bleg/</link>
	<description>devoted to matters epistemic</description>
	<lastBuildDate>Wed, 10 Apr 2019 16:37:28 +0000</lastBuildDate>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.10</generator>
	<item>
		<title>By: Dougherty</title>
		<link>http://certaindoubts.com/modal-theories-of-knowledge-and-defeat-bleg/#comment-16591</link>
		<dc:creator><![CDATA[Dougherty]]></dc:creator>
		<pubDate>Fri, 24 Jun 2011 16:43:35 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=2767#comment-16591</guid>
		<description><![CDATA[Julien,

1. Re: &quot;Justification&quot; Hmmm, I don&#039;t follow.  I was assuming proper basing in my case so I was assuming the following theory of knowledge: K = PropJ+TB+proper basing.  Since proper basing + PropJ = DoxJ, K = DoxJTB.  That&#039;s what I was suggesting.

2a. Re: &quot;essential dependence&quot; and safety: I think we see things oppositely here perhaps, but I see no reason why learning of essential dependence on a falsehood entails learning that one&#039;s belief is unsafe, for (among other reasons) safety comes in degrees and one little falsehood might not make it too unsafe (indeed, even on a binary view, it needn&#039;t make it unsafe at all since there might be a benevolent demon around to make sure that any mistake you make still winds up with true belief.  Also, the wedge that has been driven between our own reasoning and the notion of safety, at least by Williamson, seems to protect my def from this worry.  

2b. Re: &quot;essential dependence&quot; and triviality.  (i) I don&#039;t yet see that the counter-example works structurally (I&#039;ll have to diagram it out later), but (ii) I don&#039;t yet see that in any case the person will essentially assume that.  The notion of essential dependence is spelled out in terms of premises and conclusions.  When you are reasoning to p, you don&#039;t assume that you know p.  That might come after, but that&#039;s no problem.

3a. Re: &quot;essential dependence&quot; and fake barns.  I don&#039;t yet see that this is the case.  For they are surely assuming the lemma that pen cap colors are a reliable guide to pen color.  I intend essential dependence to be somewhat psychologically involved, though I&#039;m still playing with the formulations. 

3b. Re: &quot;essential dependence&quot; in general.  I find the notion so clearly suggested by the actual cases, that I&#039;m not terribly worried if I can&#039;t capture it perfectly in a definition (though I haven&#039;t seen yet that there&#039;s anything wrong with my current formulations (though I am confident that I need to make it more clear where it&#039;s psychologically involved, that&#039;s unclear to me (I&#039;ve thought about this in conjunction with a reply to McGrew, but the draft is on hold))).

4. Re: *justified* belief rather than mere belief.  Why care about mere belief?!  :-)]]></description>
		<content:encoded><![CDATA[<p>Julien,</p>
<p>1. Re: &#8220;Justification&#8221; Hmmm, I don&#8217;t follow.  I was assuming proper basing in my case so I was assuming the following theory of knowledge: K = PropJ+TB+proper basing.  Since proper basing + PropJ = DoxJ, K = DoxJTB.  That&#8217;s what I was suggesting.</p>
<p>2a. Re: &#8220;essential dependence&#8221; and safety: I think we see things oppositely here perhaps, but I see no reason why learning of essential dependence on a falsehood entails learning that one&#8217;s belief is unsafe, for (among other reasons) safety comes in degrees and one little falsehood might not make it too unsafe (indeed, even on a binary view, it needn&#8217;t make it unsafe at all since there might be a benevolent demon around to make sure that any mistake you make still winds up with true belief.  Also, the wedge that has been driven between our own reasoning and the notion of safety, at least by Williamson, seems to protect my def from this worry.  </p>
<p>2b. Re: &#8220;essential dependence&#8221; and triviality.  (i) I don&#8217;t yet see that the counter-example works structurally (I&#8217;ll have to diagram it out later), but (ii) I don&#8217;t yet see that in any case the person will essentially assume that.  The notion of essential dependence is spelled out in terms of premises and conclusions.  When you are reasoning to p, you don&#8217;t assume that you know p.  That might come after, but that&#8217;s no problem.</p>
<p>3a. Re: &#8220;essential dependence&#8221; and fake barns.  I don&#8217;t yet see that this is the case.  For they are surely assuming the lemma that pen cap colors are a reliable guide to pen color.  I intend essential dependence to be somewhat psychologically involved, though I&#8217;m still playing with the formulations. </p>
<p>3b. Re: &#8220;essential dependence&#8221; in general.  I find the notion so clearly suggested by the actual cases, that I&#8217;m not terribly worried if I can&#8217;t capture it perfectly in a definition (though I haven&#8217;t seen yet that there&#8217;s anything wrong with my current formulations (though I am confident that I need to make it more clear where it&#8217;s psychologically involved, that&#8217;s unclear to me (I&#8217;ve thought about this in conjunction with a reply to McGrew, but the draft is on hold))).</p>
<p>4. Re: *justified* belief rather than mere belief.  Why care about mere belief?!  üôÇ</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: julien dutant</title>
		<link>http://certaindoubts.com/modal-theories-of-knowledge-and-defeat-bleg/#comment-16587</link>
		<dc:creator><![CDATA[julien dutant]]></dc:creator>
		<pubDate>Fri, 24 Jun 2011 12:26:20 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=2767#comment-16587</guid>
		<description><![CDATA[Hi Trent,

1. That seems to me playing on an ambiguity: knowledge is not (propositionally) justified true belief because it is (doxastically) justified belief! When you write, about the fake barn case, that in it you knows because ‚Äúyou followed your evidence properly and you got the right result‚Äù, that sounds to me like justified (i.e. personally justified, i.e. properly based on good but fallible evidence) +¬†true belief. So again, if one knows in the fake barn case, then knowledge is justified true belief.

 2. It seems to me that ED3 faces the triviality problem (1) of my comment en ED2 the problem (2). Re ED3: indeed if I were to learn that my belief is unsafe (that most of the pens with blue cap in the box are not blue pen) I wouldn&#039;t be justified to go on believing that that particular pen is blue. But that&#039;s because I would learn that I don&#039;t know, and should consequently suspend my belief. Using that to explain knowledge as justified undefeated/non-dependent-on-a-falsehood TB is getting things backwards, in my view. The reason why learning the unsafety of your belief defeats it is that safety is a condition on knowledge, and not: the reason why safety is a condition on knowledge is that learning the unsafety of your belief defeats it.

Re ED2: doesn&#039;t predict knowledge failure in the fake barn style of case. If anything, the subject is merely inferring that that pen is blue from the premise that that pen&#039;s cap is blue. (Or make it a Lehrerian Clever Reasoner if needed!) 

Small remark about the definitions ED3: why narrow them to *justified* belief essentially depending on a falsehood? Why not defining essential dependence on a falsehood for any belief (justified or not)?]]></description>
		<content:encoded><![CDATA[<p>Hi Trent,</p>
<p>1. That seems to me playing on an ambiguity: knowledge is not (propositionally) justified true belief because it is (doxastically) justified belief! When you write, about the fake barn case, that in it you knows because ‚Äúyou followed your evidence properly and you got the right result‚Äù, that sounds to me like justified (i.e. personally justified, i.e. properly based on good but fallible evidence) +¬†true belief. So again, if one knows in the fake barn case, then knowledge is justified true belief.</p>
<p> 2. It seems to me that ED3 faces the triviality problem (1) of my comment en ED2 the problem (2). Re ED3: indeed if I were to learn that my belief is unsafe (that most of the pens with blue cap in the box are not blue pen) I wouldn&#8217;t be justified to go on believing that that particular pen is blue. But that&#8217;s because I would learn that I don&#8217;t know, and should consequently suspend my belief. Using that to explain knowledge as justified undefeated/non-dependent-on-a-falsehood TB is getting things backwards, in my view. The reason why learning the unsafety of your belief defeats it is that safety is a condition on knowledge, and not: the reason why safety is a condition on knowledge is that learning the unsafety of your belief defeats it.</p>
<p>Re ED2: doesn&#8217;t predict knowledge failure in the fake barn style of case. If anything, the subject is merely inferring that that pen is blue from the premise that that pen&#8217;s cap is blue. (Or make it a Lehrerian Clever Reasoner if needed!) </p>
<p>Small remark about the definitions ED3: why narrow them to *justified* belief essentially depending on a falsehood? Why not defining essential dependence on a falsehood for any belief (justified or not)?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Dougherty</title>
		<link>http://certaindoubts.com/modal-theories-of-knowledge-and-defeat-bleg/#comment-16567</link>
		<dc:creator><![CDATA[Dougherty]]></dc:creator>
		<pubDate>Thu, 23 Jun 2011 19:29:36 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=2767#comment-16567</guid>
		<description><![CDATA[Julien,

1. It doesn&#039;t support K = JTB because there is also proper basing required, and that might entail reliability or safety.

2. ad 1) &#038; 2)  I think I have the start of a pretty good account of essential dependence.  Here are my top three candidates.

(ED2) S&#039;s inference from premises P to conclusion C essentially depends on falsehood F iff (i) S infers C from P, and (ii) it is not possibly the case that [(a) S infers C from P and (b) C is justified for S and (c) either suspension of judgement or disbelief is the attitude to F which fits S&#039;s evidence].

(ED3) Conclusion C&#039;s being justified for S essentially depends on falsehood F iff (i) C is justified for S and (ii) ~(S&#039;s evidence remains the same with the sole exception of learning that F is false &#038; C remains justified for S)

(ED3*) Conclusion C&#039;s being justified for S essentially depends on falsehood F iff (i) C is justified for S (ii) C is inferred (consciously or unconsciously) at least in part from F, and (iii) ~(S&#039;s evidence remains the same with the sole exception of learning that F is false &#038; C remains justified for S)]]></description>
		<content:encoded><![CDATA[<p>Julien,</p>
<p>1. It doesn&#8217;t support K = JTB because there is also proper basing required, and that might entail reliability or safety.</p>
<p>2. ad 1) &amp; 2)  I think I have the start of a pretty good account of essential dependence.  Here are my top three candidates.</p>
<p>(ED2) S&#8217;s inference from premises P to conclusion C essentially depends on falsehood F iff (i) S infers C from P, and (ii) it is not possibly the case that [(a) S infers C from P and (b) C is justified for S and (c) either suspension of judgement or disbelief is the attitude to F which fits S&#8217;s evidence].</p>
<p>(ED3) Conclusion C&#8217;s being justified for S essentially depends on falsehood F iff (i) C is justified for S and (ii) ~(S&#8217;s evidence remains the same with the sole exception of learning that F is false &amp; C remains justified for S)</p>
<p>(ED3*) Conclusion C&#8217;s being justified for S essentially depends on falsehood F iff (i) C is justified for S (ii) C is inferred (consciously or unconsciously) at least in part from F, and (iii) ~(S&#8217;s evidence remains the same with the sole exception of learning that F is false &amp; C remains justified for S)</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: julien dutant</title>
		<link>http://certaindoubts.com/modal-theories-of-knowledge-and-defeat-bleg/#comment-16549</link>
		<dc:creator><![CDATA[julien dutant]]></dc:creator>
		<pubDate>Thu, 23 Jun 2011 11:17:18 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=2767#comment-16549</guid>
		<description><![CDATA[Trent (re the fake barn cases): I find it really hard to speak of knowledge here - as already stated. But rather than hammering the intuition, at least it seems to me clear that if that&#039;s knowledge, then justified true belief is.  As you write: &quot;you followed your evidence properly and you got the right result&quot;!

I think that the &quot;essential dependence on a falsehood&quot; strategy fails for either of two ways:

1) Either it&#039;s trivial. You can argue that every time one believes p, they &quot;essentially assume&quot; that *they know* that p. And thus that they satisfy all the conditions of knowledge - whatever they are. When they realize that they don&#039;t, they (typically) withdraw their belief. That notion of essential dependence on a falsehood is guaranteed to get it right, but trivially so. (So in that case you get: the person knows only if most of the blue cap pens are blue; so they &quot;essentially assume&quot; it by assuming that they know; but it is false, so they essentially rely on a falsehood.)

2)¬†Either it has a substantial sense in which you don&#039;t essentially depend on a falsehood in fake-barn styles of cases. In my variant the subject is merely essentially relying on the following assumption: if *that pen*&#039;s cap is blue, then it&#039;s a blue pen.  That assumption may itself rest on the more general assumption that most blue cap pens are blue, but that&#039;s not essential to it supporting the belief that *that pen* is blue.]]></description>
		<content:encoded><![CDATA[<p>Trent (re the fake barn cases): I find it really hard to speak of knowledge here &#8211; as already stated. But rather than hammering the intuition, at least it seems to me clear that if that&#8217;s knowledge, then justified true belief is.  As you write: &#8220;you followed your evidence properly and you got the right result&#8221;!</p>
<p>I think that the &#8220;essential dependence on a falsehood&#8221; strategy fails for either of two ways:</p>
<p>1) Either it&#8217;s trivial. You can argue that every time one believes p, they &#8220;essentially assume&#8221; that *they know* that p. And thus that they satisfy all the conditions of knowledge &#8211; whatever they are. When they realize that they don&#8217;t, they (typically) withdraw their belief. That notion of essential dependence on a falsehood is guaranteed to get it right, but trivially so. (So in that case you get: the person knows only if most of the blue cap pens are blue; so they &#8220;essentially assume&#8221; it by assuming that they know; but it is false, so they essentially rely on a falsehood.)</p>
<p>2)¬†Either it has a substantial sense in which you don&#8217;t essentially depend on a falsehood in fake-barn styles of cases. In my variant the subject is merely essentially relying on the following assumption: if *that pen*&#8217;s cap is blue, then it&#8217;s a blue pen.  That assumption may itself rest on the more general assumption that most blue cap pens are blue, but that&#8217;s not essential to it supporting the belief that *that pen* is blue.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Dougherty</title>
		<link>http://certaindoubts.com/modal-theories-of-knowledge-and-defeat-bleg/#comment-16542</link>
		<dc:creator><![CDATA[Dougherty]]></dc:creator>
		<pubDate>Wed, 22 Jun 2011 21:54:34 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=2767#comment-16542</guid>
		<description><![CDATA[Dani, yes, I refer to that all the time, as you might imagine!  I think one has to be more liberal about it than he is, but at least there&#039;s that precedent!

I think Feldman makes a compelling case from the fallibilist perspective relevant in his Prentice Hall text _Epistemology_, p. 125, last full paragraph.  It&#039;s my guiding light.]]></description>
		<content:encoded><![CDATA[<p>Dani, yes, I refer to that all the time, as you might imagine!  I think one has to be more liberal about it than he is, but at least there&#8217;s that precedent!</p>
<p>I think Feldman makes a compelling case from the fallibilist perspective relevant in his Prentice Hall text _Epistemology_, p. 125, last full paragraph.  It&#8217;s my guiding light.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Dani Rabinowitz</title>
		<link>http://certaindoubts.com/modal-theories-of-knowledge-and-defeat-bleg/#comment-16541</link>
		<dc:creator><![CDATA[Dani Rabinowitz]]></dc:creator>
		<pubDate>Wed, 22 Jun 2011 20:52:18 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=2767#comment-16541</guid>
		<description><![CDATA[Trent,

Duncan Pritchard has a distinction between benign and malignant forms of epistemic luck in his book Epistemic Luck that may be of relevance to your point.]]></description>
		<content:encoded><![CDATA[<p>Trent,</p>
<p>Duncan Pritchard has a distinction between benign and malignant forms of epistemic luck in his book Epistemic Luck that may be of relevance to your point.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Dougherty</title>
		<link>http://certaindoubts.com/modal-theories-of-knowledge-and-defeat-bleg/#comment-16537</link>
		<dc:creator><![CDATA[Dougherty]]></dc:creator>
		<pubDate>Wed, 22 Jun 2011 16:46:31 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=2767#comment-16537</guid>
		<description><![CDATA[Dani,

Thanks for the quote.  I can&#039;t say it looks at all promising to me, but I will definitely follow up on it.  Thanks also for letting us know about your entry.  Here&#039;s the link for people&#039;s convenience: http://oxford.academia.edu/DaniRabinowitz/Papers



Julien,

 I do think it&#039;s knowledge in this case, for I think that kind of luck does not threaten knowledge.  That might sound odd to you, so let me gesture, briefly, at why I think that.  Suppose your car is unreliable: it starts less than half the time.  You turn the key with hope and the engine fires.  You got lucky!  But the car started just the same as a perfect car.  I think knowing is often like that.  OK, there was some kind of luck involved (we already knew not all luck is knowledge-destroying), but you followed your evidence properly and you got the right result.  It worked out in a way that it doesn&#039;t seem standard Gettier cases don&#039;t seem to.  

But if it doesn&#039;t count as knowledge, it&#039;s easy to explain why without &quot;going modal,&quot; for there is essential dependence on a falsehood: That most of these blue-capped pens are blue pens.

So either way, I see no reason to go modal from examples like that.  I deny the fundamental starting point for modalism: that Gettier cases are case of luck.




Martin,

I agree that it is really important to separate the issues (and it&#039;s amazing what a linch-pin E=K is for Williamson), and you raise an interesting possibility which I want to think about.]]></description>
		<content:encoded><![CDATA[<p>Dani,</p>
<p>Thanks for the quote.  I can&#8217;t say it looks at all promising to me, but I will definitely follow up on it.  Thanks also for letting us know about your entry.  Here&#8217;s the link for people&#8217;s convenience: <a href="http://oxford.academia.edu/DaniRabinowitz/Papers" rel="nofollow">http://oxford.academia.edu/DaniRabinowitz/Papers</a></p>
<p>Julien,</p>
<p> I do think it&#8217;s knowledge in this case, for I think that kind of luck does not threaten knowledge.  That might sound odd to you, so let me gesture, briefly, at why I think that.  Suppose your car is unreliable: it starts less than half the time.  You turn the key with hope and the engine fires.  You got lucky!  But the car started just the same as a perfect car.  I think knowing is often like that.  OK, there was some kind of luck involved (we already knew not all luck is knowledge-destroying), but you followed your evidence properly and you got the right result.  It worked out in a way that it doesn&#8217;t seem standard Gettier cases don&#8217;t seem to.  </p>
<p>But if it doesn&#8217;t count as knowledge, it&#8217;s easy to explain why without &#8220;going modal,&#8221; for there is essential dependence on a falsehood: That most of these blue-capped pens are blue pens.</p>
<p>So either way, I see no reason to go modal from examples like that.  I deny the fundamental starting point for modalism: that Gettier cases are case of luck.</p>
<p>Martin,</p>
<p>I agree that it is really important to separate the issues (and it&#8217;s amazing what a linch-pin E=K is for Williamson), and you raise an interesting possibility which I want to think about.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Dani Rabinowitz</title>
		<link>http://certaindoubts.com/modal-theories-of-knowledge-and-defeat-bleg/#comment-16534</link>
		<dc:creator><![CDATA[Dani Rabinowitz]]></dc:creator>
		<pubDate>Wed, 22 Jun 2011 13:06:07 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=2767#comment-16534</guid>
		<description><![CDATA[Hi All

I have an entry coming out shortly on the safety condition for knowledge in the Internet Encyclopedia of Philosophy. All of the above worries are dealt with in that entry.
The entry is meant to elucidate the safety condition from A-Z for those who are not acquainted with safety. The penultimate draft is available on my website. Comments welcomed and appreciated. 

Dani]]></description>
		<content:encoded><![CDATA[<p>Hi All</p>
<p>I have an entry coming out shortly on the safety condition for knowledge in the Internet Encyclopedia of Philosophy. All of the above worries are dealt with in that entry.<br />
The entry is meant to elucidate the safety condition from A-Z for those who are not acquainted with safety. The penultimate draft is available on my website. Comments welcomed and appreciated. </p>
<p>Dani</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Martin Smith</title>
		<link>http://certaindoubts.com/modal-theories-of-knowledge-and-defeat-bleg/#comment-16533</link>
		<dc:creator><![CDATA[Martin Smith]]></dc:creator>
		<pubDate>Wed, 22 Jun 2011 11:28:00 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=2767#comment-16533</guid>
		<description><![CDATA[Julien - I completely agree with your intuitions here.

The point is just that there may be a *recipe* for comparing the safety of any two beliefs held at the same world w - so long we assume that all worlds are comparable for their similarity to w.  If we work with centered worlds instead then something similar should be possible for beliefs held at the same world by the same subject at the same time.

I can&#039;t think of a general recipe for comparing the safety of beliefs held at different worlds - but that&#039;s not to say that such comparisons can never be made.  I agree that they can - and indeed that they can be a lot more natural than certain intra-world (and intra-subject) comparisons.

One last thing, though...I&#039;m not sure that we have to think about the pizza case as involving an inter-word comparison.  Let E1 be the proposition that I remember eating a pizza two days ago, E2 be the proposition that my food diary says that I ate a pizza two days ago and P be the proposition that I ate a pizza two days ago.  Even though my actual belief is based just on E1, we can observe that, from the perspective of the actual world, the most similar worlds in which E1 and E2 are true and P is false are less similar than the most similar worlds in which E1 is true and P is false.  This would just be an intra-world comparison.]]></description>
		<content:encoded><![CDATA[<p>Julien &#8211; I completely agree with your intuitions here.</p>
<p>The point is just that there may be a *recipe* for comparing the safety of any two beliefs held at the same world w &#8211; so long we assume that all worlds are comparable for their similarity to w.  If we work with centered worlds instead then something similar should be possible for beliefs held at the same world by the same subject at the same time.</p>
<p>I can&#8217;t think of a general recipe for comparing the safety of beliefs held at different worlds &#8211; but that&#8217;s not to say that such comparisons can never be made.  I agree that they can &#8211; and indeed that they can be a lot more natural than certain intra-world (and intra-subject) comparisons.</p>
<p>One last thing, though&#8230;I&#8217;m not sure that we have to think about the pizza case as involving an inter-word comparison.  Let E1 be the proposition that I remember eating a pizza two days ago, E2 be the proposition that my food diary says that I ate a pizza two days ago and P be the proposition that I ate a pizza two days ago.  Even though my actual belief is based just on E1, we can observe that, from the perspective of the actual world, the most similar worlds in which E1 and E2 are true and P is false are less similar than the most similar worlds in which E1 is true and P is false.  This would just be an intra-world comparison.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: julien dutant</title>
		<link>http://certaindoubts.com/modal-theories-of-knowledge-and-defeat-bleg/#comment-16515</link>
		<dc:creator><![CDATA[julien dutant]]></dc:creator>
		<pubDate>Tue, 21 Jun 2011 13:33:53 +0000</pubDate>
		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=2767#comment-16515</guid>
		<description><![CDATA[Dani: thanks, that seems to be the distinction i sketched above, I should have remembered that bit! 

Martin: thanks! I agree with most points, except one: the idea that comparisons of safety should somehow be much easier from the same center (either the same world where two beliefs are held, or the same centered world where two beliefs are held). 

Take my two cases in a comment above, and make them a single centred world. Say I ate a pizza two days ago and I&#039;m currently watching a live speech by some famous politician on TV and I currently believe that (p) I ate a pizza two days ago and also that (q) that politician is presently talking. To compare the safety of the two, we&#039;d have to say which of the closest p-error case (where I mistake three days ago for two days ago, for instance) and the closest q-error case (where I mistakenly take the speech to be live, for instance). That seems to me harder than to compare the safety of my actual belief in p with the safety of my belief in p in a counterfactual case which is similar except that I additionally just checked on my food diary that I had a pizza two days ago. In the first case the closest error case is one in which my memory went wrong; in the second case the closest error case is one in which both my memory and my diary are mistaken.]]></description>
		<content:encoded><![CDATA[<p>Dani: thanks, that seems to be the distinction i sketched above, I should have remembered that bit! </p>
<p>Martin: thanks! I agree with most points, except one: the idea that comparisons of safety should somehow be much easier from the same center (either the same world where two beliefs are held, or the same centered world where two beliefs are held). </p>
<p>Take my two cases in a comment above, and make them a single centred world. Say I ate a pizza two days ago and I&#8217;m currently watching a live speech by some famous politician on TV and I currently believe that (p) I ate a pizza two days ago and also that (q) that politician is presently talking. To compare the safety of the two, we&#8217;d have to say which of the closest p-error case (where I mistake three days ago for two days ago, for instance) and the closest q-error case (where I mistakenly take the speech to be live, for instance). That seems to me harder than to compare the safety of my actual belief in p with the safety of my belief in p in a counterfactual case which is similar except that I additionally just checked on my food diary that I had a pizza two days ago. In the first case the closest error case is one in which my memory went wrong; in the second case the closest error case is one in which both my memory and my diary are mistaken.</p>
]]></content:encoded>
	</item>
</channel>
</rss>
