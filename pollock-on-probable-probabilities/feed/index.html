<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	>
<channel>
	<title>Comments on: Pollock on Probable Probabilities</title>
	<atom:link href="http://certaindoubts.com/pollock-on-probable-probabilities/feed/" rel="self" type="application/rss+xml" />
	<link>http://certaindoubts.com/pollock-on-probable-probabilities/</link>
	<description>devoted to matters epistemic</description>
	<lastBuildDate>Wed, 10 Apr 2019 16:37:28 +0000</lastBuildDate>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.10</generator>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/pollock-on-probable-probabilities/#comment-3867</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Tue, 05 Sep 2006 23:03:49 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=612#comment-3867</guid>
		<description><![CDATA[More on (2): Suppose binary operations &#038; and v satisfy the following three laws:

L1 &lt;b&gt;[commutative]&lt;/b&gt; A &#038; B = B &#038; A; A v B = B v A,
L2 &lt;b&gt;[associative]&lt;/b&gt; A &#038; (B &#038; C) = (A &#038; B) &#038; C; A v (B v C) = (A v B) v C,
L3 &lt;b&gt;[absorption law]&lt;/b&gt; A &#038; (B v A) = A v (B &#038; A) = A,

then the ordering mentioned in (2) falls out, since L1-L3 define a lattice in terms of meet and join. Note that this is more general than the truth tables for boolean &#038; and v, includes intutionistic logic, most multi-valued logics (that I know of). It does not characterize &#038; and v in either linear or relevant logic, however.

And (pure) probability logics do not satisfy L3. (Exercise: Let A = 0.6, and B=0.7. All pairwise identities of L3 fail, using (i) and (ii).)

Moreover, and this is neat, notice how A &#038; (B v A) and A v (A &#038; B) smear the value for A. This is happening with *point* values. This is behavior similar to Seidenfeld and Wasserman&#039;s dilation result, which is supposed to be a problem for interval-valued probability and conditionalization. But we get it here with precise values. Yes, we introduce intervals when we evaluate (A v B), and (A &#038; B), respectively; but the reason we get to these values is not by embracing imprecise probability theory, but by insisting upon pure logical connectives.

We can avoid this by assuming independence, which most probability logics do because the system I&#039;ve described here, though pure, is not terribly usefly. But, it does throw some light on the non-triviality of independence assumptions.

(3) is still not right. There is another point about working with sets of measures, convex sets of measure, or simply  a single measure, but (3) starts out on the wrong foot to make that work out.]]></description>
		<content:encoded><![CDATA[<p>More on (2): Suppose binary operations &amp; and v satisfy the following three laws:</p>
<p>L1 <b>[commutative]</b> A &amp; B = B &amp; A; A v B = B v A,<br />
L2 <b>[associative]</b> A &amp; (B &amp; C) = (A &amp; B) &amp; C; A v (B v C) = (A v B) v C,<br />
L3 <b>[absorption law]</b> A &amp; (B v A) = A v (B &amp; A) = A,</p>
<p>then the ordering mentioned in (2) falls out, since L1-L3 define a lattice in terms of meet and join. Note that this is more general than the truth tables for boolean &amp; and v, includes intutionistic logic, most multi-valued logics (that I know of). It does not characterize &amp; and v in either linear or relevant logic, however.</p>
<p>And (pure) probability logics do not satisfy L3. (Exercise: Let A = 0.6, and B=0.7. All pairwise identities of L3 fail, using (i) and (ii).)</p>
<p>Moreover, and this is neat, notice how A &amp; (B v A) and A v (A &amp; B) smear the value for A. This is happening with *point* values. This is behavior similar to Seidenfeld and Wasserman&#8217;s dilation result, which is supposed to be a problem for interval-valued probability and conditionalization. But we get it here with precise values. Yes, we introduce intervals when we evaluate (A v B), and (A &amp; B), respectively; but the reason we get to these values is not by embracing imprecise probability theory, but by insisting upon pure logical connectives.</p>
<p>We can avoid this by assuming independence, which most probability logics do because the system I&#8217;ve described here, though pure, is not terribly usefly. But, it does throw some light on the non-triviality of independence assumptions.</p>
<p>(3) is still not right. There is another point about working with sets of measures, convex sets of measure, or simply  a single measure, but (3) starts out on the wrong foot to make that work out.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/pollock-on-probable-probabilities/#comment-3869</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Mon, 28 Aug 2006 14:01:55 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=612#comment-3869</guid>
		<description><![CDATA[On second thought, change (3) to the open interval (0,1) and &#039;completely ignorant&#039; to &#039;almost completely ignorant&#039;.]]></description>
		<content:encoded><![CDATA[<p>On second thought, change (3) to the open interval (0,1) and &#8216;completely ignorant&#8217; to &#8216;almost completely ignorant&#8217;.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/pollock-on-probable-probabilities/#comment-3868</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Mon, 28 Aug 2006 13:26:12 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=612#comment-3868</guid>
		<description><![CDATA[John&#039;s point, and Tim McGrew&#039;s correction, generalize; for the reason that there is no single set of connectives for probability logics is that the probabilistic relationship(s) between two events, A, B, determine(s) the probability of the joint event A cap B.  There is very little logical structure to provide guidance, and what little there is was hit upon in his correction. To illustrate, consider:

If A and B are &lt;b&gt;independent&lt;/b&gt;, then prob(A &#038; B) = prob(A) x prob(B);
If A and B are &lt;b&gt;mutually exclusive&lt;/b&gt;, then prob(A &#038; B) = 0;
If A and B are &lt;b&gt;positively correlated&lt;/b&gt; (If A entails B), then prob(A &#038; B) = prob(A).
If A and B are &lt;b&gt;negatively correlated&lt;/b&gt; (If A entails not-B), then prob(A v B) = min(1, prob(A) + prob(B)).

Some interesting things follow:

(1) A pair of events may be mutually exclusive but not negatively correlated, so
there is no interdefinability of &#038; and v in a probabilistic logic.

(2) Probability logic is &lt;i&gt;not&lt;/i&gt; a type of multi-valued propositional logic, since the connectives &#038; and v cannot be characterized by the lattice properties of boolean &#038; and v (i.e., &#038; := min [f(A), v(B)]; v := max[f(A), f(B)], where &#039;f&#039; is a generic valuation function; substitute &#039;prob&#039; for &#039;f&#039; here).

(3) Two events A, B, can each have positive probability but the prob(A &#038;B) = [0,1]; in words, we can have precise probability assessments for each of A and B, but be completely ignorant about the probability of (A and B).

From the point of view of a probabilist, there is a temptation to claim that there simply is no logical structure to probability logics and that the notion of a probability logic (progic) is nonsensical: to get anything out of calculations with probabilities, one has to make substantive assumptions about the relationships between the events we wish to reason about. In some cases these assumptions are warranted, in others they are not. This point (perhaps) crystalizes the difference between bayesian &lt;i&gt;statistics&lt;/i&gt; and bayesian &lt;i&gt;epistemology&lt;/i&gt;: it is not necessarily a failure of statistical analysis to admit that we don&#039;t always have information about joint distributions.

But the point about progics have no interesting struction is too rash, and Tim McGrew&#039;s comment points out why. To generalize, for arbitrary events A, B, represented in a single probability structure M, if prob(A) and prob(B) are defined in M, then:

(i) prob(A &#038; B) lies within the interval [max(0, prob(A) + prob(B) -1), min(prob(A), prob(B)], and

(ii) prob(A v B) lies within the interval [max(prob(A), prob(B), min(prob(A)+ prob(B), 1)].

These are weak bounds, and trvialize quickly; but, they are not necessarily trivial, and they are derived without any substantive assumption about the relationship between the events A and B other than that they be represented in the algebra M. (This bound holds for lower/upper probability defined on M, too.)]]></description>
		<content:encoded><![CDATA[<p>John&#8217;s point, and Tim McGrew&#8217;s correction, generalize; for the reason that there is no single set of connectives for probability logics is that the probabilistic relationship(s) between two events, A, B, determine(s) the probability of the joint event A cap B.  There is very little logical structure to provide guidance, and what little there is was hit upon in his correction. To illustrate, consider:</p>
<p>If A and B are <b>independent</b>, then prob(A &amp; B) = prob(A) x prob(B);<br />
If A and B are <b>mutually exclusive</b>, then prob(A &amp; B) = 0;<br />
If A and B are <b>positively correlated</b> (If A entails B), then prob(A &amp; B) = prob(A).<br />
If A and B are <b>negatively correlated</b> (If A entails not-B), then prob(A v B) = min(1, prob(A) + prob(B)).</p>
<p>Some interesting things follow:</p>
<p>(1) A pair of events may be mutually exclusive but not negatively correlated, so<br />
there is no interdefinability of &amp; and v in a probabilistic logic.</p>
<p>(2) Probability logic is <i>not</i> a type of multi-valued propositional logic, since the connectives &amp; and v cannot be characterized by the lattice properties of boolean &amp; and v (i.e., &amp; := min [f(A), v(B)]; v := max[f(A), f(B)], where &#8216;f&#8217; is a generic valuation function; substitute &#8216;prob&#8217; for &#8216;f&#8217; here).</p>
<p>(3) Two events A, B, can each have positive probability but the prob(A &amp;B) = [0,1]; in words, we can have precise probability assessments for each of A and B, but be completely ignorant about the probability of (A and B).</p>
<p>From the point of view of a probabilist, there is a temptation to claim that there simply is no logical structure to probability logics and that the notion of a probability logic (progic) is nonsensical: to get anything out of calculations with probabilities, one has to make substantive assumptions about the relationships between the events we wish to reason about. In some cases these assumptions are warranted, in others they are not. This point (perhaps) crystalizes the difference between bayesian <i>statistics</i> and bayesian <i>epistemology</i>: it is not necessarily a failure of statistical analysis to admit that we don&#8217;t always have information about joint distributions.</p>
<p>But the point about progics have no interesting struction is too rash, and Tim McGrew&#8217;s comment points out why. To generalize, for arbitrary events A, B, represented in a single probability structure M, if prob(A) and prob(B) are defined in M, then:</p>
<p>(i) prob(A &amp; B) lies within the interval [max(0, prob(A) + prob(B) -1), min(prob(A), prob(B)], and</p>
<p>(ii) prob(A v B) lies within the interval [max(prob(A), prob(B), min(prob(A)+ prob(B), 1)].</p>
<p>These are weak bounds, and trvialize quickly; but, they are not necessarily trivial, and they are derived without any substantive assumption about the relationship between the events A and B other than that they be represented in the algebra M. (This bound holds for lower/upper probability defined on M, too.)</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Tim McGrew</title>
		<link>http://certaindoubts.com/pollock-on-probable-probabilities/#comment-3866</link>
		<dc:creator><![CDATA[Tim McGrew]]></dc:creator>
		<pubDate>Fri, 25 Aug 2006 00:20:36 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=612#comment-3866</guid>
		<description><![CDATA[Hmm ... I&#039;m having difficulty understanding this bit:

&lt;i&gt;Suppose a problem is described by logical compounds of a set of simple propositions P1,…,Pn. Then to be able to compute the probabilities of all logical compounds of these simple propositions, what we must generally know is the probabilities of every conjunction of the form PROB((~)P1&#038;…&#038;(~)Pn). The tildes enclosed in parentheses can be either present or absent. These n-fold conjunctions are called Boolean conjunctions. Given all but one of them, the probability calculus imposes no constraints on the probability of the remaining Boolean conjunction.&lt;/i&gt;

If I&#039;m reading this right, then John is describing a partition in P1, ..., Pn. But in that case, wouldn&#039;t the probability of the &quot;last one&quot; be one minus the sum of the probabilities of the others?

Again, this is just a nit; his major point -- that we can know quite a lot about the probabilities of individual propositions but still be largely ignorant of the probabilities of compounds built up out of them -- is well taken.]]></description>
		<content:encoded><![CDATA[<p>Hmm &#8230; I&#8217;m having difficulty understanding this bit:</p>
<p><i>Suppose a problem is described by logical compounds of a set of simple propositions P1,…,Pn. Then to be able to compute the probabilities of all logical compounds of these simple propositions, what we must generally know is the probabilities of every conjunction of the form PROB((~)P1&amp;…&amp;(~)Pn). The tildes enclosed in parentheses can be either present or absent. These n-fold conjunctions are called Boolean conjunctions. Given all but one of them, the probability calculus imposes no constraints on the probability of the remaining Boolean conjunction.</i></p>
<p>If I&#8217;m reading this right, then John is describing a partition in P1, &#8230;, Pn. But in that case, wouldn&#8217;t the probability of the &#8220;last one&#8221; be one minus the sum of the probabilities of the others?</p>
<p>Again, this is just a nit; his major point &#8212; that we can know quite a lot about the probabilities of individual propositions but still be largely ignorant of the probabilities of compounds built up out of them &#8212; is well taken.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Tim McGrew</title>
		<link>http://certaindoubts.com/pollock-on-probable-probabilities/#comment-3865</link>
		<dc:creator><![CDATA[Tim McGrew]]></dc:creator>
		<pubDate>Fri, 25 Aug 2006 00:13:40 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=612#comment-3865</guid>
		<description><![CDATA[This looks like a really interesting paper. I&#039;m just getting into it, but there&#039;s one small correction that John might want to make on p. 2. He writes:

&lt;i&gt;To illustrate, suppose we know that PROB(P) = .7 and PROB(Q) = .6. What can we conclude about PROB(P &#038; Q)? All the probability calculus enables us to infer is that 0 ≤ PROB(P &#038; Q) ≤ .6.&lt;/i&gt;

We know a bit more than that. From the information given we can deduce that .3 ≤ PROB(P &#038; Q) ≤ .6, which admittedly isn&#039;t as much as we might like to have but is still more than John claims. The lower bound is firm because from {P, Q} we can derive the claim that (P &#038; Q), and both premises are essential. The theorem on the accumulation of uncertainties therefore applies: the uncertainty (1 minus the probability) of (P &#038; Q) cannot be any greater than the sum of the uncertainties of the premises.]]></description>
		<content:encoded><![CDATA[<p>This looks like a really interesting paper. I&#8217;m just getting into it, but there&#8217;s one small correction that John might want to make on p. 2. He writes:</p>
<p><i>To illustrate, suppose we know that PROB(P) = .7 and PROB(Q) = .6. What can we conclude about PROB(P &amp; Q)? All the probability calculus enables us to infer is that 0 ≤ PROB(P &amp; Q) ≤ .6.</i></p>
<p>We know a bit more than that. From the information given we can deduce that .3 ≤ PROB(P &amp; Q) ≤ .6, which admittedly isn&#8217;t as much as we might like to have but is still more than John claims. The lower bound is firm because from {P, Q} we can derive the claim that (P &amp; Q), and both premises are essential. The theorem on the accumulation of uncertainties therefore applies: the uncertainty (1 minus the probability) of (P &amp; Q) cannot be any greater than the sum of the uncertainties of the premises.</p>
]]></content:encoded>
	</item>
</channel>
</rss>
