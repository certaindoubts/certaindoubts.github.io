<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	>
<channel>
	<title>Comments on: Truth-insensitive epistemology: radical or commonsense?</title>
	<atom:link href="http://certaindoubts.com/truth-insensitive-epistemology-radical-or-commonsense/feed/" rel="self" type="application/rss+xml" />
	<link>http://certaindoubts.com/truth-insensitive-epistemology-radical-or-commonsense/</link>
	<description>devoted to matters epistemic</description>
	<lastBuildDate>Wed, 10 Apr 2019 16:37:28 +0000</lastBuildDate>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.10</generator>
	<item>
		<title>By: Clayton</title>
		<link>http://certaindoubts.com/truth-insensitive-epistemology-radical-or-commonsense/#comment-257414</link>
		<dc:creator><![CDATA[Clayton]]></dc:creator>
		<pubDate>Wed, 24 Jun 2015 14:42:40 +0000</pubDate>
		<guid isPermaLink="false">http://certaindoubts.com/?p=4625#comment-257414</guid>
		<description><![CDATA[Hi John,

Thanks for that. I knew about the Douven and Lackey stuff because I had to address their remarks about blame in the course of arguing for/responding to objections to a truth-requirement on warranted assertion (and justified belief, justified use of premises in practical and theoretical reasoning, evidence, etc.). Fwiw, in one of the first things I managed to sneak into print on this general set of topics, I realized that I&#039;d have to try to be a bit slippery to get past referees, so I ended up saying that (a) it was weird to have something more demanding than a justified belief norm for practical reason and wrong to say (b) that it was wrong to work with something weaker than a justified, true belief norm for practical reason. Since I thought that &#039;justified belief&#039; was factive, there wasn&#039;t a difference here. My first success at slipping my view into print, but I had to do it under a dodgy description and say nothing about the redundancy of the truth-requirement.]]></description>
		<content:encoded><![CDATA[<p>Hi John,</p>
<p>Thanks for that. I knew about the Douven and Lackey stuff because I had to address their remarks about blame in the course of arguing for/responding to objections to a truth-requirement on warranted assertion (and justified belief, justified use of premises in practical and theoretical reasoning, evidence, etc.). Fwiw, in one of the first things I managed to sneak into print on this general set of topics, I realized that I&#8217;d have to try to be a bit slippery to get past referees, so I ended up saying that (a) it was weird to have something more demanding than a justified belief norm for practical reason and wrong to say (b) that it was wrong to work with something weaker than a justified, true belief norm for practical reason. Since I thought that &#8216;justified belief&#8217; was factive, there wasn&#8217;t a difference here. My first success at slipping my view into print, but I had to do it under a dodgy description and say nothing about the redundancy of the truth-requirement.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: John Turri</title>
		<link>http://certaindoubts.com/truth-insensitive-epistemology-radical-or-commonsense/#comment-257260</link>
		<dc:creator><![CDATA[John Turri]]></dc:creator>
		<pubDate>Sun, 21 Jun 2015 21:32:48 +0000</pubDate>
		<guid isPermaLink="false">http://certaindoubts.com/?p=4625#comment-257260</guid>
		<description><![CDATA[Thanks, Clayton. I suppose that the royal &quot;we&quot; might have made an unnoticed comeback. :)

Great point about the connection to Darley &#038; Robinson&#039;s findings! Convergent evidence like that shouldn&#039;t be taken lightly. In a companion paper (&quot;&lt;a href=&quot;http://john.turri.org/research/factive_norms.pdf&quot; rel=&quot;nofollow&quot;&gt;Evidence of Factive Norms of Belief and Decision&lt;/a&gt;&quot;), I actually cite those findings and note the similarity. (There I say that the results are &quot;consistent with existing findings on people’s intuitions about responsibility and punishment, where objective facts matter beyond what is reflected in the agent’s evidence about the situation.&quot;) Maybe I should put up a separate post about the remarkably consistent set of experimental findings that pertain to belief, assertion, and decision-making.

I enthusiastically agree with you that referees really should stop wielding these alleged intuitions as cudgels. I expect that you&#039;ve suffered this sort of abuse as much as anyone. I definitely sympathize!

Btw, Jennifer Lackey has an argument against a truth requirement which does not merely appeal to such intuitions (though the two sorts of objection might not always be kept completely separate). The other argument is based on an assumption about the relationship between rule-breaking and blame. Something similar can be found in Igor Douven&#039;s writings. The argument repays time spent considering it. (If you&#039;re interested in references or my response, check out &quot;&lt;a href=&quot;http://john.turri.org/research/Truth_Test.pdf&quot; rel=&quot;nofollow&quot;&gt;The Test of Truth&lt;/a&gt;.&quot;)]]></description>
		<content:encoded><![CDATA[<p>Thanks, Clayton. I suppose that the royal &#8220;we&#8221; might have made an unnoticed comeback. 🙂</p>
<p>Great point about the connection to Darley &amp; Robinson&#8217;s findings! Convergent evidence like that shouldn&#8217;t be taken lightly. In a companion paper (&#8220;<a href="http://john.turri.org/research/factive_norms.pdf" rel="nofollow">Evidence of Factive Norms of Belief and Decision</a>&#8220;), I actually cite those findings and note the similarity. (There I say that the results are &#8220;consistent with existing findings on people’s intuitions about responsibility and punishment, where objective facts matter beyond what is reflected in the agent’s evidence about the situation.&#8221;) Maybe I should put up a separate post about the remarkably consistent set of experimental findings that pertain to belief, assertion, and decision-making.</p>
<p>I enthusiastically agree with you that referees really should stop wielding these alleged intuitions as cudgels. I expect that you&#8217;ve suffered this sort of abuse as much as anyone. I definitely sympathize!</p>
<p>Btw, Jennifer Lackey has an argument against a truth requirement which does not merely appeal to such intuitions (though the two sorts of objection might not always be kept completely separate). The other argument is based on an assumption about the relationship between rule-breaking and blame. Something similar can be found in Igor Douven&#8217;s writings. The argument repays time spent considering it. (If you&#8217;re interested in references or my response, check out &#8220;<a href="http://john.turri.org/research/Truth_Test.pdf" rel="nofollow">The Test of Truth</a>.&#8221;)</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Clayton</title>
		<link>http://certaindoubts.com/truth-insensitive-epistemology-radical-or-commonsense/#comment-257245</link>
		<dc:creator><![CDATA[Clayton]]></dc:creator>
		<pubDate>Sun, 21 Jun 2015 18:00:36 +0000</pubDate>
		<guid isPermaLink="false">http://certaindoubts.com/?p=4625#comment-257245</guid>
		<description><![CDATA[But this could be true! (If he&#039;s using &#039;we&#039; to refer to RF.) 

In all seriousness, I&#039;m glad that you wrote this up. I hope it helps mitigate the ways in which the NED objection serves as a way of screening submissions.  Fwiw, your findings seem to be in line with similar research on action carried out by Darley and Robinson: http://papers.ssrn.com/sol3/papers.cfm?abstract_id=136811 

It might be nice to see some objections to truth requirements on warranted assertion, evidence, or justification that don&#039;t just involve the appeal to NED intuitions. Spent years trying to defend factive accounts of each of these notions and there was a constant appeal to &#039;our&#039; NED intuitions.]]></description>
		<content:encoded><![CDATA[<p>But this could be true! (If he&#8217;s using &#8216;we&#8217; to refer to RF.) </p>
<p>In all seriousness, I&#8217;m glad that you wrote this up. I hope it helps mitigate the ways in which the NED objection serves as a way of screening submissions.  Fwiw, your findings seem to be in line with similar research on action carried out by Darley and Robinson: <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=136811" rel="nofollow">http://papers.ssrn.com/sol3/papers.cfm?abstract_id=136811</a> </p>
<p>It might be nice to see some objections to truth requirements on warranted assertion, evidence, or justification that don&#8217;t just involve the appeal to NED intuitions. Spent years trying to defend factive accounts of each of these notions and there was a constant appeal to &#8216;our&#8217; NED intuitions.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: David Mathers</title>
		<link>http://certaindoubts.com/truth-insensitive-epistemology-radical-or-commonsense/#comment-257030</link>
		<dc:creator><![CDATA[David Mathers]]></dc:creator>
		<pubDate>Wed, 17 Jun 2015 15:37:18 +0000</pubDate>
		<guid isPermaLink="false">http://certaindoubts.com/?p=4625#comment-257030</guid>
		<description><![CDATA[&#039;Pretty early on, some influential philosophers argued for a truth-insensitive theory of justification on the grounds that justification is blamelessness, and blamelessness is truth-insensitive. (Not sure whether that counts as “muddling.”) However, the deontological conception of justification became unpopular starting in the 80s. So lots of philosophers dropped the deontology but kept the truth-insensitivity hypothesis, perhaps without realizing that they had jettisoned the pre-theoretically compelling grounds previously associated with it.&#039;

Ah ok. That makes the idea that people have just got confused here *a lot* more plausible, I think.]]></description>
		<content:encoded><![CDATA[<p>&#8216;Pretty early on, some influential philosophers argued for a truth-insensitive theory of justification on the grounds that justification is blamelessness, and blamelessness is truth-insensitive. (Not sure whether that counts as “muddling.”) However, the deontological conception of justification became unpopular starting in the 80s. So lots of philosophers dropped the deontology but kept the truth-insensitivity hypothesis, perhaps without realizing that they had jettisoned the pre-theoretically compelling grounds previously associated with it.&#8217;</p>
<p>Ah ok. That makes the idea that people have just got confused here *a lot* more plausible, I think.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: John Turri</title>
		<link>http://certaindoubts.com/truth-insensitive-epistemology-radical-or-commonsense/#comment-256978</link>
		<dc:creator><![CDATA[John Turri]]></dc:creator>
		<pubDate>Tue, 16 Jun 2015 21:48:49 +0000</pubDate>
		<guid isPermaLink="false">http://certaindoubts.com/?p=4625#comment-256978</guid>
		<description><![CDATA[Hi, David. Yes, further work is needed to make good on any of these speculative hypotheses. I happen to find the one via blame-insensitivity to be the most promising. And I don&#039;t think it pushes the question back in any problematic way. Pretty early on, some influential philosophers argued for a truth-insensitive theory of justification on the grounds that justification is blamelessness, and blamelessness is truth-insensitive. (Not sure whether that counts as &quot;muddling.&quot;) However, the deontological conception of justification became unpopular starting in the 80s. So lots of philosophers dropped the deontology but kept the truth-insensitivity hypothesis, perhaps without realizing that they had jettisoned the pre-theoretically compelling grounds previously associated with it.

In general, questions about what we judge and why can be really interesting but also challenging to sort out. Since so much of recent philosophy trades in claims about what&#039;s intuitively obvious, commonsense, and the like, it seems important for the field as a whole gain more awareness of the potential pitfalls and challenges in this area.]]></description>
		<content:encoded><![CDATA[<p>Hi, David. Yes, further work is needed to make good on any of these speculative hypotheses. I happen to find the one via blame-insensitivity to be the most promising. And I don&#8217;t think it pushes the question back in any problematic way. Pretty early on, some influential philosophers argued for a truth-insensitive theory of justification on the grounds that justification is blamelessness, and blamelessness is truth-insensitive. (Not sure whether that counts as &#8220;muddling.&#8221;) However, the deontological conception of justification became unpopular starting in the 80s. So lots of philosophers dropped the deontology but kept the truth-insensitivity hypothesis, perhaps without realizing that they had jettisoned the pre-theoretically compelling grounds previously associated with it.</p>
<p>In general, questions about what we judge and why can be really interesting but also challenging to sort out. Since so much of recent philosophy trades in claims about what&#8217;s intuitively obvious, commonsense, and the like, it seems important for the field as a whole gain more awareness of the potential pitfalls and challenges in this area.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: David Mathers</title>
		<link>http://certaindoubts.com/truth-insensitive-epistemology-radical-or-commonsense/#comment-256950</link>
		<dc:creator><![CDATA[David Mathers]]></dc:creator>
		<pubDate>Tue, 16 Jun 2015 18:20:54 +0000</pubDate>
		<guid isPermaLink="false">http://certaindoubts.com/?p=4625#comment-256950</guid>
		<description><![CDATA[I&#039;m not sure either of those explanations seem totally satisfying to me (which doesn&#039;t mean their wrong!). The mistake one might well be correct, but it just seems to push the question back; what is it about philosophers that makes them muddle blameworthiness with the other concepts. Maybe it&#039;s that philosophy selects for individuals who are particularly concerned with the normative evaluation of other people&#039;s epistemic conduct, and that means they&#039;re more likely to run together other important epistemic notions with the notion of whether or not someone is blameworthy for forming a belief? 
My gut instinct meanwhile says that the gatekeeper thing is just likely to be false, unless there&#039;s some strong correlation of not believing that things like &#039;justification&#039; are insensitive with other unpopular views (always possible), because students who are exposed to epistemology at a level where the issue of truth-sensitivity comes up, are also likely to be exposed to externalist views on which at least many important dimensions of epistemic assessment do turn out to be truth sensitive. But maybe that&#039;s underestimating how off-putting people find it to find things they disagree with described as &#039;intuitive&#039;? Is there any hard data on this?

By the way, this is totally outside my area as a philosopher, but it seems to me, having now looked at the paper a bit more closely, that there are some not that unreasonable looking ways in which your results could fail to refute the claim that truth-insensitivity is intuitive IF truth-insensitivity is defined as &#039;certain core, philosophically important evaluative properties of a belief are insensitive to whether it is true&#039;, even if the 2nd and 3rd experiments do show that certain claims about what&#039;s obviously or intuitively true about the beliefs of brains in vats being equally just are false. I may well be totally wrong about this though, and maybe you address one or both of the points somewhere in the paper and I just missed it. 

In the case of the first experiment, it&#039;s notable that the most commonly given answer amongst those who refused to say that the subject should X the belief, where X is the relevant epistemic content, that the owned a certain watch or there was a gunshot in the woods, was that they should instead believe that it is probable that they own the watch/there is a gunshot in the woods. Now, it seems to me that one reason this might be the case is that the people actually think that &#039;strictly speaking&#039; your only ever ok to X the probablistic claim, but that it&#039;s ok to speak loosely and give people permission to X the absolute claim, when the possibility of error isn&#039;t conversationally salient. (This is absolutely not evidence for the view in the previous sentence, but Tim Williamson attributes something like the view that strictly speaking, were only allowed to believe that things are probably true to the voice of post-scientific commonsense in his book for beginners that came out recently). This would be a &#039;high standards, loose talk&#039; view, a bit like the view that Peter Unger use to have about &#039;flat&#039; and &#039;empty&#039; and &#039;knowledge&#039; in the 70s.

The above seems a bit desperate even to me, but I think there&#039;s a more serious problem with the 2nd and 3rd cases. There, the subjects in the cases where the belief is false, whether or not they are envatted, are all presumably (at least, this seems a reasonable way to read the descriptions), coming to believe something on the basis of misleading perceptual evidence, in the sense that the perceptual states in question aren&#039;t veridical. But it seems like thinking that beliefs formed on misleading perceptual evidence are less justified/responsibly formed etc., is perfectly compatible with thinking that whether or not a belief is justified is insensitive to its truth, unless you identify the belief with the misleading perceptual state. In fact something like this problem *might* even come up for the first experiment, if people are thinking of the beliefs as essentially grounded in the belief that the list is reliable or the perceptual evidence is not probablistically misleading, in the sense that it ought to raise the degree that one thinks a proposition which is in fact false is likely to be true. Those are both differences between the true and the false beliefs which are not about whether those beliefs are true/false, strictly speaking. Though it&#039;s clearly much more of a stretch to say something like this with the first experiment.]]></description>
		<content:encoded><![CDATA[<p>I&#8217;m not sure either of those explanations seem totally satisfying to me (which doesn&#8217;t mean their wrong!). The mistake one might well be correct, but it just seems to push the question back; what is it about philosophers that makes them muddle blameworthiness with the other concepts. Maybe it&#8217;s that philosophy selects for individuals who are particularly concerned with the normative evaluation of other people&#8217;s epistemic conduct, and that means they&#8217;re more likely to run together other important epistemic notions with the notion of whether or not someone is blameworthy for forming a belief?<br />
My gut instinct meanwhile says that the gatekeeper thing is just likely to be false, unless there&#8217;s some strong correlation of not believing that things like &#8216;justification&#8217; are insensitive with other unpopular views (always possible), because students who are exposed to epistemology at a level where the issue of truth-sensitivity comes up, are also likely to be exposed to externalist views on which at least many important dimensions of epistemic assessment do turn out to be truth sensitive. But maybe that&#8217;s underestimating how off-putting people find it to find things they disagree with described as &#8216;intuitive&#8217;? Is there any hard data on this?</p>
<p>By the way, this is totally outside my area as a philosopher, but it seems to me, having now looked at the paper a bit more closely, that there are some not that unreasonable looking ways in which your results could fail to refute the claim that truth-insensitivity is intuitive IF truth-insensitivity is defined as &#8216;certain core, philosophically important evaluative properties of a belief are insensitive to whether it is true&#8217;, even if the 2nd and 3rd experiments do show that certain claims about what&#8217;s obviously or intuitively true about the beliefs of brains in vats being equally just are false. I may well be totally wrong about this though, and maybe you address one or both of the points somewhere in the paper and I just missed it. </p>
<p>In the case of the first experiment, it&#8217;s notable that the most commonly given answer amongst those who refused to say that the subject should X the belief, where X is the relevant epistemic content, that the owned a certain watch or there was a gunshot in the woods, was that they should instead believe that it is probable that they own the watch/there is a gunshot in the woods. Now, it seems to me that one reason this might be the case is that the people actually think that &#8216;strictly speaking&#8217; your only ever ok to X the probablistic claim, but that it&#8217;s ok to speak loosely and give people permission to X the absolute claim, when the possibility of error isn&#8217;t conversationally salient. (This is absolutely not evidence for the view in the previous sentence, but Tim Williamson attributes something like the view that strictly speaking, were only allowed to believe that things are probably true to the voice of post-scientific commonsense in his book for beginners that came out recently). This would be a &#8216;high standards, loose talk&#8217; view, a bit like the view that Peter Unger use to have about &#8216;flat&#8217; and &#8217;empty&#8217; and &#8216;knowledge&#8217; in the 70s.</p>
<p>The above seems a bit desperate even to me, but I think there&#8217;s a more serious problem with the 2nd and 3rd cases. There, the subjects in the cases where the belief is false, whether or not they are envatted, are all presumably (at least, this seems a reasonable way to read the descriptions), coming to believe something on the basis of misleading perceptual evidence, in the sense that the perceptual states in question aren&#8217;t veridical. But it seems like thinking that beliefs formed on misleading perceptual evidence are less justified/responsibly formed etc., is perfectly compatible with thinking that whether or not a belief is justified is insensitive to its truth, unless you identify the belief with the misleading perceptual state. In fact something like this problem *might* even come up for the first experiment, if people are thinking of the beliefs as essentially grounded in the belief that the list is reliable or the perceptual evidence is not probablistically misleading, in the sense that it ought to raise the degree that one thinks a proposition which is in fact false is likely to be true. Those are both differences between the true and the false beliefs which are not about whether those beliefs are true/false, strictly speaking. Though it&#8217;s clearly much more of a stretch to say something like this with the first experiment.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: John Turri</title>
		<link>http://certaindoubts.com/truth-insensitive-epistemology-radical-or-commonsense/#comment-256764</link>
		<dc:creator><![CDATA[John Turri]]></dc:creator>
		<pubDate>Mon, 15 Jun 2015 17:58:57 +0000</pubDate>
		<guid isPermaLink="false">http://certaindoubts.com/?p=4625#comment-256764</guid>
		<description><![CDATA[Hello David,

A good question! I offer two (compatible) hypotheses. On the one hand, proponents of truth-insensitive epistemology might just have idiosyncratic intuitions and then suffer from a false consensus effect, perhaps amplified by a gate-keeper or self-selection effect within the discipline. On the other hand, they might be relying on perfectly normal intuitions about blamelessness — which, according to my results, was the only truth-insensitive form of evaluation — which they either misunderstand or misdescribe using vocabulary that, as it turns out, expresses highly truth-sensitive forms of evaluation. (Check p. 23 of the paper linked above.)

In connection with this, I&#039;ll note that I&#039;ve not found experimental philosophers to be reticent about offering hypotheses about why people disagree. However, I&#039;ve had anonymous reviewers express indignation at the suggestion that philosophers would be susceptible to factors knowns to bias human judgment. So perhaps in some of the cases you&#039;re familiar with, the authors shied away from this sort of thing because they worried about inciting referees, or removed hypotheses that referees complained about. The anonymous review process presents plenty of opportunities for referees to abuse authors, so I can definitely sympathize with such a decision even if, in the bigger picture, the research would be better if accompanied by such hypotheses.]]></description>
		<content:encoded><![CDATA[<p>Hello David,</p>
<p>A good question! I offer two (compatible) hypotheses. On the one hand, proponents of truth-insensitive epistemology might just have idiosyncratic intuitions and then suffer from a false consensus effect, perhaps amplified by a gate-keeper or self-selection effect within the discipline. On the other hand, they might be relying on perfectly normal intuitions about blamelessness — which, according to my results, was the only truth-insensitive form of evaluation — which they either misunderstand or misdescribe using vocabulary that, as it turns out, expresses highly truth-sensitive forms of evaluation. (Check p. 23 of the paper linked above.)</p>
<p>In connection with this, I&#8217;ll note that I&#8217;ve not found experimental philosophers to be reticent about offering hypotheses about why people disagree. However, I&#8217;ve had anonymous reviewers express indignation at the suggestion that philosophers would be susceptible to factors knowns to bias human judgment. So perhaps in some of the cases you&#8217;re familiar with, the authors shied away from this sort of thing because they worried about inciting referees, or removed hypotheses that referees complained about. The anonymous review process presents plenty of opportunities for referees to abuse authors, so I can definitely sympathize with such a decision even if, in the bigger picture, the research would be better if accompanied by such hypotheses.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: David Mathers</title>
		<link>http://certaindoubts.com/truth-insensitive-epistemology-radical-or-commonsense/#comment-256746</link>
		<dc:creator><![CDATA[David Mathers]]></dc:creator>
		<pubDate>Mon, 15 Jun 2015 15:05:28 +0000</pubDate>
		<guid isPermaLink="false">http://certaindoubts.com/?p=4625#comment-256746</guid>
		<description><![CDATA[Any guesses as to why philosophers have reported the contrary intuitions? I find it a bit frustrating that a lot of X-Phi stuff stops with noting that philosophers and the folk diverge about X, without even speculating as to why philosophers have the intuitions they have.]]></description>
		<content:encoded><![CDATA[<p>Any guesses as to why philosophers have reported the contrary intuitions? I find it a bit frustrating that a lot of X-Phi stuff stops with noting that philosophers and the folk diverge about X, without even speculating as to why philosophers have the intuitions they have.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: John Turri</title>
		<link>http://certaindoubts.com/truth-insensitive-epistemology-radical-or-commonsense/#comment-256628</link>
		<dc:creator><![CDATA[John Turri]]></dc:creator>
		<pubDate>Sun, 14 Jun 2015 19:51:15 +0000</pubDate>
		<guid isPermaLink="false">http://certaindoubts.com/?p=4625#comment-256628</guid>
		<description><![CDATA[Hello Ben,

Thanks for chiming in.

Perhaps I&#039;ve written something misleading in the comments here, but I actually did not find a significant difference between any of the questions used in Experiment 1. These included questions about what the evidence supports, what the agent is justified in believing, reasonable in believing, rational in believing, responsible in believing, and what the agent should believe. Basically, all of these resulted in the same basic pattern of truth-sensitivity. Neither did I find any indication that the pattern of truth-sensitivity disappeared or even diminished when people considered both cases in the same context (i.e. here&#039;s a normally embodied human and here&#039;s his &quot;brain-in-a-vat&quot; counterpart — which one, if either, has better evidence?).

In Experiment 2, I found an interesting, large difference between judgments about what an agent should believe and quality of evidence, on the one hand, and whether the agent is blameworthy, on the other. Blame judgments were truth-insensitive, whereas the other judgments were truth-sensitive.

Also, I&#039;ll just quickly mention that New Evil Demon cases (and BIV cases) do *not* differ only on the true/false dimension. They differ in many other potentially important dimensions. However, I did test that type of case alongside cases which actually did differ only on whether the target proposition was true/false. I found no difference between an ordinary false-belief case and a BIV case.]]></description>
		<content:encoded><![CDATA[<p>Hello Ben,</p>
<p>Thanks for chiming in.</p>
<p>Perhaps I&#8217;ve written something misleading in the comments here, but I actually did not find a significant difference between any of the questions used in Experiment 1. These included questions about what the evidence supports, what the agent is justified in believing, reasonable in believing, rational in believing, responsible in believing, and what the agent should believe. Basically, all of these resulted in the same basic pattern of truth-sensitivity. Neither did I find any indication that the pattern of truth-sensitivity disappeared or even diminished when people considered both cases in the same context (i.e. here&#8217;s a normally embodied human and here&#8217;s his &#8220;brain-in-a-vat&#8221; counterpart — which one, if either, has better evidence?).</p>
<p>In Experiment 2, I found an interesting, large difference between judgments about what an agent should believe and quality of evidence, on the one hand, and whether the agent is blameworthy, on the other. Blame judgments were truth-insensitive, whereas the other judgments were truth-sensitive.</p>
<p>Also, I&#8217;ll just quickly mention that New Evil Demon cases (and BIV cases) do *not* differ only on the true/false dimension. They differ in many other potentially important dimensions. However, I did test that type of case alongside cases which actually did differ only on whether the target proposition was true/false. I found no difference between an ordinary false-belief case and a BIV case.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: John Turri</title>
		<link>http://certaindoubts.com/truth-insensitive-epistemology-radical-or-commonsense/#comment-256626</link>
		<dc:creator><![CDATA[John Turri]]></dc:creator>
		<pubDate>Sun, 14 Jun 2015 19:40:17 +0000</pubDate>
		<guid isPermaLink="false">http://certaindoubts.com/?p=4625#comment-256626</guid>
		<description><![CDATA[Hi Anon Grad,

That&#039;s an elegant and appealing theory. Properly contextualized and developed, it sounds like it has the makings of a publishable unit.

I was thinking about whether any of the results shed light on whether this two-part evaluative structure (belief aiming at evidential fit, evidence aiming at truth) is reflected in our ordinary evaluative practices. (Of course, this is separate from whether it &lt;i&gt;should be&lt;/i&gt; reflected, and I&#039;m not prejudging that question.) In Experiment 1, one of the questions asked participants, &quot;What does [the agent&#039;s] evidence justify him/her in believing?&quot; To me, this sounds like a matter of evidential fit. As it turns out, people&#039;s judgments of evidential fit were sensitive to the truth. When P was true, the central tendency was to judge that &lt;i&gt;believing P&lt;/i&gt; fit the evidence. By contrast, when P was false, the central tendency was to judged that &lt;i&gt;believing probably P&lt;/i&gt; fit the evidence.]]></description>
		<content:encoded><![CDATA[<p>Hi Anon Grad,</p>
<p>That&#8217;s an elegant and appealing theory. Properly contextualized and developed, it sounds like it has the makings of a publishable unit.</p>
<p>I was thinking about whether any of the results shed light on whether this two-part evaluative structure (belief aiming at evidential fit, evidence aiming at truth) is reflected in our ordinary evaluative practices. (Of course, this is separate from whether it <i>should be</i> reflected, and I&#8217;m not prejudging that question.) In Experiment 1, one of the questions asked participants, &#8220;What does [the agent&#8217;s] evidence justify him/her in believing?&#8221; To me, this sounds like a matter of evidential fit. As it turns out, people&#8217;s judgments of evidential fit were sensitive to the truth. When P was true, the central tendency was to judge that <i>believing P</i> fit the evidence. By contrast, when P was false, the central tendency was to judged that <i>believing probably P</i> fit the evidence.</p>
]]></content:encoded>
	</item>
</channel>
</rss>
