<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>epistemic paradoxes &#8211; </title>
	<atom:link href="http://certaindoubts.com/category/epistemic-paradoxes/feed/" rel="self" type="application/rss+xml" />
	<link>http://certaindoubts.com</link>
	<description>devoted to matters epistemic</description>
	<lastBuildDate>Wed, 10 Apr 2019 01:35:13 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.10</generator>
	<item>
		<title>A simple problem for (unrestricted) Conditionalization</title>
		<link>http://certaindoubts.com/a-simple-problem-for-unrestricted-conditionalization/</link>
		<comments>http://certaindoubts.com/a-simple-problem-for-unrestricted-conditionalization/#comments</comments>
		<pubDate>Sun, 04 Nov 2018 15:38:33 +0000</pubDate>
		<dc:creator><![CDATA[Ralph Wedgwood]]></dc:creator>
				<category><![CDATA[epistemic paradoxes]]></category>
		<category><![CDATA[formal epistemology]]></category>

		<guid isPermaLink="false">http://certaindoubts.com/?p=4890</guid>
		<description><![CDATA[Many formal epistemologists think that Conditionalization is always the uniquely rational way to update one’s credences. But this cannot be correct. In certain troublesome cases, Conditionalization would take the thinker to rationally forbidden destinations. Conditionalization has to be restricted somehow, &#8230; <a class="more-link" href="http://certaindoubts.com/a-simple-problem-for-unrestricted-conditionalization/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p>Many formal epistemologists think that Conditionalization is <i>always</i> the uniquely rational way to update one’s credences. But this cannot be correct. In certain troublesome cases, Conditionalization would take the thinker to rationally forbidden destinations. Conditionalization has to be restricted somehow, so that it does not apply in these troublesome cases.</p>
<p><span id="more-4890"></span></p>
<p>There is actually quite a range of such troublesome cases. But the simplest example is that of certain <i>Moore-paradoxical propositions</i> – propositions that the thinker could express by uttering something of the form ‘<i>P</i>, and there is no time <i>t</i> at which I assign a high credence to the proposition that <i>P</i>’.</p>
<p>(In contemplating this proposition, the thinker has to refer to <i>herself</i> in a distinctively first-personal way. However, to bracket worries about how to accommodate indexical references to <em>times</em> in our formal framework, I have chosen a Moore-paradoxical proposition that quantifies over times in its second conjunct – rather than a proposition that contains a distinctively indexical reference to the present time.)</p>
<p>Now, nothing prevents the thinker from rationally having a prior system of credences that assigns arbitrarily high probability to this Moore-paradoxical proposition, conditional on a certain possible body of evidence <i>E</i>.</p>
<p>Indeed, <i>E</i> might just be <i>P</i> itself. It might be obvious from the nature of <i>P</i> that <i>P</i> is the kind of proposition that one is extraordinarily unlikely to have a high credence in even if <i>P</i> is true. For example, suppose that <i>P</i> is the proposition that the number of flies in the world right now is exactly 17,000,000,000,000,000. Even conditional on the truth of <i>P</i>, it is unbelievably unlikely that one will ever have a high credence in <i>P</i>. So, conditional on the supposition of <i>P</i>, one rationally assigns an extremely high conditional credence to the proposition that one could express by uttering ‘<i>P</i>, and I never have a high credence in the proposition that <i>P</i>’.</p>
<p>However, one’s prior credence in <i>P</i> is still non-zero. So, it could still happen that one day one learns that <i>P</i> is true. But then, if one updates one’s credences by Conditionalization, one will end up assigning an extremely high credence to the proposition that one could express by uttering the relevant instance of ‘<i>P</i> and I never have a high credence in the proposition that <i>P</i>’.</p>
<p>This, surely, is an irrational place to end up in. It is <i>a priori</i> obvious that if one has a high credence in this proposition, the proposition cannot be true (given that it is also <i>a priori</i> obvious that if one has a high credence in a conjunction, one also has high credence in each of its conjuncts).</p>
<p>So, in these cases, it seems to me, it is irrational to update by Conditionalization. Conditionalization must be restricted so that it does not apply to these cases.</p>
<p>Indeed, this point seems so obvious to me that I feel sure that someone must have thought of this point before. I would be very grateful if someone could let me know who (if anyone) has made this point before!</p>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/a-simple-problem-for-unrestricted-conditionalization/feed/</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
		<item>
		<title>Of Elections and Lotteries</title>
		<link>http://certaindoubts.com/3770/</link>
		<comments>http://certaindoubts.com/3770/#comments</comments>
		<pubDate>Thu, 08 Nov 2012 01:44:35 +0000</pubDate>
		<dc:creator><![CDATA[Trent Dougherty]]></dc:creator>
				<category><![CDATA[epistemic paradoxes]]></category>
		<category><![CDATA[formal epistemology]]></category>
		<category><![CDATA[justification]]></category>
		<category><![CDATA[knowledge]]></category>
		<category><![CDATA[skepticism]]></category>

		<guid isPermaLink="false">http://certaindoubts.com/?p=3770</guid>
		<description><![CDATA[Synopsis: I wonder why, in light of some solid cases of lottery knowledge, people still doubt lottery knowledge.  I also suggest an X-phi research project that thought would boom after 2004 but didn’t. General motivational prolegomena: So I made it &#8230; <a class="more-link" href="http://certaindoubts.com/3770/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p><em>Synopsis</em>: I wonder why, in light of some solid cases of lottery knowledge, people still doubt lottery knowledge.  I also suggest an X-phi research project that thought would boom after 2004 but didn’t.</p>
<p><em>General motivational prolegomena</em>: So I made it until 4:02 Central Daylight Time before obtaining testimony that Obama was re-elected.  FBF’s Jason Rogers and Jeremy Fantl deftly crushed my blissful ignorance.  Not caring about politics much more than knowledge, I had isolated myself from any and all reports about the election from the time polls opened until late this afternoon.  I made a public display of my ignorance on Facebook about this matter (as is my wont, true) and I assumed that many people were saying to themselves “He knows darn good and well Obama has been re-elected, he’s just being provocative!”  I thought this because I thought that there was so much hatred of Romney that he didn’t have a serious chance.  It turns out that was an artifact of my having such a high proportion of friends in Academia (it looks like he got 51-ish% of the popular vote.)</p>
<p>But assume that things were as I took them to be.</p>
<p><span id="more-3770"></span></p>
<p>Assume that polls had consistently indicated for months that Obama had a 30% lead in the polls.  It would be very natural for a Romney supporter to say, “Man, we are not going to win this time around, damn.”   And it would be natural for a fellow-supporter to respond “I know, it really bums me out.”  The nondefectiveness of the first utterance gives friends of the knowledge norm of assertion a reason to think there is knowledge there.  And foes of it&#8211;like me!&#8211;will take the nondefectiveness of the reply as evidence that they knew that Obama would win.  Frequently through the day I heard voices (you know what I mean) saying “Oh come off it, you know who won, sheesh.”  It never occurred to me to reply to the voices that this was not a case of knowledge.  I carefully avoided saying that I didn’t know who won for this very reason.  I did take myself to know, just not to have been told and so acquired *testimonial* knowledge.</p>
<p>Similar kind of case in the neighborhood.  Suppose a precinct in a very, very wealthy area occupied almost exclusively by movie stars come back as expected: 88% Obama.  However, Romney frivolously sues for a recount and wins.  The head re-counter will naturally say to his staff: “Look, we all know how this is going to end, but the law requires us to do it.”</p>
<p><strong>Objection</strong>: Ever heard of “Dewey Beats Truman”?<br />
<strong>Reply</strong>: That error wasn’t based straightforwardly on polls.  If Wikipedia can be trusted here, then “The paper relied on its veteran Washington correspondent and political analyst Arthur Sears Henning, who had predicted the winner in four out of five presidential contests.”  And there may have been some wishful thinking involved, too, for Wiki also says the paper was “famously pro-Republican.”</p>
<p><strong>Objection</strong>: It is just immediately obvious that that margin is not enough for knowledge.<br />
<strong>Reply</strong>: The polling evidence isn’t supposed to be one poll, it is supposed to be a series of polls returning the same results.  It is essentially structurally similar to Vogel’s Heartbreaker case.</p>
<p>This leads me to a clarification about my point here.  My reflective response to lottery propositions is the same as my first reaction: Of course we know losers are losers! To think otherwise is to be either bad at math or in the grip of a theory.  When it is pointed out so carefully&#8211;a la Hawthorne 2004&#8211;that ordinary knowledge has a lottery structure we should not for a moment (okay, not for more than a moment) become skeptical of ordinary knowledge, we should use that structural similarity to be skeptical of our skepticism about knowledge in explicit lotteries (especially since people are constantly bamboozled by numbers (#Kahnman&amp;Tversky)).</p>
<p>But for some, their lottery skepticism runs pretty deep and they need other reasons to support lottery knowledge besides the fact that we take ourselves to know so many things that turn out to be, essentially, lottery propositions.  I think the election case is that kind of case, as is the Heartbreaker case, the retirement fund case, the matchbox case, and the mispronunciation case.</p>
<p>I just don’t understand why these cases aren’t considered decisive in favor of lottery knowledge.  One of the most interesting (if quite speculative) aspects of Hawthorne 2004 is the investigation of the vacillation of our intuitions regarding lottery knowledge.  People should look into this more.  It would be a great combo of epistemology and X-phi.  #ResearchProject  (Some of Jennifer Nagel’s stuff is relevant here, but has a bit different focus than what I have in mind here.  I have in mind specific application of the cog sci lit’s demonstration of how jacked up we are about thinking with numbers to an explanation of why some people’s (not mine, ever, at all) intuitions go all skeptic-y when they think of life as a lottery.</p>
<p>Perhaps I can make an explicit argument.  Let’s see.</p>
<p><strong>Data</strong>:<br />
a. A significant number of people (not me!) are such that their intuitions about particular cases vacillate based on how they are described in the following kind of way.  When they are described in explicit lottery terms&#8211;“L-descriptions”&#8211;(say, as a first pass, low-value equiprobability cases) the subjects have skeptical intuitions and when they are given ordinary descriptions&#8211;“O -descriptions”&#8211;(intuitive, no def, essentially non-lottery descriptions), they attribute knowledge (and ascribe “knowledge,” that’s how you can tell.</p>
<p>b. A single case C can have two versions Cl and Co when it is given a L-description and an O-description.</p>
<p>c.The skeptical intuitions generated by cases c1-cn given L-descriptions form class K.  Ordinary common sense (knowledge-affirming (and, ordinarily, “knowledge” ascribing)) intuitions generated by cases c1-cn given O-descriptions form class O.</p>
<p><em>Premise 1/Methodological Assumption</em>: When intuitions vacillate about a case or a set of structurally similar cases, we should favor the intuitions which are more solid, if one class is more solid.</p>
<p><em>Premise 2</em>: Class-O intuitions are more solid than Class-K intuitions.</p>
<p><em>Evidence for Premise 2</em>: L-descriptions, unlike O-descriptions, involve known bamboozlers (numbers, large numbers, games of chance, risk, and math).</p>
<p><em>Lemma 1</em>: We ought to favor (give more credence to) Class-O intuitions than to Class-K intuitions.  From 1, 2 and some obvious stuff.</p>
<p><em>Premise 3</em>: If Lemma 1, then, ceteris paribus, we ought to extend ordinary confidence to lottery cases rather than extending lottery skepticism to ordinary cases.</p>
<p><em>Premise 4</em>: Ceteris is paribus</p>
<p><em>Conclusion</em>: We ought to extend ordinary confidence to lottery cases rather than extending lottery skepticism to ordinary cases.</p>
<p>We have several of the right kind of cases in the literature already, and they are not hard to generate, so I’m pretty convinced of the conclusion.</p>
<p><strong><br />
</strong></p>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/3770/feed/</wfw:commentRss>
		<slash:comments>11</slash:comments>
		</item>
		<item>
		<title>Smullyan Fooled at Last</title>
		<link>http://certaindoubts.com/smullyan-fooled-at-last/</link>
		<comments>http://certaindoubts.com/smullyan-fooled-at-last/#comments</comments>
		<pubDate>Mon, 24 Oct 2011 17:47:27 +0000</pubDate>
		<dc:creator><![CDATA[Claudio de Almeida]]></dc:creator>
				<category><![CDATA[a priori knowledge]]></category>
		<category><![CDATA[epistemic paradoxes]]></category>
		<category><![CDATA[justification]]></category>
		<category><![CDATA[knowledge]]></category>
		<category><![CDATA[major figures]]></category>

		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=3030</guid>
		<description><![CDATA[In a short article entitled “Was I Fooled?”, with which he opens his book What Is the Name of This Book? The Riddle of Dracula and Other Logical Puzzles  (Prentice Hall, 1978), Raymond Smullyan reports on the problem which he &#8230; <a class="more-link" href="http://certaindoubts.com/smullyan-fooled-at-last/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p>In a short article entitled “Was I Fooled?”, with which he opens his book <em>What Is the Name of This Book? The Riddle of Dracula and Other Logical Puzzles </em> (Prentice Hall, 1978), Raymond Smullyan reports on the problem which he claims introduced him to logic. We are led to believe that the problem is deep, that it is, as it turns out toward the end of the book, a version of the Liar Paradox. But is it really deep? And, if so, is it really the <em>logical</em> puzzle that Smullyan thinks it is? I suspect that the answer to the former question is “yes”, and I further suspect that, if that answer is right, the right answer to the second question is “no”.</p>
<p>But, first, here’s the relevant text from Smullyan’s report (which I should have the good sense not to paraphrase): <span id="more-3030"></span></p>
<blockquote><p>My introduction to logic was at the age of six. It happened this way: On 1 April 1925, I was sick in bed with grippe, or flu, or something. In the morning my brother Emile (ten years my senior) came into my bedroom and said: ‘Well, Raymond, today is April Fool’s Day, and I will fool you as you have never been fooled before!’ I waited all day long for him to fool me, but he didn’t. Late that night, my mother asked me, ‘Why don’t you go to sleep?’ I replied, ‘I’m waiting for Emile to fool me.’ My mother turned to Emile and said, ‘Emile, will you please fool the child!’ Emile then turned to me, and the following dialogue ensued:</p>
<p><em>Emile:</em> So, you expected me to fool you, didn’t you?</p>
<p><em>Raymond:</em> Yes.</p>
<p><em>Emile:</em> But I didn’t, did I?</p>
<p><em>Raymond:</em> No.</p>
<p><em>Emile:</em> But you expected me to, didn’t you?</p>
<p><em>Raymond:</em> Yes.</p>
<p><em>Emile:</em> So I fooled you, didn’t I!</p>
<p>Well, I recall lying in bed long after the lights were turned out wondering whether or not I had really been fooled. On the one hand, if I wasn’t fooled, then I did not get what I expected, hence I was fooled. (This was Emile’s argument.) But with equal reason it can be said that if I was fooled, then I <em>did</em> get what I expected, so then, in what sense was I fooled [?] So, was I fooled or wasn’t I?</p></blockquote>
<p>The problem that is explicitly identified in the excerpt should not, of course, be thought deep by 21st-century standards. Taken at face value, Emile’s puzzle for little Raymond is at best as deep as the Barber “paradox”. The Barber is useful as an introduction to the much deeper paradoxes in the Liar family (which, Barber aside, are not all equally deep, by the way). But, unlike what goes on with the deep ones, there is a familiar, fairly simple solution to the Barber: You use the reasoning leading to the contradiction as a <em>reductio</em> of the assumption that there is a barber who shaves all and only the non-self-shaving men in town. (There can be no such barber. You’ve failed to describe a possible barber. Yes, it looks like there could be such a person, but, alas, there cannot. Live with it!) In Smullyan’s April-1st. problem, the solution, likewise, seems simple enough: there can be no such fooling. Smullyan’s mother gets it right when she suspects that Emile is up to some kind of fancy nonsense. Somehow, as she understands, there is something improper in the kind of fooling that Emile is torturing the kid with. The fact that the kid plays along only makes for late-night nuisance, not late-night philosophy. Mother knows best.</p>
<p>Now, if I’m not mistaken, as educational as it may otherwise be, the setup leading to the contradiction is, to epistemological eyes, largely wasted by Smullyan, its epistemic depths remaining unplowed. But that setup can be rescued for a deeper lesson. (Footnote: I hope I’m not unfair to the venerable Smullyan, needless to say. But it is a bit disappointing that, writing in the late 70’s, he seems to miss the fairly obvious resemblance that his setup bears to the then-familiar Surprise Exam Paradox.) It takes minimal tweaking for us to do so. The original setup leads us to assume what our rescue mission should now turn into an explicit assumption: the assumption that the kid is required to have a <em>rational</em> expectation. To be sure, anyone can be fooled, if one’s being fooled simply requires one’s believing some falsehood on the basis of testimony. (Think of a gullible person falling prey to a false assertion the falsehood of which should be obvious to her.) But only <em>rational</em> acceptance of testimony (whatever the conditions for that may be) will allow for philosophically interesting surprise. So, the assumption we need is that the kid is rational in expecting to be fooled. Under this assumption, Smullyan has it that the kid is fooled by Emile iff the kid rationally expects to be fooled by Emile and is surprised by (i.e., does not rationally expect) something Emile does before midnight, something the kid <em>could not</em>, under those circumstances, have rationally believed that Emile would do.</p>
<p>The interesting problem here is whether anybody can rationally believe a person who promises to surprise the rational believer. How can the kid rationally believe that Emile will fulfill his promise? To feel the sting of the question, you have to think about what the rational doxastic attitude would be when the agent is confronted with the troubling promise. Suppose the kid thinks about the range of acts that Emile cannot rationally be expected to perform. For instance, Emile had never burned the house down, nor did he ever show any tendency to do so. The kid wouldn’t rationally take his doing so as a possibility. So, if Emile did end up burning down the house, his act would come as a total surprise. The arson scenario is one that the kid believes will not be actualized. He has numerous such beliefs (both occurrent and dispositional): Emile will not kill himself; Emile will not kill a neighbor; Emile will not produce a genuine bank statement showing a billion-dollar balance, etc. And then there are the acts about which the kid suspends judgment (either occurrently or dispositionally): “Emile might claim that I’m an adopted child (though I have no reason to believe that he will)”; “Emile might put salt in my coffee (though I have no reason to believe that he will)”; etc. If Emile’s action provided the kid with evidence for believing any of those propositions to which the kid is, prior to the prank, disposed to react with either disbelief or suspension of judgment, the kid would be surprised; and, so, Emile would have fulfilled his promise to fool the kid. But all of those acts are outside the range of the kid’s rational expectations. So, how would the kid rationally believe that he was going to be fooled? The promise that he will be fooled is an epistemic blindspot for the kid. (Footnote: For the concept of an epistemic blindspot, see Roy Sorensen’s book on the subject. Here, I borrow liberally from Sorensen and use the term “epistemic blindspot” as synonymous with “contingent proposition that a given person cannot rationally believe (or know) under any circumstance”. Footnote to the footnote: As I see it, self-refuting beliefs, for instance, a belief the content of which is the proposition expressed by “I have no beliefs now”, are not clear cases of rationally unbelievable propositions, though they are clearly not knowable. There is opposition to this claim in the literature.)</p>
<p>But, now, I hear two objections. First, there is the objection according to which there is no stable path to the conclusion that the fooling cannot rationally be anticipated. This is Emile’s own conclusion; it gives us the first half of Smullyan’s contradiction. (It’s also a popular reaction to the Surprise Exam.) Here, the problem seems to be that, if nothing in the range of rationally unpredictable acts takes place, fooling comes “from above”, as it were: higher-order fooling is the absence of first-order fooling. <em>Some fooling</em> will have taken place, if no obvious (first-order) fooling does. So, if one is not obviously fooled, then one is “higher-orderly” fooled. The objection would have it that this is deducible from the promise alone. So, necessarily, given a belief in the promise, fooling ensues. The truth of the promise is knowable a priori. And, if the promise is knowable, it’s not a blindspot. (Smullyan’s contradiction is then derived from the simple fact that, if he is necessarily fooled, the promise is fulfilled, as he expected, and, so, no fooling has ultimately taken place. If fooled, then not fooled. And this inference is, of course, okay.)</p>
<p>Isn’t this a little too easy? Do we really have the requisite concept of <em>higher-order fooling</em>? Let it be granted that we do. Even so, isn’t it open to little Raymond to object that there is, after all, a promise left unfulfilled by Emile, namely, the promise of first-order fooling? It takes only the weakest form of contextualism for us to see that little Raymond will legitimately object that it was the promise of first-order fooling that he was really excited about – and <em>both</em> will go to bed frustrated. The easy way out for little Raymond is that Emile was equivocating between senses of “fooling”, or that he simply was ambiguous when the promise was made. That’s the kind of frustration that bad philosophy <em>and bad pranks</em> are made of. (Incidentally, it should be noted that nothing like this gets us to the bottom of the Surprise Exam situation.)</p>
<p>So, the first objection is a huge letdown in its attempt to turn the promise into something knowable a priori. Common sense says “boo!”.</p>
<p>The second objection likewise denies that Emile’s promise is an epistemic blindspot for little Raymond, but does so with much more philosophical finesse. It will claim that the kid can rationally expect to be fooled, but the kid’s rational expectation arises from good induction. (That fooling will take place is not knowable a priori.) He’s been fooled by Emile many times before on April 1st. When obvious (first-order) fooling doesn’t come, <em>some fooling</em> – whatever you may want to call it – <em>will</em> have taken place. The kid will have been frustrated. His frustration naturally gives rise to some concept of fooling. The kid acquires the concept right there and then, and accepts that he was fooled. (And again, if he was fooled, then he did get what he expected, and, so, he wasn’t fooled after all.)</p>
<p>Against the objections, I would claim, decisively, that the promise <em>is</em> an epistemic blindspot. If it is a blindspot – if the kid cannot rationally believe it to begin with – then Smullyan doesn’t get his contradiction. He <em>will</em> have been fooled, the fooling will be paradoxical, and there will be no contradiction in sight. (The stage where one grapples with a contradiction is just too late.)</p>
<p>So, why is the promise a blindspot? Simply because, again, from the fact that no act in the range of predictable acts can rationally be expected, we should infer that the promise is rationally unbelievable. The second objection rests on fallacious induction. It is fallacious to infer that, because something unpredictable has occurred many times, it will occur again. <em>Ceteris paribus</em>, the unpredictable does not become predictable just because it occurs over and over again. It remains unpredictable every time. This is reminiscent of the Gambler’s Fallacy. The fact that I’ve won a fair game of roulette 50 times in a row, doesn’t make me a favorite when the 51st spin comes around. A superficially similar fallacy is at work here. Every trick Emile performed on previous April Fool’s days was rationally unpredictable. The kid cannot rationally expect him to succeed again – not this year, not ever. (Not ever? Well, maybe not <em>not ever</em>. Once he acquires the concept of higher-order fooling, Smullyan’s reasoning may be applicable. I’m not pursuing the issue right now.) And the fact that – unlike the Gambler situation – Emile’s track-record <em>should</em> allow the kid rationally to infer that Emile will somehow do it again clashes with the conclusion that no first-order prank is predictable. We then resort to the claim that a non-existent prank will be as good as a prank. We are now positing a new kind of prank: the higher-order prank. Which reminds us of the Preface situation. No act in the range of Emile’s possible first-order surprises can rationally be expected. And yet we are tempted to say that some higher-order act by Emile should be thought surprising. But this is not the Preface in disguise either. In the Preface, you can have excellent <em>prima facie</em> justification for the prefatorial belief. And that evidence is independent of your evidence for each first-order belief. Here, we want to infer that a (higher-order) prank is coming on the basis of past unpredictable (first-order) pranks. There is, however, no evidence for expecting a higher-order prank. (And there is also the issue of whether the concept of a higher-order prank can be acquired prior to the absence of first-order pranks, which I’m putting aside.) All the evidence, if, <em>per impossibile</em>, there were any, would be for expecting a first-order prank. And yet, we can see that Emile is in an ideal position to strike, since, surely, we’re still in the grip of what <em>seems</em> to be good induction. (Nothing of the sort seems to be going on in the Preface scenario.)</p>
<p>If an epistemic blindspot is essentially involved in the situation, Emile’s trap <em>is</em> paradoxical – deeply so. But it should have introduced little Raymond to <em>epistemology</em> as well, not just logic.</p>
<p>Is there something I’m missing here?</p>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/smullyan-fooled-at-last/feed/</wfw:commentRss>
		<slash:comments>9</slash:comments>
		</item>
		<item>
		<title>Stanley on Certainty and Possibility</title>
		<link>http://certaindoubts.com/stanley-on-certainty-and-possibility/</link>
		<comments>http://certaindoubts.com/stanley-on-certainty-and-possibility/#respond</comments>
		<pubDate>Sat, 28 Aug 2010 22:23:39 +0000</pubDate>
		<dc:creator><![CDATA[Trent Dougherty]]></dc:creator>
				<category><![CDATA[contextualism]]></category>
		<category><![CDATA[epistemic paradoxes]]></category>
		<category><![CDATA[formal epistemology]]></category>
		<category><![CDATA[justification]]></category>
		<category><![CDATA[knowledge]]></category>
		<category><![CDATA[major figures]]></category>

		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=2138</guid>
		<description><![CDATA[In his very interesting “Knowledge and Certainty” (Phil Issues, 2008), Jason says the following things to which we might want to refer later. A.  “A person’s belief satisfies the property expressed by a subjective use of “certain” relative to  a &#8230; <a class="more-link" href="http://certaindoubts.com/stanley-on-certainty-and-possibility/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p>In his very interesting “Knowledge and Certainty” (<em>Phil Issues</em>, 2008), Jason says the following things to which we might want to refer later.</p>
<p>A.  “A person’s belief satisfies the property expressed by a subjective use of “certain” relative to  a context if and only if that person holds that belief at or above the contextually salient degree of confidence; <em>mutatis mutandis </em>for epistemic certainty and degrees of justification.”</p>
<p>B. “Just as many beliefs may satisfy “certain”, many beliefs may, in context, satisfy “absolutely certain”. The semantic function of “absolutely” is to raise the degree on the scale above that for “certain”. So in any context, it will be harder for a belief or a proposition to satisfy “absolutely certain” than it will be for it to satisfy “certain”. But it still is possible for a belief to satisfy “absolutely certain” in one context, while not satisfying “absolutely certain” (or even “certain”) in another.”</p>
<p>C. “Construed this way, the notions of certainty relevant for the norms of assertion are nowhere near as demanding as willingness to bet on a proposition no matter what the odds, or having Cartesian grounds for its truth.”</p>
<p>D. “In previous work (Stanley, 2005), I have argued that fallibilism does not entail that I can know that p, despite it being possible that ~p. In this paper, I have argued that while fallibilism does entail that I can know that p despite being less than certain that p, it follows from independent facts about norms for assertion that I cannot <em>say </em>that I know that p and am less than certain that p.”</p>
<p>Now, I don’t believe there are any such things as constitutive norms of assertion, or speech acts generally.  I think the only norms for speech acts are the norms for all acts, rational and moral norms (perfectly sufficient to explain all the Gricean and relevance theory truths).</p>
<p><span id="more-2138"></span></p>
<p>And if I was inclined to believe that there were such critters—and it’s hard for me to even imagine it—I wouldn’t be the least bit inclined to accept an invariant norm of assertion.  And if I *was* inclined to accept the existence of sui generis norms of assertion and invariant ones at that, it would be a reasonable belief norm.  Still, I think I can make my objection to D without running afoul of the assertion stuff Jason likes.  To do so, though, I’ll have to pose a question with a bit of a torturous grammar.</p>
<ul>
<li> Q1  If one were in context C where the contextually salient degree of confidence for a belief to satisfy the property expressed by “certain” was d=.95 and one knew p with .95 certainty, what would prevent it from being the case that ~p was epistemically possible for one?</li>
</ul>
<p>In my previous post on this, I picked on the idea that knowledge could do the trick when the evidence that secured that knowledge didn’t (still seems incredible to me).  But I think adding the new bit about certainty in 2008 offers a bit more of a foothold to put pressure on the 2005 view.</p>
<p>For if prob(~p)=.05, then there’s a chance that ~p.  And if there’s a chance that ~p, then ~p is an epistemic possibility.  Hawthorne explicitly accepts the equivalence between possibilities and chances (2004: 26), and it seems pretty darned intuitive.</p>
<ul>
<li> (Equiv)  The is an epistemic possibility that p iff there is a(n epistemic) chance that p.</li>
</ul>
<p>For (Equiv) to be false, one of the following would have to be true.</p>
<p>(Equiv<sup>¬→</sup>)  There is an epistemic possibility that p but there is no chance that p.</p>
<p>(Equiv<sup>¬←</sup>)  There’s no epistemic possibility that p but there is a chance that p.</p>
<p>But both these sound weird, even contradictory.  That’s evidence for entailment.</p>
<p>It could be that what Jason said in the last post is supposed to work here too, but I don’t see it yet.  And I have tried and been unable to see how any norm of assertion business could help here (in part, because the way I put the question didn’t involve any assertions at all, it was all “in the abstract”).</p>
<p>So I remain puzzled by a fallibilist stricture against CKA’s, and especially by a fallibilist with varying standards for certainty (or “certainty”).  (After all, one could be a fallibilist in the non-entailing-evidence sense and hold that “certain” expressed the highest possible degree of justification.  J(p) = 1 would seem to be an at least plausible basis for saying ~p was impossible.)</p>
<p>PS &#8211; This appears to be the 1001<sup>st</sup> post, which is pretty cool.  A milestone for CD!</p>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/stanley-on-certainty-and-possibility/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Moore&#8217;s Paradox, First-person Plural</title>
		<link>http://certaindoubts.com/moores-paradox-first-person-plural/</link>
		<comments>http://certaindoubts.com/moores-paradox-first-person-plural/#comments</comments>
		<pubDate>Fri, 02 Nov 2007 18:16:16 +0000</pubDate>
		<dc:creator><![CDATA[John Turri]]></dc:creator>
				<category><![CDATA[epistemic paradoxes]]></category>
		<category><![CDATA[general]]></category>

		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=750</guid>
		<description><![CDATA[We all know that G.E. Moore famously pointed out that there is something extremely odd about statements like &#8216;It&#8217;s raining, but I don&#8217;t believe it is&#8217;, even though such statements would often be true. Unger, Williamson, and others have claimed &#8230; <a class="more-link" href="http://certaindoubts.com/moores-paradox-first-person-plural/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p>We all know that G.E. Moore famously <a href="http://en.wikipedia.org/wiki/Moore's_paradox">pointed out</a> that there is something extremely odd about statements like &#8216;It&#8217;s raining, but I don&#8217;t believe it is&#8217;, even though such statements would often be true. Unger, Williamson, and others have claimed that the same applies to statements such as &#8216;It&#8217;s raining, but I don&#8217;t know that it is&#8217;.</p>
<p>If we switch to the third-person, such statements don&#8217;t sound odd at all: &#8216;It&#8217;s raining, but she doesn&#8217;t believe it is&#8217; and &#8216;It&#8217;s raining, but he doesn&#8217;t know it is&#8217; both sound just fine. Likewise, the sense of paradox disappears even in the first-person when we speak in the eternal present, as I would, for example, were I asked to describe what I was doing in a video recording of my past self (&#8216;Oh, look, how quaint: it&#8217;s raining, but I don&#8217;t believe it!&#8217;)</p>
<p>The sense of paradox isn&#8217;t limited to statements of the form &#8216;Q, but I don&#8217;t believe/know that Q&#8217;. It also infects statements such as: &#8216;not-Q, although I believe Q&#8217; and &#8216;Q, although I believe that not-Q&#8217;.</p>
<p>I&#8217;m not aware of any discussion of the correlative first-person <em>plural</em> statements, such as the following one I came across in a recent <a href="http://news.yahoo.com/s/ap/20071102/ap_on_he_me/teen_contraceptives_ap_poll_7">article</a>:</p>
<blockquote><p>&#8220;Kids are kids,&#8221; said Danielle Kessenger, 39, a mother of three young children from Jacksonville, Fla., who supports providing contraceptives to those who request them. &#8220;I was a teenager once and parents don&#8217;t know everything, though we think we do.&#8221;</p></blockquote>
<p>So, two questions.</p>
<ul>
First, do you think that statements of the form &#8216;Not-Q, but we believe Q&#8217; likewise sound odd?</ul>
<ul>
Second, do you know whether the the first-person plural version is discussed in the literature?</ul>
<p>I&#8217;ll reserve expressing my opinion about the first question, although I don&#8217;t believe I will.</p>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/moores-paradox-first-person-plural/feed/</wfw:commentRss>
		<slash:comments>7</slash:comments>
		</item>
		<item>
		<title>Hawthorne and Stanley&#8217;s Knowledge Principle for Good Reasons</title>
		<link>http://certaindoubts.com/reasons-to-act-and-reasons-to-believe/</link>
		<comments>http://certaindoubts.com/reasons-to-act-and-reasons-to-believe/#comments</comments>
		<pubDate>Mon, 30 Jul 2007 14:20:42 +0000</pubDate>
		<dc:creator><![CDATA[Kvanvig Jon]]></dc:creator>
				<category><![CDATA[epistemic paradoxes]]></category>
		<category><![CDATA[justification]]></category>
		<category><![CDATA[knowledge]]></category>
		<category><![CDATA[skepticism]]></category>

		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=723</guid>
		<description><![CDATA[I&#8217;ve been reading Hawthorne and Stanley&#8217;s new piece &#8220;Knowledge and Action,&#8221; (downloadable here at Jason&#8217;s website), and will post a couple of things about this really fine piece. So here&#8217;s one issue. Restrict what we are talking about to propositional &#8230; <a class="more-link" href="http://certaindoubts.com/reasons-to-act-and-reasons-to-believe/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p>I&#8217;ve been reading Hawthorne and Stanley&#8217;s new piece &#8220;Knowledge and Action,&#8221; (downloadable <a href="http://www.rci.rutgers.edu/~jasoncs/knowledgeactionfinal.pdf">here</a> at Jason&#8217;s website), and will post a couple of things about this really fine piece.</p>
<p>So here&#8217;s one issue.  Restrict what we are talking about to propositional reasons, either for belief or for action.  Think, then, about transmission principles for rational belief.  What must be true about the reasons in question for transmission to be possible?  The usual answer is that an originating condition can&#8217;t transmit something it doesn&#8217;t possess already, so if a proposition is one&#8217;s reason for believing something, that proposition must itself be rational to believe.  With H&#038;S, I&#8217;ll assume that we should say similar things about rational action and rational belief on this score, and H&#038;S give a stronger requirement here:  in order for one&#8217;s reasons for believing q or doing A to be p, one must know that p.  I know Peter Unger defends such a principle about rational belief, but he&#8217;s a rare exception (and motivated toward strong requirements in service of a skepticism of amazingly wide scope).  The more ordinary viewpoint is narrower, insisting only that what gets transmitted must be present, but asking no more than this.</p>
<p>Here I&#8217;ll voice a worry I have about the stronger requirement, one concerning rational plans of relatively complex sorts.<br />
<span id="more-723"></span>Your long-term goal is to achieve G.  You can&#8217;t achieve G just by trying to do so directly, so you think about what small steps you can take that collectively will get you to G.  Call the small steps S1, S2, and S3, with the assumption that these are sequential steps (such as &#8220;finish high school&#8221;, &#8220;go to college&#8221;, &#8220;get a job,&#8221; etc.)  Associated with this plan, then, are conditionals:  if I try at all, S1 will be accomplished; if S1 is accomplished and and I try some more, S2 will obtain; etc.  One&#8217;s reasoning here is explicit, I will suppose:  one resolves to try, and by MP concludes that S1 is in the bag.  Same with the next level of trying, and so S2 is no problem.  Etc.  The result is a rational plan aimed at G, with lots of individual beliefs, both conditional beliefs and unconditional ones.  As described, this is a paradigm case of rational deliberation, and even if there are ways to fill out the example so that the beliefs turn out to be irrational, there is no hint of such in the story as told, so the proper view to take is that the schematic character of the example is fully compatible with being filled out so that the beliefs are fully rational.</p>
<p>But if propositional reasons have to be known to be true in order to be good reasons for believe, we&#8217;ll have problems here.  Ask me what&#8217;s going to happen over the next 10 years.  I tell you I&#8217;m going to take step S1, then S2, then S3, thereby securing goal G.  Now, which of these beliefs are rational and which aren&#8217;t?  One doesn&#8217;t have to be much of a skeptic to deny that every belief short of the last one about achieving G counts as knowledge.  In fact, the farther into the future the features of the plan, the more inclined we are to be skeptical here.  So maybe beliefs about the initial steps count as knowledge, but not much beyond those, and the farther into the future the plan extends, the lower the percentage of beliefs that will count as knowledge.  But long-term plans can be rational nonetheless, and we certainly don&#8217;t want a theory of rational planning that lets the plan be rational when a large percentage of the beliefs involved in the plan aren&#8217;t rational.</p>
<p>What we have here is an analogue of the preface paradox:  the plan is rational at least partly in virtue of the rationality of the beliefs about the parts of the plan, even though it is also rational to believe that not everything will go according to plan and that the plan will therefore have to be adjusted.  If so, however, that looks like a reason to doubt that transmission principles always require something epistemically stronger concerning the transmitter of rationality than what is transmitted itself, even if H&#038;S are correct that in a great many cases, something epistemically stronger is required of the transmitter of rationality than mere rationality itself.   For, as described, the formation of the plan involves inferences to intermediate conclusions that are themselves believed on the basis of prior inferential steps, depending ultimately on one&#8217;s intention to adopt the entire plan in order to achieve goal G.</p>
<p>One might try to give a rational reconstruction of what people actually do when they adopt plans, where the reconstruction involves only conditional attitudes of various sorts in place of the categorical beliefs that I&#8217;ve put in the example (i.e., so that you don&#8217;t really believe that the last steps in the plan will occur, but you only believe that if all goes well in prior steps, then the last steps will occur).  I think we should resist saying that plans of the sort here are only honorifically rational in virtue of some close relationship to a rational reconstruction in which the plan would really deserve the name.  Such rational reconstruction does give us one way to save the knowledge requirement, just as it gives us a way to try to escape the preface paradox (we might say that you don&#8217;t really believe each of the claims in the book, but only that you have a number of particular beliefs of the following sort:  if the really obvious parts of the book are true, then so is a given less obvious part (and you refuse to deduce from this claim!)).  So I don&#8217;t think rational reconstruction is a plausible way out.</p>
<p>One might also play the piggy-back game, requiring now that beliefs are rational only if known.  I won&#8217;t argue against this claim here, but it would be an interesting result if the H&#038;S requirement implied this apparently stronger connection between rationality and knowledge.  But maybe there are other escape routes?</p>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/reasons-to-act-and-reasons-to-believe/feed/</wfw:commentRss>
		<slash:comments>11</slash:comments>
		</item>
		<item>
		<title>Probability and Inference: Essays in Honour of Henry E. Kyburg, Jr.</title>
		<link>http://certaindoubts.com/probability-and-inference-essays-in-honour-of-henry-e-kyburg-jr/</link>
		<comments>http://certaindoubts.com/probability-and-inference-essays-in-honour-of-henry-e-kyburg-jr/#respond</comments>
		<pubDate>Sat, 02 Jun 2007 08:33:56 +0000</pubDate>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
				<category><![CDATA[epistemic paradoxes]]></category>
		<category><![CDATA[formal epistemology]]></category>
		<category><![CDATA[general]]></category>
		<category><![CDATA[major figures]]></category>

		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=705</guid>
		<description><![CDATA[Probability and Inference: Essays in Honour of Henry E. Kyburg, Jr., William Harper and Gregory Wheeler (eds.) (King&#8217;s College Publications, London, 2007) is now available at Amazon in the US and UK. New essays by Gert de Cooman and Enrique &#8230; <a class="more-link" href="http://certaindoubts.com/probability-and-inference-essays-in-honour-of-henry-e-kyburg-jr/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p><i>Probability and Inference: Essays in Honour of Henry E. Kyburg, Jr.</i>, William Harper and Gregory Wheeler (eds.) (King&#8217;s College Publications, London, 2007) is now available at Amazon in the <a href="http://www.amazon.com/Probability-Inference-Essays-Honour-Kyburg/dp/1904987184/ref=sr_11_1/104-1610350-8353521?ie=UTF8&#038;qid=1177965837&#038;sr=11-1">US</a> and <a href="http://www.amazon.co.uk/Probability-Inference-Essays-Honour-Kyburg/dp/1904987184/ref=sr_1_7/203-8618406-4985560?ie=UTF8&#038;s=books&#038;qid=1179081613&#038;sr=8-7">UK</a>.</p>
<p>New essays by Gert de Cooman and Enrique Miranda, Clark Glymour, William Harper, Isaac Levi, Ron Loui, John Pollock, Teddy Seidenfeld, Choh Man Teng, Mariam Thalos, Gregory Wheeler, Jon Williamson, and Henry E. Kyburg, Jr.</p>
<p>A pr&eacute;cis follows below the fold.<br />
<span id="more-705"></span></p>
<p>Recent advances in philosophy, artificial intelligence, mathematical psychology, and the decision sciences have brought a renewed focus to the role and interpretation of probability in theories of uncertain reasoning. Henry E. Kyburg, Jr. has long resisted the now dominate Bayesian approach to the role of probability in scientific inference and practical decision. The sharp contrasts between the Bayesian approach and Kyburg&#8217;s program offer a uniquely powerful framework within which to study several issues at the heart of scientific inference, decision, and reasoning under uncertainty.</p>
<p>Contents:</p>
<p>	<b>Gregory Wheeler&#8217;s</b> &#8220;A Review of the Lottery Paradox&#8221; serves as both a review of  the lottery paradox and a thematic introduction to the volume. The difference between the theory of rational acceptance and the Carnap-Jeffrey conception of rational acceptance is presented, and minimal conditions for resolving the puzzle are proposed and defended. Meeting these conditions, it is argued, entails engaging several of the issues discussed in this volume.</p>
<p><b>William Harper&#8217;s</b> &#8220;Acceptance and Scientific Method&#8221; presents an application of Kyburg&#8217;s theory of rational acceptance to capture the structure of Newton&#8217;s reasoning in <i>Principia</i>;</p>
<p><b>Choh Man Teng&#8217;s </b>&#8220;Conflict and Consistency&#8221; addresses consistency maintenance within evidential probability and presents a proposal for updating;</p>
<p><b>Gert de Cooman</b> and <b>Enrique Miranda&#8217;s</b> &#8220;Symmetry of Models versus Models of Symmetry&#8221; gives an introduction to the theory of Imprecise Probability and presents new results within IP that offer fresh insight into de Finetti&#8217;s exchangeability theorem;</p>
<p><b>Jon Williamson&#8217;s</b> &#8220;Motivating Objective Bayesianism&#8221; presents a defense of Objective Bayesianism, remarks on de Cooman and Miranda&#8217;s argument for interval valued probabilities, and argues that Evidential Probability ought to go whole hog and embrace Objective Bayesianism;</p>
<p><b>Clark Glymour&#8217;s</b> &#8220;Bayesian Ptolemaic Psychology&#8221; presents a thorough objection on complexity grounds to the claim that actual causal reasoning in humans is a form of Bayesian reasoning;</p>
<p><b>Ron Loui&#8217;s </b>&#8220;An Architecture for Purely Probabilistic Negotiating Agents&#8221; outlines an entirely new framework for agent negotiation;</p>
<p><b>Mariam Thalos</b> &#8220;Navigation: An Engineer&#8217;s Perspective&#8221; remarks on the source of Kyburg&#8217;s pragmatism;</p>
<p><b>John Pollock</b> &#8220;The Y-Function&#8221; presents an important new result in his Nomic Probability theory of direct inference;</p>
<p><b>Isaac Levi</b>, &#8220;Probability Logic and Logical Probability&#8221;, remarks on the historical place of evidential probability and launches an objection to how evidential probability handles conditioning;</p>
<p><b>Teddy Seidenfeld</b>, &#8220;Forbidden Fruit: When Epistemological Probability may <i>not</i> take a bite of the Bayesian apple&#8221; launches an objection to evidential probability as well, but also notes that EP avoids a problem that falls most interval probability accounts: dilation;</p>
<p><b>Henry Kyburg</b>, &#8220;Bayesian Inference with Evidential Probability&#8221; provides a brief history of evidential probability, including changes in recent versions of the theory, a reply to Seidenfeld&#8217;s Hollow Cube objection, and Levi&#8217;s objections, and presents a detailed example of an EP-inference.</p>
<p>The essays by Levi, Loui, Seidenfeld, and Thalos were presented at a Kyburg Symposium at the University of Rochester in October 2004. Finally, the volume is indexed to link together other common themes within the essays.</p>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/probability-and-inference-essays-in-honour-of-henry-e-kyburg-jr/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>Deductive Cogency and Probabilistic Coherence</title>
		<link>http://certaindoubts.com/deductive-closure-and-probabilistic-coherence/</link>
		<comments>http://certaindoubts.com/deductive-closure-and-probabilistic-coherence/#comments</comments>
		<pubDate>Fri, 01 Jun 2007 12:11:28 +0000</pubDate>
		<dc:creator><![CDATA[Kvanvig Jon]]></dc:creator>
				<category><![CDATA[epistemic paradoxes]]></category>
		<category><![CDATA[formal epistemology]]></category>

		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=695</guid>
		<description><![CDATA[A fairly standard approach to the preface paradox is basically Lockean. On the Lockean story, belief is degree of belief past a certain threshold. We then explain away the inconsistency involved in the paradox by an underlying probabilistic coherence. The &#8230; <a class="more-link" href="http://certaindoubts.com/deductive-closure-and-probabilistic-coherence/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p>A fairly standard approach to the preface paradox is basically Lockean.  On the Lockean story, belief is degree of belief past a certain threshold.  We then explain away the inconsistency involved in the paradox by an underlying probabilistic coherence.  The preface claim exceeds the threshold of belief, is inconsistent with the beliefs about the contents of the book, but probabilistically coheres because it accurately measures the risk of error in the book (and since the book is large enough, the risk of error is high enough to exceed the threshold).</p>
<p>In other cases, though, we explain away an underlying probabilistic incoherence among degrees of belief by cogency at the coarse-grained level.  Below the fold is a case of this sort.</p>
<p><span id="more-695"></span>You are an expert witness in a court case.  You are required to provide a sworn affadavit regarding what you may be called on to testify about in court.  The matter is fairly complex, etc., so your sworn testimony is quite extensive.  But you are an expert on the subject, so you prepare the content.  You then must swear in writing that what you have told is the truth, which you correctly interpret to mean that you have left out no significant facts that pertain to the case and that nothing you have written is mistaken.  You sign, and the document is notarized.</p>
<p>Note, though, that the legal document can precisely mirror a standard preface case, so if in the standard preface case, we can explain away inconsistency at the level of belief by probabilistic coherence at the level of credence, here we can explain away probabilistic incoherence at the level of credence by appeal to cogency at the level of belief.  For, since the two cases mirror each other, if probabilistic coherence exists in the preface case because the book is large enough, probabilistic incoherence will exist when you are asked to swear veracity regarding the content of your written testimony.</p>
<p>What to learn from this isn&#8217;t completely clear, except for one point.  I think the mere fact that one can find an underlying probabilistic coherence even though there is inconsistency at the level of coarse-grained belief, doesn&#8217;t by itself give us a solution to the paradox.  For what to explain away and what to explain it away with isn&#8217;t uniform, as the above example shows.</p>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/deductive-closure-and-probabilistic-coherence/feed/</wfw:commentRss>
		<slash:comments>11</slash:comments>
		</item>
		<item>
		<title>More on the Preface</title>
		<link>http://certaindoubts.com/687/</link>
		<comments>http://certaindoubts.com/687/#comments</comments>
		<pubDate>Sun, 22 Apr 2007 10:59:52 +0000</pubDate>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
				<category><![CDATA[epistemic paradoxes]]></category>
		<category><![CDATA[formal epistemology]]></category>
		<category><![CDATA[general]]></category>

		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=687</guid>
		<description><![CDATA[Back at Acme, our team of epistemologists are on the scene looking at Inspector 14&#8217;s record of length measurements for pole 453-01-120. Name this pole &#8220;p&#8221;. To simplify, suppose there are n physical measurements, the conditions for measurement were standardized, &#8230; <a class="more-link" href="http://certaindoubts.com/687/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p><a href="http://fleetwood.baylor.edu/certain_doubts/?p=686#comments">Back at Acme</a>, our team of epistemologists are on the scene looking at Inspector 14&#8217;s  record of length measurements for pole 453-01-120. Name this pole &#8220;p&#8221;. To simplify, suppose there are n physical measurements, the conditions  for measurement were standardized, the measurement device was calibrated, errors are distributed normally, et cetera, et cetera:</p>
<p>1. The measured length-1 of p is 6.005 meters.<br />
2. The measured length-2 of p is 6.003 meters.<br />
&#8230;<br />
<i>n</i>. The measured length-<i>n</i> of p is 5.099 meters.<br />
<span id="more-687"></span><br />
Our assumptions about those <i>n</i> measurements license us to view them as a random sample from all possible measurements of p. This in effect is what it means to assume that errors are normally distributed. The negation of any of the other conditions (e.g., calibration, standardization) would function like <i>defeaters</i> for the randomization assumption, and block the inference I am about to make.</p>
<p>Because we have good reason to view those <i>n</i> measurements as randomly drawn, and no evidence that would suggest that <i>n</i> is a biased sample, we say the length of p is the mean of those <i>n</i> measurements plus or minus the product of 1.96 and 2 over the square root of <i>n</i>.</p>
<p>The example imagines that the mean of n is 6.003; the &#8216;plus or minus (1.96 x 2/root n)&#8217; gives us the confidence interval, at 2 standard deviations, which we imagine to be [6.002, 6.004] meters.  This says that the 95% confidence interval for the length of the flag pole p is [6.002, 6.004] meters. This expresses that 95% of the intervals calculated in this fashion will contain the true value of p; or, alternatively, that the probability that the true length of p is equal to a value within [6.002, 6.004] meters is 0.95.</p>
<p>Now to the philosophy of statistics:</p>
<p>The first point to notice is that we are talking about the actual length of the pole, which is a fixed-value (under standardized conditions, et cetera, et cetera) rather than a random variable. The actual length of the pole is either between 6.002 and 6.004 or it isn&#8217;t; there is no random variable to attach this 0.95 probability to. Rather, we say that we are confident to level 0.95 that the interval we calculated contains the length in meters of this pole. So, assuming here that 0.95 is a high enough confidence level, we accept flat out that the pole is between 6.002 and 6.004. And notice that this is what Inspector 14 <i>does</i>: he puts his &#8220;Inspected by 14&#8221; sticker on <i>p</i> and adds it to the stock of Model A&#8217;s; he does not put 95% of the sticker on the pole, nor hedge his bet by thinking that it is partially <i>not</i> in the stock of Model A&#8217;s. It&#8217;s an A, in his judgment, on his evidence.</p>
<p>Notice what happens when you switch to the Bayesian view, for you <i>can</i> tell a mathematically equivalent story in Bayesian dress. If you want to hedge, want to assign a probability to a proposition, then you&#8217;ve got to have a random variable. So, the Bayesian interpretation lets the length of the pole be a random variable! This doesn&#8217;t make sense directly, of course: a pole can be no more 95% a magnitude of length than a woman can be 95% pregnant. Either pole or woman has or has not the stated attribute. But, if you change the story around to talk about the belief about the pole, or the belief that a certain woman is pregnant, then you can view the interval as bounds on your (non-extreme) credal probability that the length of the pole (pregnancy status of the woman in question) is in that prescribed interval.</p>
<p>Now the jump from statistics to epistemology!</p>
<p>To effect this move, a Bayesian must assume a flat <i>subjective</i> prior probability distribution to get his mathematical representation of this story into equilibrium with the frequentist story. If you are a statistician doing this, this isn&#8217;t (or doesn&#8217;t need to be) such a big problem: you are interested in tools for modeling parameters, and you know about this equivalence between methods and can exploit whichever seems right for the task.</p>
<p>But, if you are an epistemologist making this move, you&#8217;ll likely be driven to do so by ideas about principles of rational belief fixation, and if you are making this move to explicate partial belief and use the Kolmogorov axioms as consistency constraints on such a notion, then you are doing so not as a technical ploy but rather because you think that issues of rationality are in play.</p>
<p>But for the epistemologist so-described the assumption of a flat prior takes on enormous weight, for it is not always reasonable to assume a flat prior, and the only reason that it is reasonable to do so here is because this is the assumption needed to get the Bayesian reconstruction to agree with the classical treatment of measurement! So, this reliance on a flat prior is crucial; the whole normative story hangs on it, but it is a thin reed from which to hang rational belief.</p>
<p>(<i>I should stress that Ralph rejects probabilism, but he does accept a notion of partial belief; it remains to be seen whether there is a view of partial belief as opposed to full belief that escapes the general objection here.</i>)</p>
<p>To tie this post up, a thought about the vaguely contemporary, mainstream view on the lottery paradox. In my view the move to avoid inconsistency by adopting partial belief  pushes the key epistemic problems into the philosophy of science, where one finds ready assurances that the complaints I have just made are old  and solved, or old and nearly solved, or at least <i>old</i>. From a review of the literature on the lottery paradox, the most striking feature I found in my reading was that at some point in the late 70s or early 80s, people largely stopped engaging the philosophy of statistics and instead focused on  the <i>thought experiments</i> of the lottery and the preface. Epistemologists began writing on the particulars of these puzzles while assuming that the fundamental parameters of the puzzle were fixed, such as the interpretation of probability.</p>
<p>Christensen&#8217;s book is remarkable in this sense, because it is constructed entirely within this contemporary and restricted view of the paradoxes; it primarily engages the post 1980 &#8220;received view&#8221; literature.</p>
<p>There is, however, a recent lottery literature outside of this tradition, some of which is arguable outside of philosophy. Nevertheless, there is engaging work to consider. Joe Halpern addresses the lottery with a theory of first-order probability; the field of non-monotonic reasoning, going back to Hayes and McCarthy in 1969, has observed a tension between defeasible conditions for belief fixation and logical rules for manipulating those beliefs: David Makinson is more famous for the AGM belief revision paradigm than his Analysis paper; Eric Neufeld and Scott Goodwin rebut Pollock&#8217;s claim that there are fundamental differences between the lottery and the preface (which might make a good foil, Ralph, btw: <i>Computational Intelligence</i>, 14(3),1998); and, to self promote, I&#8217;ve proposed a limited system for 1-monotone capacities (JoLLI,2006) that gives a unified treatment of the lottery and preface, a statistical default logic (NMR2004) which treats the normality assumption for sample distributions as a default assumption, and a review of the lottery paradox in the 2007 Harper and Wheeler collection.</p>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/687/feed/</wfw:commentRss>
		<slash:comments>6</slash:comments>
		</item>
		<item>
		<title>A Simple Solution to the &#8220;Preface Paradox&#8221;</title>
		<link>http://certaindoubts.com/a-simple-solution-to-the-preface-paradox/</link>
		<comments>http://certaindoubts.com/a-simple-solution-to-the-preface-paradox/#comments</comments>
		<pubDate>Fri, 20 Apr 2007 13:37:44 +0000</pubDate>
		<dc:creator><![CDATA[Ralph Wedgwood]]></dc:creator>
				<category><![CDATA[epistemic paradoxes]]></category>

		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=686</guid>
		<description><![CDATA[Many philosophers argue that paradoxes like the so-called &#8220;preface paradox&#8221; show that it is not a requirement of rationality that the contents of one’s beliefs should all be consistent with each other. (For example, David Christensen argues for this in &#8230; <a class="more-link" href="http://certaindoubts.com/a-simple-solution-to-the-preface-paradox/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p>Many philosophers argue that paradoxes like the so-called &#8220;preface paradox&#8221; show that it is not a requirement of rationality that the contents of one’s beliefs should all be consistent with each other. (For example, David Christensen argues for this in his recent book <em>Putting Logic in its Place</em>.) I believe that this argument is mistaken.</p>
<p>As a matter of fact, I don’t actually accept that it is a requirement of rationality that the contents of one’s beliefs should all be consistent with each other. If the contradictions are lurking in obscure hidden parts of one’s belief set – so that it would take an extremely long and complicated chain of reasoning to derive any contradiction from the contents of one’s beliefs, and one has not in fact performed any such long and complicated chain of reasoning – then it need not be irrational for one to persist in one’s beliefs (or so I am inclined to think).</p>
<p>However, I am prepared to defend a different connection between consistency and rational belief. If one consciously considers a set of propositions, and this set of propositions is <em>obviously</em> logically inconsistent, then one should revise one’s beliefs in such a way as to avoid fully believing all members of this inconsistent set. (The paradigm case of a set of propositions that is &#8220;obviously logically inconsistent&#8221; is the set {<em>p</em>, ‘¬ <em>p</em>’}, but other sets of propositions may also count as &#8220;obviously logically inconsistent&#8221; as well, such as {<em>p</em>, <em>q</em>, ‘¬ (<em>p</em> &#038; <em>q</em>)’}, and so on.) <span id="more-686"></span></p>
<p>Still, I don’t think that the preface paradox is a good argument against the stronger consistency requirement. In fact, the flaw that I see in the argument has already been pointed out some years ago by Simon Evnine – albeit in the course of arguing for a very strong claim that I wouldn’t myself accept, that rationality requires one to believe the conjunction of all of one’s beliefs (see his &#8220;Believing Conjunctions&#8221;, <em>Synthese</em> 1999).</p>
<p>The paradox is that it seems rational to believe the &#8220;Preface Proposition&#8221;:</p>
<blockquote><p>At least one of the propositions that I believe (other than this proposition) is false.</p></blockquote>
<p>But if one believes this proposition, that <em>guarantees</em> that at least one of one’s beliefs is false! So it seems that it is rational to believe this proposition, even though if one does so, one’s beliefs are inconsistent.</p>
<p>In fact, this argument is fallacious: even if one believes the &#8220;Preface Proposition&#8221;, it does <em>not</em> follow that the <em>contents</em> of one’s beliefs – that is, the <em>set of propositions that one believes</em> – are logically inconsistent. Unlike most of the propositions that one believes, the Preface Proposition is a <em>higher-order</em> proposition. As Evnine rightly insists, the conjunction of the propositions that one believes is not logically equivalent to the proposition that everything that one believes is true; and the proposition that something that one believes is false is not equivalent to the disjunction of the negations of the propositions that one believes. Even if one believes the &#8220;Preface Proposition&#8221; there is a possible situation in which all of the propositions that one actually believes are true – namely, a situation in which one in addition believes some <em>further</em> proposition (which one does not actually believe) and that further proposition is false.</p>
<p>Christensen objects to Evnine’s point by saying that an ideally self-aware thinker would recognize that a certain long conjunction was equivalent to the conjunction of all the propositions that she believes, and so would take the Preface Proposition to be equivalent to the negation of this long conjunction (and so, presumably, this ideally self-aware thinker would also infer the negation of this long conjunction from the Preface Proposition, in which case the contents of this thinker&#8217;s beliefs would be logically inconsistent). It would be &#8220;unpersuasive&#8221;, Christensen says, to insist that it is only thinkers who are not ideally self-aware in this way who are rationally entitled to believe the Preface Proposition. Our lack of such ideal self-awareness is a &#8220;superficial limitation&#8221; (<em>Putting Logic in its Place</em>, p. 38).</p>
<p>But it seems to me that our lack of such ideal self-awareness is anything but a “superficial limitation&#8221;. This inability to survey such large totalities of propositions seems one of the fundamental limitations of any finite mind. Of course an infinite or omniscient mind (in so far as we can make sense of what such a mind would be like) would not be subject to such limitations, but presumably it would not be rational for such an omniscient mind to believe the Preface Proposition.</p>
<p>Contrast the following two cases. In the first case, I write an extremely short &#8220;book&#8221; containing just one simple assertion. It seems that it would be irrational for me to both to believe that assertion and to believe that the book contains errors. In the second case, I write an enormous 600-page book containing thousands upon thousands of assertions. Then it seems to me that may well be perfectly rational for me to believe each assertion in the book but also to believe that the book must contain some errors. It seems to me that one crucial difference between these cases is that it is so much harder to survey the totality of propositions that are asserted in the longer book. No rational thinker who has evidence of his own fallibility would believe of any single proposition that it is logically equivalent to the conjunction of all and only the assertions in that book. So I don’t think that Christensen has identified a problem with this aspect of Evnine’s response to the so-called &#8220;preface paradox&#8221;.</p>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/a-simple-solution-to-the-preface-paradox/feed/</wfw:commentRss>
		<slash:comments>14</slash:comments>
		</item>
	</channel>
</rss>
