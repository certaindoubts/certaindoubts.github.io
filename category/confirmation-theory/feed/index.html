<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>confirmation theory &#8211; </title>
	<atom:link href="http://certaindoubts.com/category/confirmation-theory/feed/" rel="self" type="application/rss+xml" />
	<link>http://certaindoubts.com</link>
	<description>devoted to matters epistemic</description>
	<lastBuildDate>Wed, 10 Apr 2019 01:35:13 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.10</generator>
	<item>
		<title>A Puzzle for E=K from Incremental Confirmation via Mounting Evidence</title>
		<link>http://certaindoubts.com/a-puzzle-for-ek-from-incremental-confirmation-via-mounting-evidnece/</link>
		<comments>http://certaindoubts.com/a-puzzle-for-ek-from-incremental-confirmation-via-mounting-evidnece/#comments</comments>
		<pubDate>Sat, 18 Jun 2011 15:29:33 +0000</pubDate>
		<dc:creator><![CDATA[Trent Dougherty]]></dc:creator>
				<category><![CDATA[confirmation theory]]></category>
		<category><![CDATA[justification]]></category>
		<category><![CDATA[knowledge]]></category>
		<category><![CDATA[major figures]]></category>

		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=2765</guid>
		<description><![CDATA[This has probably been discussed somewhere, but I haven’t seen it. 1. Here’s a pretty natural picture of learning from experience.  A scientist S proposes some hypothesis H (which we will assume is true).  She gathers evidence and it’s favorable &#8230; <a class="more-link" href="http://certaindoubts.com/a-puzzle-for-ek-from-incremental-confirmation-via-mounting-evidnece/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<div>This has probably been discussed somewhere, but I haven’t seen it.</p>
<p>1. Here’s a pretty natural picture of learning from experience.  A scientist S proposes some hypothesis H (which we will assume is true).  She gathers evidence and it’s favorable to H, so she’s encouraged.  She gathers some more evidence, it’s also favorable, so she really thinks she’s on to something.  More inquiry, more evidence, more (properly based) rational credence.  Yet more.  Finally, at some point, S “fully believes” H, in that her evidence supports it very strongly (though not maximally) and her rational credence is very strong as well (or whatever you want belief to be).  One can conceive experiments which could falsify it, but those experiments can’t be done with current technology, and there is no particular reason to think those experiments would go badly for H.  S has come to know H on the basis of accumulating evidence (I stipulate that this is explanatory and experimental evidence, not merely statistical evidence, though some statistics might be involved).</p>
<p>Here’s what seems like a platitude in light of the naturalness of the picture above. <span id="more-2765"></span></p>
<p>EP &#8211; One can come to learn some true P via accumulating evidence that P.</p>
<p>2. I’m highly confident that Timothy Williamson believes the following two claims.</p>
<p>A. “Some beliefs fall shorter of justification than others. In that respect we can grade beliefs by their probability on the subject’s evidence&#8230;”</p>
<p>B.  “a belief is fully justified if and only if it constitutes knowledge”</p>
<p>This makes it sound like he accepts the Natural Picture above.  But&#8230;</p>
<p>How do we answer the question How probable must something be to be known?  I’m not raising the common vagueness question here, I’m fine with borderline cases.  This problem generalizes for any item of true belief at any degree of justification which becomes an item of knowledge.  Suppose it’s n/m.  Now consider the case above.  What happens the *moment* it get’s to n/m?  It becomes an item of knowledge, right?  But if E = K, then its probability must be 1.  And if it is *in virtue of* it’s being n/m probable (modulo the other assumptions) and it’s being evidence is *constituted* by it’s being knowledge, it seems as if we have a proposition the probability of which—at one and the same time—has probability n/m *and* probability 1.  What’s going on?</p>
<p>A related but different problem from the Dual Probability Problem (DPP) is the Quantum Leap Problem (QLP).  The puzzle is that the graph of the probability of H over time will look very strange.  It starts out low and rises at a smooth slope of 30-45 degrees and then BAMB it shoots straight up&#8211;perfectly vertically&#8211;to 100%.  That just seems strange to me.</p>
<p>In what way can Williamson and other E=Kers (pronounced “eekers”) affirm EP?  What’s the story or surrogate?</p></div>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/a-puzzle-for-ek-from-incremental-confirmation-via-mounting-evidnece/feed/</wfw:commentRss>
		<slash:comments>9</slash:comments>
		</item>
		<item>
		<title>There&#8217;s no such thing as defeat</title>
		<link>http://certaindoubts.com/theres-no-such-thing-as-defeat/</link>
		<comments>http://certaindoubts.com/theres-no-such-thing-as-defeat/#comments</comments>
		<pubDate>Wed, 29 Sep 2010 01:34:36 +0000</pubDate>
		<dc:creator><![CDATA[Trent Dougherty]]></dc:creator>
				<category><![CDATA[confirmation theory]]></category>
		<category><![CDATA[justification]]></category>

		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=2220</guid>
		<description><![CDATA[Or, more cautiously (sometimes I&#8217;m cautious), I find defeat talk misleading.  And here&#8217;s why. Let q be the true proposition that S gives testimony with content p.  This is a reason for me, at t1, the time of my hearing &#8230; <a class="more-link" href="http://certaindoubts.com/theres-no-such-thing-as-defeat/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p>Or, more cautiously (sometimes I&#8217;m cautious), I find defeat talk misleading.  And here&#8217;s why.</p>
<p>Let q be the true proposition that S gives testimony with content p.  This is a reason for me, at t1, the time of my hearing the testimony, to accept p, since I’m aware that S is generally reliable.  But then I learn, at t2, d: that in spite of his general reliability, S is known to be unreliable when it comes to p-type matters.</p>
<p><span id="more-2220"></span>In defeat language, d defeats the support q gives to p. But what can this mean?  What has been defeated?  Certainly not the connection between my original evidence (captured in E1 below) and p.  Consider the following two sets of evidence.</p>
<p>E1: S said “p” &amp; S is generally reliable.</p>
<p>E2: E1 &amp; S is unreliable concerning p-type questions.</p>
<p>E1 characterizes my evidence at t1, and E2 characterizes my evidence at t2.  At t1 my total relevant evidence supported p.  At t2 it did not.  But what is it that has been defeated?  Not my justification at t1!  Justification is a synchronic matter.  My transition from being justified in believing p to not being so is a diachronic matter.</p>
<p>And even at t2 E1 supports p.  Its support for p is an objective relation it bears at all times if at any time.  That relation doesn’t fail to bear just because it’s part of a larger body of evidence such that that larger body of evidence doesn’t bear that relation.</p>
<p>In short: If there is defeat, then there is something that is defeated.  But there is no viable candidate for anything that has been defeated.</p>
<p>I followed Chisholm in defining defeat over a support relation (3rd ed, 55).  But I don&#8217;t see that it&#8217;s any better following Bergmann in defining defeat over target beliefs or propositions.  My justification at t1 was never defeated.  I just got new evidence and on that evidence I was no longer justified.  Nothing is defeated.</p>
<p>We sometimes talk about one item of information &#8220;screening off&#8221; another, but I think that&#8217;s more helpful when thinking about causation.  Perhaps defeat talk is innocuous, but I find it misleading and sometimes (today) it irks me.</p>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/theres-no-such-thing-as-defeat/feed/</wfw:commentRss>
		<slash:comments>19</slash:comments>
		</item>
		<item>
		<title>FEW Review</title>
		<link>http://certaindoubts.com/few-review/</link>
		<comments>http://certaindoubts.com/few-review/#respond</comments>
		<pubDate>Tue, 07 Sep 2010 01:49:25 +0000</pubDate>
		<dc:creator><![CDATA[Trent Dougherty]]></dc:creator>
				<category><![CDATA[confirmation theory]]></category>
		<category><![CDATA[formal epistemology]]></category>

		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=2154</guid>
		<description><![CDATA[I got back yesterday from the 7th annual Formal Epistemology Workshop (FEW).  [I presented my over-titled “Dealing with Disagreement from the First-person Perspective: A Probabilist(tm) Proposal” (draft) (slides).]  It was held in Konstanz, Germany, just north of the Swiss border &#8230; <a class="more-link" href="http://certaindoubts.com/few-review/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<div>I got back yesterday from the 7th annual Formal Epistemology Workshop (<a href="http://fitelson.org/few/" target="_blank">FEW</a>).  [I presented my over-titled “Dealing with Disagreement from the First-person Perspective: A Probabilist(tm) Proposal” (<a href="http://bearspace.baylor.edu/Trent_Dougherty/Papers/Dougherty_Dealing_with_Disagreement.pdf" target="_blank">draft</a>) (<a href="http://bearspace.baylor.edu/Trent_Dougherty/Papers/Dougherty_Disagreement_Slides.pdf" target="_blank">slides</a>).]  It was held in Konstanz, Germany, just north of the Swiss border at the University of Konstanz.  It was awesome.  Konstanz is an absolutely beautiful location.  I can’t possibly convey how wonderful it was.  And local organizer <a href="http://www.uni-konstanz.de/philosophie/huber/" target="_blank">Franz Huber</a> was the consummate host, along with his able assistants.  Franz and Branden and all the folks who helped organize are to be congratulated for a wonderful conference in a wonderful locale.</p>
<p>In my next post, I’ll be focusing on the work of <a href="http://www.schupbach.org/" target="_blank">Jonah Schupbach</a>, but I want to make a general comment on the conference as a whole.  I went to almost every session over the three or four days and I did not hear a single paper which focused on barren formalisms for the sake of formalism.  Whatever negative stereotypes there might be of formal philosophy, they were in short supply at the FEW.  On the contrary, movement leaders like <a href="http://www.fitelson.org/" target="_blank">Branden Fitelson</a> and <a href="http://sites.google.com/site/michaeltitelbaum/" target="_blank">Mike Titlebaum</a> (and, hey, what’s the deal with formal epistemologists with surnames of the form X-itel/itle-Y?) know their traditional epistemology plenty well and connect their formal work to it in concrete ways.</p>
<p>So I really encourage traditional folks&#8211;and the majority of my work is (very) traditional&#8211;to give formal epistemology a chance if you’ve been a bit skeptical.  There’s some really good stuff going on!</p></div>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/few-review/feed/</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>What is a probability judgment?</title>
		<link>http://certaindoubts.com/what-is-a-probability-judgment/</link>
		<comments>http://certaindoubts.com/what-is-a-probability-judgment/#comments</comments>
		<pubDate>Mon, 31 May 2010 23:32:12 +0000</pubDate>
		<dc:creator><![CDATA[Trent Dougherty]]></dc:creator>
				<category><![CDATA[confirmation theory]]></category>
		<category><![CDATA[formal epistemology]]></category>
		<category><![CDATA[major figures]]></category>

		<guid isPermaLink="false">http://el-prod.baylor.edu/certain_doubts/?p=1906</guid>
		<description><![CDATA[I find the notion of outright belief pretty puzzling.  I’m pretty inclined to take it to be a (probably context-sensitive or interest-relative) notion concerning degree of belief sufficiently close to certainty, or “practical certainty”. One reason to think there’s a &#8230; <a class="more-link" href="http://certaindoubts.com/what-is-a-probability-judgment/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p>I find the notion of outright belief pretty puzzling.  I’m pretty inclined to take it to be a (probably context-sensitive or interest-relative) notion concerning degree of belief sufficiently close to certainty, or “practical certainty”.</p>
<p>One reason to think there’s a purely epistemic notion of outright belief I often here&#8211;when I’m holding forth about the sufficiency of assigning probabilities&#8211;is that to assign a probability n to some proposition p entails holding the outright belief that the probability of p is n.  I don’t have a satisfying answer to this.  I am quite certain that I am never fully certain in a probability judgment, but I’m completely at a loss as to what to do about higher-order probabilities (other than to punt to psychological limitations and say something like “Well, as many orders as I can ascend, the probabilities don’t dwindle that much.”)</p>
<p>Jeffrey gives a reply on p. 46 of _Probability and the Art of Judgment_ but it seems completely unconvincing to me.  One move is to go operationalistc to some degree or other.  Pure operationalism is a dead end, but it’s often hard to tell whether an approach&#8211;like that of Kaplan’s&#8211;is operationalistic or holistic.  Every time I try to state or understand or defend a holistic view of what it is to assign a probability or make a probability judgment it sounds operationalistic.  This is one of the very most frustrating problems to me.</p>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/what-is-a-probability-judgment/feed/</wfw:commentRss>
		<slash:comments>18</slash:comments>
		</item>
		<item>
		<title>Christensen’s De-pragmatized Dutch Book Argument</title>
		<link>http://certaindoubts.com/christensen%e2%80%99s-de-pragmatized-dutch-book-argument/</link>
		<comments>http://certaindoubts.com/christensen%e2%80%99s-de-pragmatized-dutch-book-argument/#comments</comments>
		<pubDate>Mon, 30 Apr 2007 14:29:58 +0000</pubDate>
		<dc:creator><![CDATA[Kvanvig Jon]]></dc:creator>
				<category><![CDATA[confirmation theory]]></category>
		<category><![CDATA[formal epistemology]]></category>

		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=694</guid>
		<description><![CDATA[I got back yesterday from the conference on formal and traditional epistemology at Oklahoma organized by Jim Hawthorne and Wayne Riggs. It was utterly fabulous! Except that I was really sick when I talked, and had to leave early to &#8230; <a class="more-link" href="http://certaindoubts.com/christensen%e2%80%99s-de-pragmatized-dutch-book-argument/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p>I got back yesterday from the conference on formal and traditional epistemology at Oklahoma organized by Jim Hawthorne and Wayne Riggs.  It was utterly fabulous!  Except that I was really sick when I talked, and had to leave early to get home (with a fever of 102!).  I have very little idea what I said, except for one remarkable lapse:  I forgot what my last argument was supposed to be!  I don&#8217;t know what I said, but what I wanted to say, I&#8217;ll write here.</p>
<p>It&#8217;s about David Christensen&#8217;s DBA from his beautiful book <em>Putting Logic in Its Place</em> and the way in which subjectivists should understand the perspectival character of rationality.  The book is easy to read, very entertaining, and the arguments quite compelling, especially the ones about the import of the Preface Paradox for deductive closure principles about rationality.   But the argument that I don&#8217;t think quite succeeds is the argument for probabilistic incoherence being a defect.</p>
<p>Here&#8217;s how the argument goes.</p>
<p><span id="more-694"></span>The argument is intended to sustain the following result:<br />
	Simple Agent Probabilism: If a simple agent’s degrees of belief violate the probability axioms, they are defective.</p>
<p>A simple agent is one who values money and nothing else, and whose money preferences are positively linear.  In this way, the value of each extra dollar is the same as the value of any dollar, no matter how much money the agent has or lacks.  The argument begins with the following principle:<br />
	Sanctioning: A simple agent’s degrees of belief sanction as fair monetary bets at odds matching his degrees of belief.</p>
<p>Christensen says,</p>
<blockquote><p>I take these as very plausible normative judgements: any agent who values money positively and linearly, and who cares about nothing else, should evaluate bets in this way.  The way in question here is the way described in Sanctioning: if your degree of belief in p is 2/3 and you are offered a bet that will pay you $1 if p is true and cost you $2 if p is false, then if you are a simple agent, you should regard this bet as fair.  We can understand, then, the role of the appeal to simplicity of the agent: it is a way of controlling for interference into the assessment of rationality of degrees of belief by messiness concerning preferences.  A person’s preferences might be inconsistent; they might be non-linear; they might violate transitivity; etc.  When we want to connect attitudes about fairness of bets to the rationality of degrees of belief, we want to control for such insanity (if such we prefer to label it).  Once we control for this by stipulating that the agent is simple, the hope is that we can read off conclusions about rational credences from information about attitudes toward fair bets.  </p></blockquote>
<p>The remainder of the argument uses the following principles:<br />
	Bet Defectiveness: For a simple agent, a set of bets that is logically guaranteed to leave him monetarily worse off is rationally defective.<br />
	Belief Defectiveness: If a simple agent’s beliefs sanction as fair each of a set of bets, and that set of bets is rationally defective, then the agent’s beliefs are rationally defective.<br />
	Dutch Book Theorem: If an agent&#8217;s degrees of belief violate the probability axioms, then there is a set of monetary bets, at odds matching those degrees of belief, that will logically guarantee the agent’s monetary loss.</p>
<p>So, the idea of the argument is this.  First, restrict discuss to simple agents, so that money and a linear ordering on it is all we need to talk about to measure the agent’s preferences.  Fair bets for such an agent are ones that match the agent’s degree of belief.  So, to have rational degrees of belief, a simple agent’s degrees of belief have to match his attitudes about fair bets.  Unfair bets favor the book or favor the agent.  One class of bets that favors the book are those that are logically guaranteed to favor the bookie.  These we say are rationally defective.</p>
<p>The trick of the argument is to connect this Bet Defectiveness principle with the principle that follows it, Belief Defectiveness.  Here is the way Christensen argues.  He first notes that Belief Defectiveness doesn’t follow from Bet Defectiveness.  He considers cases of what I will call “messy preferences&#8221; to show that the second doesn’t follow from the first.  In such cases, the value of a payoff for one bet may affect the value of the payoff for the next bet: I’ll value roast duck more, he says, if I don’t yet have one than if I do.  But, of course, this possibility is ruled out by noting the qualification in the principles in question that the agents in question are assumed to be simple agents.  So, the claim is, without messy preferences, Belief Defectiveness follows from Bet Defectiveness, since without value interference, bets that are individually acceptable will also be acceptable in combination.  Once we get to Belief Defectiveness, the rest is just math!  Add in the Dutch Book Theorem and you get Simple Agent Probabilism.<br />
This argument assumes that messy preferences are the only things that can block the move from defects in bets to defects in beliefs, but they clearly are not.  Here is where holism about rationality conflicts with probabilistic coherence requirements.  Christensen claims that the bet described in the last paragraph is one that, given your degree of belief you should regard as fair.  We should ask, however, whether this is a prima facie or ultima facie “should&#8221;.  As an ultima facie claim, it won’t be true unless messy preferences are all that need to be controlled for to line up attitudes about fair bets with degrees of belief.</p>
<p>The point to note, however, is this:  it is not only preferences that can be messy, but beliefs as well.  Suppose I am a strongly puritanical Calvinist, believing wholeheartedly in the work ethic of such a view, and holding that divine anger will be visited on any who try to benefit in ways outside of honest toil.  My degree of belief in p might be 2/3, but I won’t regard the bet in question as fair; in fact, I’ll regard every bet offered as an unfair bet.  I will think any bet of any sort will work to my disadvantage.  Or, again, suppose I’m convinced that bets are quite often finkish: that accepting them triggers a change in the prospects of winning.  So, no matter what bet we’re considering, I can’t regard any bet as a fair bet, because there aren’t any such bets:  fairness of a bet, I also believe, requires that no party to the bet will have a special advantage over the other party, and that viewpoint is one we can’t ever accept.  Or, once more, suppose I can’t tell what my degree of belief is and I have no idea on reflection what the chances of p are.  You ask me whether a bet on p that costs $2 and pays $3 if p is true is a fair bet.  I have no idea.  Perhaps I’ll reflect further and come to some decision in some way or other, but my reflection may lead to either answer compatible with my inaccessible degree of belief being 2/3.</p>
<p>In each of these cases, nothing said contravenes the assumption that the agent in question is a simple agent.  What is messy here are not the preferences of the agent, but rather the beliefs of the agent.  Moreover, in each case, there is no reason to view the additional features of the agent’s perspective on the world to be out of order or inadmissible or illegitimate in a way that would allow one to ignore such complexities.  In each case, the point of the example is to show that a proper understanding of the agent’s perspective on the world requires more, and perhaps less, than an account of some particular degree of belief they have.  Rationality is perspectival in this sense, and it is a mistake to think that the degree of belief in question is all that needs to be noted in characterizing the perspective in question.</p>
<p>Once these points are noted, we can only accept a prima facie version of the Sanctioning principle:<br />
	PFS (Prima Facie Sanctioning): A simple agent’s degrees of belief prima facie sanction as fair, i.e. give one a defeasible reason for regarding as fair, monetary bets at odds matching his degrees of belief.<br />
PFS, however, is not strong enough to sustain Simple Agent Probabilism, except when the defect of suffering from probabilistic incoherence is itself taken only to be a prima facie defect.</p>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/christensen%e2%80%99s-de-pragmatized-dutch-book-argument/feed/</wfw:commentRss>
		<slash:comments>12</slash:comments>
		</item>
		<item>
		<title>Christensen&#8217;s De-pragmatized Dutch Book Argument</title>
		<link>http://certaindoubts.com/christensen%e2%80%99s-de-pragmatized-dutch-book-argument-2/</link>
		<comments>http://certaindoubts.com/christensen%e2%80%99s-de-pragmatized-dutch-book-argument-2/#comments</comments>
		<pubDate>Mon, 30 Apr 2007 14:29:58 +0000</pubDate>
		<dc:creator><![CDATA[Kvanvig Jon]]></dc:creator>
				<category><![CDATA[confirmation theory]]></category>
		<category><![CDATA[formal epistemology]]></category>

		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=694</guid>
		<description><![CDATA[I got back yesterday from the conference on formal and traditional epistemology at Oklahoma organized by Jim Hawthorne and Wayne Riggs. It was utterly fabulous! Except that I was really sick when I talked, and had to leave early to &#8230; <a class="more-link" href="http://certaindoubts.com/christensen%e2%80%99s-de-pragmatized-dutch-book-argument-2/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p>I got back yesterday from the conference on formal and traditional epistemology at Oklahoma organized by Jim Hawthorne and Wayne Riggs.  It was utterly fabulous!  Except that I was really sick when I talked, and had to leave early to get home (with a fever of 102!).  I have very little idea what I said, except for one remarkable lapse:  I forgot what my last argument was supposed to be!  I don&#8217;t know what I said, but what I wanted to say, I&#8217;ll write here.</p>
<p>It&#8217;s about David Christensen&#8217;s DBA from his beautiful book <em>Putting Logic in Its Place</em> and the way in which subjectivists should understand the perspectival character of rationality.  The book is easy to read, very entertaining, and the arguments quite compelling, especially the ones about the import of the Preface Paradox for deductive closure principles about rationality.   But the argument that I don&#8217;t think quite succeeds is the argument for probabilistic incoherence being a defect.</p>
<p>Here&#8217;s how the argument goes.</p>
<p><span id="more-940"></span>The argument is intended to sustain the following result:<br />
	Simple Agent Probabilism: If a simple agent&#8217;s degrees of belief violate the probability axioms, they are defective.</p>
<p>A simple agent is one who values money and nothing else, and whose money preferences are positively linear.  In this way, the value of each extra dollar is the same as the value of any dollar, no matter how much money the agent has or lacks.  The argument begins with the following principle:<br />
	Sanctioning: A simple agent&#8217;s degrees of belief sanction as fair monetary bets at odds matching his degrees of belief.</p>
<p>Christensen says,</p>
<blockquote><p>I take these as very plausible normative judgements: any agent who values money positively and linearly, and who cares about nothing else, should evaluate bets in this way.  The way in question here is the way described in Sanctioning: if your degree of belief in p is 2/3 and you are offered a bet that will pay you $1 if p is true and cost you $2 if p is false, then if you are a simple agent, you should regard this bet as fair.  We can understand, then, the role of the appeal to simplicity of the agent: it is a way of controlling for interference into the assessment of rationality of degrees of belief by messiness concerning preferences.  A person&#8217;s preferences might be inconsistent; they might be non-linear; they might violate transitivity; etc.  When we want to connect attitudes about fairness of bets to the rationality of degrees of belief, we want to control for such insanity (if such we prefer to label it).  Once we control for this by stipulating that the agent is simple, the hope is that we can read off conclusions about rational credences from information about attitudes toward fair bets.  </p></blockquote>
<p>The remainder of the argument uses the following principles:<br />
	Bet Defectiveness: For a simple agent, a set of bets that is logically guaranteed to leave him monetarily worse off is rationally defective.<br />
	Belief Defectiveness: If a simple agent&#8217;s beliefs sanction as fair each of a set of bets, and that set of bets is rationally defective, then the agent&#8217;s beliefs are rationally defective.<br />
	Dutch Book Theorem: If an agent&#8217;s degrees of belief violate the probability axioms, then there is a set of monetary bets, at odds matching those degrees of belief, that will logically guarantee the agent&#8217;s monetary loss.</p>
<p>So, the idea of the argument is this.  First, restrict discuss to simple agents, so that money and a linear ordering on it is all we need to talk about to measure the agent&#8217;s preferences.  Fair bets for such an agent are ones that match the agent&#8217;s degree of belief.  So, to have rational degrees of belief, a simple agent&#8217;s degrees of belief have to match his attitudes about fair bets.  Unfair bets favor the book or favor the agent.  One class of bets that favors the book are those that are logically guaranteed to favor the bookie.  These we say are rationally defective.</p>
<p>The trick of the argument is to connect this Bet Defectiveness principle with the principle that follows it, Belief Defectiveness.  Here is the way Christensen argues.  He first notes that Belief Defectiveness doesn&#8217;t follow from Bet Defectiveness.  He considers cases of what I will call &#8220;messy preferences&#8221; to show that the second doesn&#8217;t follow from the first.  In such cases, the value of a payoff for one bet may affect the value of the payoff for the next bet: I&#8217;ll value roast duck more, he says, if I don&#8217;t yet have one than if I do.  But, of course, this possibility is ruled out by noting the qualification in the principles in question that the agents in question are assumed to be simple agents.  So, the claim is, without messy preferences, Belief Defectiveness follows from Bet Defectiveness, since without value interference, bets that are individually acceptable will also be acceptable in combination.  Once we get to Belief Defectiveness, the rest is just math!  Add in the Dutch Book Theorem and you get Simple Agent Probabilism.<br />
This argument assumes that messy preferences are the only things that can block the move from defects in bets to defects in beliefs, but they clearly are not.  Here is where holism about rationality conflicts with probabilistic coherence requirements.  Christensen claims that the bet described in the last paragraph is one that, given your degree of belief you should regard as fair.  We should ask, however, whether this is a prima facie or ultima facie &#8220;should&#8221;.  As an ultima facie claim, it won&#8217;t be true unless messy preferences are all that need to be controlled for to line up attitudes about fair bets with degrees of belief.</p>
<p>The point to note, however, is this:  it is not only preferences that can be messy, but beliefs as well.  Suppose I am a strongly puritanical Calvinist, believing wholeheartedly in the work ethic of such a view, and holding that divine anger will be visited on any who try to benefit in ways outside of honest toil.  My degree of belief in p might be 2/3, but I won&#8217;t regard the bet in question as fair; in fact, I&#8217;ll regard every bet offered as an unfair bet.  I will think any bet of any sort will work to my disadvantage.  Or, again, suppose I&#8217;m convinced that bets are quite often finkish: that accepting them triggers a change in the prospects of winning.  So, no matter what bet we&#8217;re considering, I can&#8217;t regard any bet as a fair bet, because there aren&#8217;t any such bets:  fairness of a bet, I also believe, requires that no party to the bet will have a special advantage over the other party, and that viewpoint is one we can&#8217;t ever accept.  Or, once more, suppose I can&#8217;t tell what my degree of belief is and I have no idea on reflection what the chances of p are.  You ask me whether a bet on p that costs $2 and pays $3 if p is true is a fair bet.  I have no idea.  Perhaps I&#8217;ll reflect further and come to some decision in some way or other, but my reflection may lead to either answer compatible with my inaccessible degree of belief being 2/3.</p>
<p>In each of these cases, nothing said contravenes the assumption that the agent in question is a simple agent.  What is messy here are not the preferences of the agent, but rather the beliefs of the agent.  Moreover, in each case, there is no reason to view the additional features of the agent&#8217;s perspective on the world to be out of order or inadmissible or illegitimate in a way that would allow one to ignore such complexities.  In each case, the point of the example is to show that a proper understanding of the agent&#8217;s perspective on the world requires more, and perhaps less, than an account of some particular degree of belief they have.  Rationality is perspectival in this sense, and it is a mistake to think that the degree of belief in question is all that needs to be noted in characterizing the perspective in question.</p>
<p>Once these points are noted, we can only accept a prima facie version of the Sanctioning principle:<br />
	PFS (Prima Facie Sanctioning): A simple agent&#8217;s degrees of belief prima facie sanction as fair, i.e. give one a defeasible reason for regarding as fair, monetary bets at odds matching his degrees of belief.<br />
PFS, however, is not strong enough to sustain Simple Agent Probabilism, except when the defect of suffering from probabilistic incoherence is itself taken only to be a prima facie defect.</p>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/christensen%e2%80%99s-de-pragmatized-dutch-book-argument-2/feed/</wfw:commentRss>
		<slash:comments>13</slash:comments>
		</item>
		<item>
		<title>Is the Problem of Grue the Problem of Old Evidence?</title>
		<link>http://certaindoubts.com/is-the-problem-of-grue-the-problem-of-old-evidence/</link>
		<comments>http://certaindoubts.com/is-the-problem-of-grue-the-problem-of-old-evidence/#comments</comments>
		<pubDate>Thu, 22 Feb 2007 13:12:01 +0000</pubDate>
		<dc:creator><![CDATA[weiner]]></dc:creator>
				<category><![CDATA[confirmation theory]]></category>
		<category><![CDATA[epistemic paradoxes]]></category>
		<category><![CDATA[formal epistemology]]></category>
		<category><![CDATA[general]]></category>
		<category><![CDATA[justification]]></category>

		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=662</guid>
		<description><![CDATA[via Jon&#8217;s post below and Gillian Russell at TAR, Greg Restall summarizes and links to Branden Fitelson&#8217;s very interesting talk (pdf of Branden&#8217;s slides) at the Banff Mathematical Methods in Philosophy workshop. I&#8217;ve only seen the slides, but I have &#8230; <a class="more-link" href="http://certaindoubts.com/is-the-problem-of-grue-the-problem-of-old-evidence/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p>via <a href="http://fleetwood.baylor.edu/certain_doubts/?p=661">Jon&#8217;s post below</a> and <a href="http://tar.weatherson.org/2007/02/19/the-lark-of-a-definite-precisely-formulated-formal-system/">Gillian Russell at TAR</a>, <a href="http://consequently.org/news/2007/02/20/in_banff_branden_fitelson_on_formal_epistemology/">Greg Restall</a> summarizes and links to Branden Fitelson&#8217;s very interesting talk (<a href="http://fitelson.org/banff_handout.pdf">pdf of Branden&#8217;s slides</a>) at the Banff Mathematical Methods in Philosophy workshop. I&#8217;ve only seen the slides, but I have some questions about Branden&#8217;s argument; I hope Branden or some Banffer will be able to say more about how it works.<br />
<span id="more-662"></span><br />
Branden starts by discussing the new developments in formal epistemology that Jon mentioned below. But his main argument is an analogy between the problem of &#8216;grue&#8217; and, on the one hand, the relevantist critique of classical logic, and on the other, the problem of old evidence.</p>
<p>Branden quickly summarizes the relevantist critique thus:</p>
<blockquote><p>(1) On classical logic, if a set X of beliefs is inconsistent, then any p is a consequence of X.<br />
(2) If your total set of beliefs entails p (and you know this), then you are justified in believing p.<br />
(3) But even if you know that your beliefs are inconsistent, there are some things you aren&#8217;t justified in believing.<br />
Conclusion: We should reject classical logic, on which an inconsistent set entails anything.</p></blockquote>
<p>Branden points out that Harman acknowledges the inconsistency but rejects the bridge principle (2) linking entailment and inference. We can preserve the classical entailment relation by denying that we are entitled to infer whatever our beliefs entailed. When our beliefs are inconsistent the proper response is often to reject one of our beliefs rather than to infer all their consequences.</p>
<p>Branden argues that the grue argument against a Carnapian conception of confirmation uses the Requirement of Total Evidence as a similar bridge principle. Carnap sees confirmation as an <i>a priori</i> logical relation between sentences. But this relation alone doesn&#8217;t tell us how to apply itself in forming new beliefs. That requires the bridge principle of the Requirement of Total Evidence:</p>
<blockquote><p>(RTE) E evidentially supports H for S in C iff E confirms H relative to K, where K is S&#8217;s total evidence in C. </p></blockquote>
<p>(E confirms H relative to K means Pr(H|E&#038;K) &gt; Pr(H|K), for a suitable probability function Pr; Carnap thought &#8216;suitable&#8217; meant &#8216;logical&#8217;, but Branden points out that the grue argument he canvasses doesn&#8217;t depend on which probability function is used.) The grue argument&mdash;if I&#8217;m reading Branden aright&mdash;works because we admit &#8216;the emerald is observed before time t&#8217; into the total background evidence, and once we do this our new evidence confirms &#8220;This emerald is green&#8221; and thus &#8220;All emeralds are green&#8221; just in case it confirms &#8220;This emerald is grue&#8221; and thus &#8220;All emeralds are grue.&#8221; [See step (iii) in the lower right hand slide on Branden&#8217;s p. 3.]</p>
<p>But, as Branden points out, (RTE) has to be rejected anyway because of the Problem of Old Evidence, that hypotheses can be evidentially supported by evidence that is already in K. Bayesians and Carnapians are much better off defining evidential support in terms of confirmation relative to an empty set of background knowledge.</p>
<p>Branden concludes, &#8220;So, many Bayesians <i>already</i> reject (RTE). They shouldn&#8217;t be <i>too</i> worried about &#8220;Grue&#8221;. It&#8217;s a new twist on &#8220;Old Evidence&#8221;.</p>
<p>Here&#8217;s my main question: If &#8220;grue&#8221; is just a new twist on old evidence, then presumably the solution to old evidence is meant to work for grue. That solution is to replace (RTE) with the idea that evidential support depends on confirmation relative to an empty background. But I don&#8217;t see how that helps with the grue problem.</p>
<p>Consider the choice between two hypotheses, that all emeralds are green and that all emeralds are grue with respect to Jan. 1, 2150. Against an empty background, does the observation of a green emerald now support one hypothesis more than the other? I don&#8217;t see how; and answering the question why it should just is solving the problem of grue, it seems to me. Similarly, we&#8217;d need to solve the problem of grue to  explain why, on an empty background, we might assign a higher probability to &#8220;All emeralds are green&#8221; than to &#8220;All emeralds are grue<sub>2150</sub>.&#8221; If we can&#8217;t do either of those things, the move from (RTE) to confirmation against an empty background just doesn&#8217;t seem to give us any progress in explaining why the observation of a green emerald now supports &#8220;All emeralds are green&#8221; rather than &#8220;All emeralds are grue.&#8221; Any thoughts?</p>
<p>[I have another question about a more minor point of Branden&#8217;s presentation. In the lower left slide on p. 3, he presents a counterexample to the hypothesis that the examination of a green emerald before t confirms &#8220;All emeralds are green&#8221; iff it confirms &#8220;All emeralds are grue.&#8221; This is based on I.J. Gold&#8217;s counterexample to the idea that a black raven always confirms &#8220;All ravens are black&#8221;; in Branden&#8217;s example are background knowledge is that either there are 1000 green emeralds 900 of which have been examined before t, no non-green emeralds, and 1 million other things, or there are 100 green emeralds all of which have been examined before time t, 900 non-green emeralds that have not been examined before time t, and 1 million other things. Then, if you randomly select an object and it turns out to be an examined green emerald, against this background knowledge this confirms that all emeralds are green but not that all emeralds are grue. My question is: Can we set up the background knowledge this way? Shouldn&#8217;t the observations that have already been carried out be part of the background knowledge? This problem, however, could be easily taken care of by changing the second hypothesis so that there are nine times as many of each kind of emerald, and in both cases we&#8217;ve examined 900 green emeralds; we might also have to add an unexamined green emerald to the second case, because on my view we should see the next examined object as drawn from the ones that haven&#8217;t been examined already.]</p>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/is-the-problem-of-grue-the-problem-of-old-evidence/feed/</wfw:commentRss>
		<slash:comments>19</slash:comments>
		</item>
		<item>
		<title>Pollock on Probable Probabilities</title>
		<link>http://certaindoubts.com/pollock-on-probable-probabilities/</link>
		<comments>http://certaindoubts.com/pollock-on-probable-probabilities/#comments</comments>
		<pubDate>Thu, 24 Aug 2006 15:49:59 +0000</pubDate>
		<dc:creator><![CDATA[Kvanvig Jon]]></dc:creator>
				<category><![CDATA[confirmation theory]]></category>
		<category><![CDATA[formal epistemology]]></category>

		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=612</guid>
		<description><![CDATA[John just sent me a really neat paper entitled &#8220;Probable Probabilities.&#8221; The abstract should entice reading the full manuscript: In concrete applications of probability, statistical investigation gives us knowledge of some probabilities, but we generally want to know many others &#8230; <a class="more-link" href="http://certaindoubts.com/pollock-on-probable-probabilities/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p>John just sent me a really neat paper entitled &#8220;Probable Probabilities.&#8221;  The abstract should entice reading the full manuscript:</p>
<blockquote><p>In concrete applications of probability, statistical investigation gives us knowledge of some probabilities, but we generally want to know many others that are not directly revealed by our data. For instance, we may know prob(P|Q) (the probability of P given Q) and prob(P|R), but what we really want is prob(P|Q&#038;R), and we may not have the data required to assess that directly. The probability calculus is of no help here. Given prob(P|Q) and prob(P|R), it is consistent with the probability calculus for prob(P|Q&#038;R) to have any value between 0 and 1. Is there any way to make a reasonable estimate of the value of prob(P|Q&#038;R)?<br />
A related problem occurs when probability practitioners adopt undefended assumptions of statistical independence simply on the basis of not seeing any connection between two propositions. This is common practice, but its justification has eluded probability theorists, and researchers are typically apologetic about  making such assumptions. Is there any way to defend the practice?<br />
This paper shows that on a certain conception of probability — nomic probability — there are principles of &#8220;probable probabilities&#8221; that license inferences of the above sort.  These are principles telling us that although certain inferences from probabilities to probabilities are not deductively valid, nevertheless the second-order probability of their yielding correct results is 1. This makes it defeasibly reasonable to make the inferences. Thus I argue that it is defeasibly reasonable to assume statistical independence when we have no information to the contrary. And I show that there is a function Y(r,s,a) such that if<br />
prob(P|Q) = r, prob(P|R) = s, and prob(P|U) = a (where U is our background knowledge) then it is defeasibly reasonable to expect that prob(P|Q&#038;R) = Y(r,s,a). Numerous other defeasible inferences are licensed by similar principles of probable probabilities. This has the potential to greatly enhance the usefulness of probabilities in practical application.</p></blockquote>
<p>The full manuscript can be accessed at <a href="http://oscarhome.soc-sci.arizona.edu/ftp/PAPERS/probable%20probabilities-simple.pdf">http://oscarhome.soc-sci.arizona.edu/ftp/PAPERS/probable%20probabilities-simple.pdf </a>.  As always, discussion encouraged.</p>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/pollock-on-probable-probabilities/feed/</wfw:commentRss>
		<slash:comments>5</slash:comments>
		</item>
		<item>
		<title>A Symmetry between Defeat and Support?</title>
		<link>http://certaindoubts.com/a-symmetry-between-defeat-and-support/</link>
		<comments>http://certaindoubts.com/a-symmetry-between-defeat-and-support/#comments</comments>
		<pubDate>Mon, 24 Jul 2006 14:11:22 +0000</pubDate>
		<dc:creator><![CDATA[Kvanvig Jon]]></dc:creator>
				<category><![CDATA[confirmation theory]]></category>
		<category><![CDATA[justification]]></category>

		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=603</guid>
		<description><![CDATA[One of the lessons of Plantinga&#8217;s argument against evolutionary naturalism is that the mere fact that a claim is improbable on a certain piece of information doesn&#8217;t imply that the latter information is a defeater of any evidence in favor &#8230; <a class="more-link" href="http://certaindoubts.com/a-symmetry-between-defeat-and-support/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p>One of the lessons of Plantinga&#8217;s argument against evolutionary naturalism is that the mere fact that a claim is improbable on a certain piece of information doesn&#8217;t imply that the latter information is a defeater of any evidence in favor of the claim in question.  In the context of his argument, this point plays out as follows.  Even if the claim that our faculties are reliable is improbable given only the assumption of evolutionary naturalism doesn&#8217;t imply that these assumptions defeat any and every defense of the reliability of our cognitive faculties.</p>
<p>The debate about Plantinga&#8217;s argument thus turned to the interesting question of how to determine when such a conditional improbability would count as a defeater and when it wouldn&#8217;t.  There&#8217;s some interesting literature on that question, but I&#8217;m more interested in the  analogy on the other side:  if conditional improbability doesn&#8217;t signal defeat, then conditional probability may not signal support, either.</p>
<p><span id="more-603"></span>Here&#8217;s what I mean.  Consider the following detachment rule:<br />
DR:  if Pr(p) exceeds some threshold X (less than 1), then one is epistemically justified in believing p.</p>
<p>The idea behind DR is as follows.  You learn some information, and by some complicated rule, (perhaps Bayesian conditionalization, perhaps something else), that information teaches you that the probability of p is high (meets the threshold in question but is less than 1).  What should you believe?  Well, for one thing, it&#8217;s surely OK to believe that p is likely to be true.  But what of p itself?  If DR is correct, then in such a case one can detach the probability operator and believe p as well.</p>
<p>I expect this proposal to fail, for much the same reasons that negative probabilistic relevance fails to imply that something is a defeater.  That is, I expect positive probabilistic relevance sometimes tracks evidential support and sometimes not.  I&#8217;m working out details on specific cases, which I&#8217;ll post a bit later, but wondered if others have been thinking about this much.</p>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/a-symmetry-between-defeat-and-support/feed/</wfw:commentRss>
		<slash:comments>12</slash:comments>
		</item>
		<item>
		<title>The Principal Principle</title>
		<link>http://certaindoubts.com/the-principal-principle/</link>
		<comments>http://certaindoubts.com/the-principal-principle/#comments</comments>
		<pubDate>Mon, 29 May 2006 12:31:31 +0000</pubDate>
		<dc:creator><![CDATA[Kvanvig Jon]]></dc:creator>
				<category><![CDATA[confirmation theory]]></category>
		<category><![CDATA[justification]]></category>

		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=556</guid>
		<description><![CDATA[David Lewis thought the following is true: The Principal Principle (PP): Ps(A&#124;Po(A)=x)=x (where Ps is a rational subjective probability, and Po is some objective probability). Here&#8217;s a gloss of this claim, more or less accurate: if you know that the &#8230; <a class="more-link" href="http://certaindoubts.com/the-principal-principle/">Continue reading <span class="meta-nav">&#8594;</span></a>]]></description>
				<content:encoded><![CDATA[<p>David Lewis thought the following is true:<br />
The Principal Principle (PP):  Ps(A|Po(A)=x)=x (where Ps is a rational subjective probability, and Po is some objective probability).</p>
<p>Here&#8217;s a gloss of this claim, more or less accurate:  if you know that the objective probability of A is x, then you should assign degree of belief x to A given that information.</p>
<p>So, first, a confession:  I know there&#8217;s been considerable discussion of this principle in the literature but I haven&#8217;t read as much of it as I should.  So there may easy answers here to the things that concern me, and pointing them out in the comments would be useful.</p>
<p>Second, my problem:   I think, to go straight to the bottom line, that I may not know how to interpret conditional probability claims.</p>
<p><span id="more-556"></span>Suppose I think of a conditional probability as implicitly referring to some background knowledge in the given condition, so that PP implicitly quantifies over every cognitive agent and their systems of information.  Then the principle is false, regardless of what kind of probability is included here, since the background information might conflict with the stated condition.</p>
<p>One alternative here is to abstract from one&#8217;s actual system of information, ridding it of everything epistemically relevant to the truth or falsity of A before adding (just) the information contained in the condition.  Then PP says something like this:  if the only relevant information you had about A was that its objective probability is x, then your subjective probability for A should be x as well.</p>
<p>I&#8217;ve never been comfortable with abstractionist interpretations of conditional probability claims.  What is the conditional probability that you exist given that I tell you so?  (This is sloppy:  I don&#8217;t want the report of testimony to guarantee your existence but just to be a sincere and honest assertion of it on my part.)  The abstractionist interpretation requires you to consider a situation in which your only information about your existence is my testimony.  I don&#8217;t see how such a situation is possible, or conceivable, or imaginable.</p>
<p>Perhaps one should put the claim in form of a conditional, as my gloss of PP above did, where I reported the principle as claiming that if one knows that the objective probability of a claim is x, then one&#8217;s subjective probability should be x as well.  But that won&#8217;t work with probabilities that conditionalize on one&#8217;s own non-existence.  What is the conditional probability of anything given that you do not exist?  If we understanding probability in the subjectivist sense, this makes little sense.  It&#8217;s hard to imagine your knowing of your non-existence.  Of course, this might be an argument for a different understanding of probability, but we can&#8217;t take that route for the principle above, since it explicitly appeals to subjective probability.</p>
<p>So, assume that the condition in the principle is that the objective probability of my existence is zero.  Note that this supposition is compatible with my existence, so we can&#8217;t rely on such a claim to generate an answer here concerning the application of the Principal Principle to such a case.  What should my subjective opinion be about my existence?</p>
<p>Is there something I&#8217;m missing here?  Maybe so, but if not, here&#8217;s what I think the idea is that makes the principle look plausible.  It&#8217;s a claim about evidence or confirmation.  It makes a claim about prima facie evidential support, to the effect that if you learn the objective probability of a claim, that is evidence that the total evidence supports the claim to the same degree.   That allows one to say that the degree of confirmation provided for p by the evidence that the objective probability of p is x, is precisely the same (even though all such evidential claims are only prima facie).</p>
]]></content:encoded>
			<wfw:commentRss>http://certaindoubts.com/the-principal-principle/feed/</wfw:commentRss>
		<slash:comments>11</slash:comments>
		</item>
	</channel>
</rss>
