<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	>
<channel>
	<title>Comments on: Huemer and Foley on Swampman</title>
	<atom:link href="http://certaindoubts.com/huemer-and-foley-on-swampman/feed/" rel="self" type="application/rss+xml" />
	<link>http://certaindoubts.com/huemer-and-foley-on-swampman/</link>
	<description>devoted to matters epistemic</description>
	<lastBuildDate>Wed, 10 Apr 2019 16:37:28 +0000</lastBuildDate>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.10</generator>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/huemer-and-foley-on-swampman/#comment-3414</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Wed, 29 Mar 2006 15:16:17 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=553#comment-3414</guid>
		<description><![CDATA[(i) Don&#039;t the conditions for evaluating the conditional bet  depend crucially on what a swampman is? If a swampman is a thing with these epistemic capabilities, then he would be reasonable to fully believe that the next swampman would be a swampman. Yes. Now, betting one&#039;s life on tautologies presupposes a particular model of full belief that we needn&#039;t adopt. One could also tweak the details of the example to make it more reasonable for the swampman to suspend judgment and not take the bet, a state that I suspect you&#039;d have a hard time representing in your model.

(ii) The bit on frequentist theories is off the mark. Recall that there are also logical theories that appeal to known statistical frequencies. Here Matt and I might come apart...which is why I wanted to pin down more information about what a Swampman is, and what evidence is available to it, or us about it. This will go a very long way to settling the issue, but your model might get banged up in the process. But anyway, Matt&#039;s right: the point can be made much more generally. So, maybe we can stay together for a while.

To get the result that swampman is being unreasonable, you have to put him in a class where it is practically impossible to gain any epistemic advantage by lighting strike, like the class of human beings. There is no reason to put Swampman in this class. He was created by a lighting strike; He belongs to the class of Swampmen, which contains only himself.

What is known about this class? A lot. He himself has all and only true beliefs. And if he is able to interact with us, I suspect he would be able to confirm that he has true beliefs. (Here again, details of the example matter.) If so, then that&#039;s the end of the story. You don&#039;t turn to statistical reduction when you have the answer already in hand.]]></description>
		<content:encoded><![CDATA[<p>(i) Don&#8217;t the conditions for evaluating the conditional bet  depend crucially on what a swampman is? If a swampman is a thing with these epistemic capabilities, then he would be reasonable to fully believe that the next swampman would be a swampman. Yes. Now, betting one&#8217;s life on tautologies presupposes a particular model of full belief that we needn&#8217;t adopt. One could also tweak the details of the example to make it more reasonable for the swampman to suspend judgment and not take the bet, a state that I suspect you&#8217;d have a hard time representing in your model.</p>
<p>(ii) The bit on frequentist theories is off the mark. Recall that there are also logical theories that appeal to known statistical frequencies. Here Matt and I might come apart&#8230;which is why I wanted to pin down more information about what a Swampman is, and what evidence is available to it, or us about it. This will go a very long way to settling the issue, but your model might get banged up in the process. But anyway, Matt&#8217;s right: the point can be made much more generally. So, maybe we can stay together for a while.</p>
<p>To get the result that swampman is being unreasonable, you have to put him in a class where it is practically impossible to gain any epistemic advantage by lighting strike, like the class of human beings. There is no reason to put Swampman in this class. He was created by a lighting strike; He belongs to the class of Swampmen, which contains only himself.</p>
<p>What is known about this class? A lot. He himself has all and only true beliefs. And if he is able to interact with us, I suspect he would be able to confirm that he has true beliefs. (Here again, details of the example matter.) If so, then that&#8217;s the end of the story. You don&#8217;t turn to statistical reduction when you have the answer already in hand.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Michael Huemer</title>
		<link>http://certaindoubts.com/huemer-and-foley-on-swampman/#comment-3423</link>
		<dc:creator><![CDATA[Michael Huemer]]></dc:creator>
		<pubDate>Wed, 29 Mar 2006 00:37:11 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=553#comment-3423</guid>
		<description><![CDATA[A brief continuation on F-Swampman. Suppose that you offered Fs a conditional bet: &quot;So, Fs, if another lightning strike occurs creating another swampman, what odds would you give that he will also have all true beliefs, just like you?&quot; Will Fs say, &quot;Why, I&#039;d bet in favor of that proposition at &lt;i&gt;any&lt;/i&gt; odds!&quot;? Would Fs cheerfully bet his life for a dime on that proposition?

If yes, then I think he&#039;s irrational. If no, then he doesn&#039;t think the probability of a swampman getting all true beliefs is 1.

Greg and Matt, I think your mistake is deploying an overly simple version of the frequency interpretation of probability. Even frequentists don&#039;t say that the probability of an event occurring in a given type of circumstance equals the &lt;i&gt;actual&lt;/i&gt; frequency with which it has occurred, in a small number of trials. I believe the standard version of frequency theory is that the probability of E occurring in some circumstances equals the limit of the proportion of times E would occur in an unlimited sequence of trials.]]></description>
		<content:encoded><![CDATA[<p>A brief continuation on F-Swampman. Suppose that you offered Fs a conditional bet: &#8220;So, Fs, if another lightning strike occurs creating another swampman, what odds would you give that he will also have all true beliefs, just like you?&#8221; Will Fs say, &#8220;Why, I&#8217;d bet in favor of that proposition at <i>any</i> odds!&#8221;? Would Fs cheerfully bet his life for a dime on that proposition?</p>
<p>If yes, then I think he&#8217;s irrational. If no, then he doesn&#8217;t think the probability of a swampman getting all true beliefs is 1.</p>
<p>Greg and Matt, I think your mistake is deploying an overly simple version of the frequency interpretation of probability. Even frequentists don&#8217;t say that the probability of an event occurring in a given type of circumstance equals the <i>actual</i> frequency with which it has occurred, in a small number of trials. I believe the standard version of frequency theory is that the probability of E occurring in some circumstances equals the limit of the proportion of times E would occur in an unlimited sequence of trials.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/huemer-and-foley-on-swampman/#comment-3422</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Wed, 22 Mar 2006 17:04:23 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=553#comment-3422</guid>
		<description><![CDATA[Fs would have low probability if it is reasonable to assume that Fs is in the reference class of human beings w.r.t. belief formation.

Fs would not have a low probability if Fs is in the reference class of swampmen, which I assume is the singleton set, {Fs}.

The reply then is that your claim about the method holds for humans, not swampmen. And because therefore it is sensitive to the selection of a reference class, the generality of the proposal is suspect.]]></description>
		<content:encoded><![CDATA[<p>Fs would have low probability if it is reasonable to assume that Fs is in the reference class of human beings w.r.t. belief formation.</p>
<p>Fs would not have a low probability if Fs is in the reference class of swampmen, which I assume is the singleton set, {Fs}.</p>
<p>The reply then is that your claim about the method holds for humans, not swampmen. And because therefore it is sensitive to the selection of a reference class, the generality of the proposal is suspect.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Matt</title>
		<link>http://certaindoubts.com/huemer-and-foley-on-swampman/#comment-3421</link>
		<dc:creator><![CDATA[Matt]]></dc:creator>
		<pubDate>Tue, 21 Mar 2006 23:55:05 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=553#comment-3421</guid>
		<description><![CDATA[Greg has complexified it for us, in a not unimportant way; but I want to reply to Mike.

Clearly, the method by which Fs gained his beliefs was successful: he knows how he gained them, and he knows they are all true.  And this is why I distinguished between (1) and (2) in my post.  It seems to me that Fs knows two facts here, one of which has to do with himself, my earlier (2) (or something like it). And giving it a probability of 1 just has to do with this: the emergence of an Fs-like creature from a lightning-struck swamp happened once (presumably), and in Fs&#039;s case, it worked epistemic wonders.  So out of 1 chance there was 1 successful case; that&#039;s all I mean by probability being 1.

Mike may dispute (2)&#039;s use of probability, but why shouldn&#039;t probability be indexed to persons (particularly when we&#039;re dealing with a phenom like Fs)?

Or maybe a better approach would be this.  Fs knows that the *prior* probability of coming to have all true beliefs in this way is VERY low.  But Fs also knows that in his case, it worked out, however remarkable that is.  What would be incoherent about Fs knowing those two things?]]></description>
		<content:encoded><![CDATA[<p>Greg has complexified it for us, in a not unimportant way; but I want to reply to Mike.</p>
<p>Clearly, the method by which Fs gained his beliefs was successful: he knows how he gained them, and he knows they are all true.  And this is why I distinguished between (1) and (2) in my post.  It seems to me that Fs knows two facts here, one of which has to do with himself, my earlier (2) (or something like it). And giving it a probability of 1 just has to do with this: the emergence of an Fs-like creature from a lightning-struck swamp happened once (presumably), and in Fs&#8217;s case, it worked epistemic wonders.  So out of 1 chance there was 1 successful case; that&#8217;s all I mean by probability being 1.</p>
<p>Mike may dispute (2)&#8217;s use of probability, but why shouldn&#8217;t probability be indexed to persons (particularly when we&#8217;re dealing with a phenom like Fs)?</p>
<p>Or maybe a better approach would be this.  Fs knows that the *prior* probability of coming to have all true beliefs in this way is VERY low.  But Fs also knows that in his case, it worked out, however remarkable that is.  What would be incoherent about Fs knowing those two things?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Michael Huemer</title>
		<link>http://certaindoubts.com/huemer-and-foley-on-swampman/#comment-3420</link>
		<dc:creator><![CDATA[Michael Huemer]]></dc:creator>
		<pubDate>Tue, 21 Mar 2006 22:53:45 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=553#comment-3420</guid>
		<description><![CDATA[Matt,

Why would Fs hold that the probability of his having all true beliefs is 1? Fs is supposed to have only true beliefs, and I don&#039;t think that&#039;s true.

My claim was that the probability of getting all true beliefs, given that one forms one&#039;s beliefs by the method Fs used, is very low. Substituting different proper names into that in place of &quot;one&quot; doesn&#039;t make any difference to the probability. Maybe you&#039;re conditionalizing on more information than just the method Fs used. Or maybe you&#039;re wanting to characterize &quot;the method&quot; in some very &quot;thick&quot; way such that the truth of the beliefs is entailed by a description of the method.]]></description>
		<content:encoded><![CDATA[<p>Matt,</p>
<p>Why would Fs hold that the probability of his having all true beliefs is 1? Fs is supposed to have only true beliefs, and I don&#8217;t think that&#8217;s true.</p>
<p>My claim was that the probability of getting all true beliefs, given that one forms one&#8217;s beliefs by the method Fs used, is very low. Substituting different proper names into that in place of &#8220;one&#8221; doesn&#8217;t make any difference to the probability. Maybe you&#8217;re conditionalizing on more information than just the method Fs used. Or maybe you&#8217;re wanting to characterize &#8220;the method&#8221; in some very &#8220;thick&#8221; way such that the truth of the beliefs is entailed by a description of the method.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/huemer-and-foley-on-swampman/#comment-3419</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Tue, 21 Mar 2006 15:03:04 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=553#comment-3419</guid>
		<description><![CDATA[This was the worry, Matt. I was eager to complain about how evidential relations are handled in Bayesian frameworks and got the sequence of issues wrong for this example. Maybe these issues will come up later when we get an account for why Fs thinks this way.

There is another piece to this, namely specifying what a swampman is. Here are some candidates:

(i) Swamp-phonebook: this is an inert collection of &#039;beliefs&#039; about facts. Structurally, it is a set of statements. And it doesn&#039;t really do anything. This would be an &lt;i&gt;non&lt;/i&gt;rational creature; ascribing rationality would be a category mistake.

(ii) Talking Swamp-phonebook: this is a swamp-phonebook with a speaker. We assume that the facts it broadcasts are selected stochastically. Also an nonrational creature.

(iii) Swamp data-hub: Imagine all of the truths of oracles of various mythologies passing through swamp data-hub on their way to answering people&#039;s questions posed to various oracles. This is more interesting than swamp phonebook; there is non-stochastic structure here. If we were a large government agency, we might like to spy on this data hub to learn about what those people were asking. Still, Swamp data-hub is not the kind of thing we&#039;d ascribe rationality to.

(iv) Swamp Savant: Has an IQ of 25 and lots of true beliefs but has little idea what to do with them. On one reading, this seems like what people imagine Fs to be. A case can be made for swamp savant being the kind of thing that could be rational. It is also the kind of thing that is arguably irrational. I doubt that it is Huemer-irrational, however.

(v) Swamp Press Secretary: Has an IQ of 250 and is sought after by government Vice Presidents (or former Presidents, if your politics run this way). This thing has all and only truths, tells all and only truths, and can systematically mislead anyone by telling all and only truths. He is hyper-rational. Huemer might score him irrational, however.

(vi) Ordinary Swampan: This is a human-like thing in a swamp who is imagined to be cognitively like us except, in so far as this is at all possible, he&#039;s right about all of the things he believes and he believes a lot of non-trivial things. Questions: Does he learn new things? Can he understand us when we ask him things? If so, can he give *correct* answers to our questions? (If so, then he has some grasp of right and wrong answers, and so some rough idea of true and false even though he himself only believes true things.) Can we count on him telling us the truth? Can we count on him not misleading us with the truth? (What does he eat? Plants? Humans?)

I would caution against waving away all these questions as irrelevant, even those that appear to be jokes. There is a lot of information packed into the answers to these questions that impacts the selection of the model.]]></description>
		<content:encoded><![CDATA[<p>This was the worry, Matt. I was eager to complain about how evidential relations are handled in Bayesian frameworks and got the sequence of issues wrong for this example. Maybe these issues will come up later when we get an account for why Fs thinks this way.</p>
<p>There is another piece to this, namely specifying what a swampman is. Here are some candidates:</p>
<p>(i) Swamp-phonebook: this is an inert collection of &#8216;beliefs&#8217; about facts. Structurally, it is a set of statements. And it doesn&#8217;t really do anything. This would be an <i>non</i>rational creature; ascribing rationality would be a category mistake.</p>
<p>(ii) Talking Swamp-phonebook: this is a swamp-phonebook with a speaker. We assume that the facts it broadcasts are selected stochastically. Also an nonrational creature.</p>
<p>(iii) Swamp data-hub: Imagine all of the truths of oracles of various mythologies passing through swamp data-hub on their way to answering people&#8217;s questions posed to various oracles. This is more interesting than swamp phonebook; there is non-stochastic structure here. If we were a large government agency, we might like to spy on this data hub to learn about what those people were asking. Still, Swamp data-hub is not the kind of thing we&#8217;d ascribe rationality to.</p>
<p>(iv) Swamp Savant: Has an IQ of 25 and lots of true beliefs but has little idea what to do with them. On one reading, this seems like what people imagine Fs to be. A case can be made for swamp savant being the kind of thing that could be rational. It is also the kind of thing that is arguably irrational. I doubt that it is Huemer-irrational, however.</p>
<p>(v) Swamp Press Secretary: Has an IQ of 250 and is sought after by government Vice Presidents (or former Presidents, if your politics run this way). This thing has all and only truths, tells all and only truths, and can systematically mislead anyone by telling all and only truths. He is hyper-rational. Huemer might score him irrational, however.</p>
<p>(vi) Ordinary Swampan: This is a human-like thing in a swamp who is imagined to be cognitively like us except, in so far as this is at all possible, he&#8217;s right about all of the things he believes and he believes a lot of non-trivial things. Questions: Does he learn new things? Can he understand us when we ask him things? If so, can he give *correct* answers to our questions? (If so, then he has some grasp of right and wrong answers, and so some rough idea of true and false even though he himself only believes true things.) Can we count on him telling us the truth? Can we count on him not misleading us with the truth? (What does he eat? Plants? Humans?)</p>
<p>I would caution against waving away all these questions as irrelevant, even those that appear to be jokes. There is a lot of information packed into the answers to these questions that impacts the selection of the model.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Matt</title>
		<link>http://certaindoubts.com/huemer-and-foley-on-swampman/#comment-3418</link>
		<dc:creator><![CDATA[Matt]]></dc:creator>
		<pubDate>Tue, 21 Mar 2006 01:16:25 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=553#comment-3418</guid>
		<description><![CDATA[Greg was getting at something earlier which interests me, but it seems to have been bypassed in favor of this discussion of Dan&#039;s experiments (which I skimmed, and so don&#039;t quite follow).  Greg&#039;s point had to do with the oddity of assigning a probablistic argument to Fs beliefs.  Remember what Mike said:

 &quot;Fs holds a set of beliefs about where his first-order beliefs came from, on which it is extremely improbable that his first-order beliefs are true (and he acknowledges that); yet he still holds on to those first-order beliefs.&quot;

But maybe Fs wouldn&#039;t acknowledge this in the way Mike thinks.  For wouldn&#039;t Fs hold both of the following? (1) This is extremely improbable, should this happen to anyone at all, that such a person&#039;s beliefs all turn out true; and (2) In my own case (the case of Fs), it is not at all improbable, for it has a probability of 1.

If this is right, then Fs wouldn&#039;t exhibit the alleged incoherence.  Or am I missing something crucial?]]></description>
		<content:encoded><![CDATA[<p>Greg was getting at something earlier which interests me, but it seems to have been bypassed in favor of this discussion of Dan&#8217;s experiments (which I skimmed, and so don&#8217;t quite follow).  Greg&#8217;s point had to do with the oddity of assigning a probablistic argument to Fs beliefs.  Remember what Mike said:</p>
<p> &#8220;Fs holds a set of beliefs about where his first-order beliefs came from, on which it is extremely improbable that his first-order beliefs are true (and he acknowledges that); yet he still holds on to those first-order beliefs.&#8221;</p>
<p>But maybe Fs wouldn&#8217;t acknowledge this in the way Mike thinks.  For wouldn&#8217;t Fs hold both of the following? (1) This is extremely improbable, should this happen to anyone at all, that such a person&#8217;s beliefs all turn out true; and (2) In my own case (the case of Fs), it is not at all improbable, for it has a probability of 1.</p>
<p>If this is right, then Fs wouldn&#8217;t exhibit the alleged incoherence.  Or am I missing something crucial?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: kvanvig</title>
		<link>http://certaindoubts.com/huemer-and-foley-on-swampman/#comment-3417</link>
		<dc:creator><![CDATA[kvanvig]]></dc:creator>
		<pubDate>Mon, 20 Mar 2006 17:25:45 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=553#comment-3417</guid>
		<description><![CDATA[Mike, I feel a post coming on about Lewis&#039;s Principal Principle!  I&#039;m glad you mentioned it--it will be interesting to see if there is a true principle in the neighborhood here.]]></description>
		<content:encoded><![CDATA[<p>Mike, I feel a post coming on about Lewis&#8217;s Principal Principle!  I&#8217;m glad you mentioned it&#8211;it will be interesting to see if there is a true principle in the neighborhood here.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/huemer-and-foley-on-swampman/#comment-3416</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Mon, 20 Mar 2006 17:00:17 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=553#comment-3416</guid>
		<description><![CDATA[Robin, thanks much for this reference. Cheers, Greg]]></description>
		<content:encoded><![CDATA[<p>Robin, thanks much for this reference. Cheers, Greg</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Michael Huemer</title>
		<link>http://certaindoubts.com/huemer-and-foley-on-swampman/#comment-3415</link>
		<dc:creator><![CDATA[Michael Huemer]]></dc:creator>
		<pubDate>Mon, 20 Mar 2006 16:35:00 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=553#comment-3415</guid>
		<description><![CDATA[Well, if the physical probability of our visual system getting things right is only .25, then it&#039;s a miracle that it keeps getting things right -- and that&#039;s too much of a coincidence to be believed. Recall Lewis&#039; &quot;principal principle&quot;: if you are certain that the objective probability of A is x, then you should assign degree of belief x to A given that information: Ps(A&#124;Po(A)=x)=x (where Ps is a rational subjective probability, and Po is some objective probability).

However, what you say makes me think that it could be that the physical probability of the visual system getting things right is .25 conditional on Dan&#039;s theory, &lt;i&gt;but&lt;/i&gt; it is .99 conditional on Dan&#039;s theory plus some other physical conditions that, as a matter of fact, usually obtain when we look at things. Or in other words: maybe there are some common background facts that Dan didn&#039;t take account of (and perhaps haven&#039;t been discovered) when he calculated the .25.

About mathematical beliefs: you&#039;re right to point out that probabilistic coherence isn&#039;t a rational requirement for all necessary truths, both because a false mathematical claim might have some degree of justification, and because a true mathematical claim might have less than full justification. But notice how nicely my metacoherence requirement deals with that. Say you form a mathematical belief M as a result of an attempted proof. Then your rational degree of belief in M should equal your estimate of the probability of a belief of yours being true given that it was formed by that method. So suppose you think your &#039;proofs&#039; are only 80% reliable. Then, according to metacoherence, you should have an 80% degree of belief in M. That&#039;s true regardless of whether M is actually true.]]></description>
		<content:encoded><![CDATA[<p>Well, if the physical probability of our visual system getting things right is only .25, then it&#8217;s a miracle that it keeps getting things right &#8212; and that&#8217;s too much of a coincidence to be believed. Recall Lewis&#8217; &#8220;principal principle&#8221;: if you are certain that the objective probability of A is x, then you should assign degree of belief x to A given that information: Ps(A|Po(A)=x)=x (where Ps is a rational subjective probability, and Po is some objective probability).</p>
<p>However, what you say makes me think that it could be that the physical probability of the visual system getting things right is .25 conditional on Dan&#8217;s theory, <i>but</i> it is .99 conditional on Dan&#8217;s theory plus some other physical conditions that, as a matter of fact, usually obtain when we look at things. Or in other words: maybe there are some common background facts that Dan didn&#8217;t take account of (and perhaps haven&#8217;t been discovered) when he calculated the .25.</p>
<p>About mathematical beliefs: you&#8217;re right to point out that probabilistic coherence isn&#8217;t a rational requirement for all necessary truths, both because a false mathematical claim might have some degree of justification, and because a true mathematical claim might have less than full justification. But notice how nicely my metacoherence requirement deals with that. Say you form a mathematical belief M as a result of an attempted proof. Then your rational degree of belief in M should equal your estimate of the probability of a belief of yours being true given that it was formed by that method. So suppose you think your &#8216;proofs&#8217; are only 80% reliable. Then, according to metacoherence, you should have an 80% degree of belief in M. That&#8217;s true regardless of whether M is actually true.</p>
]]></content:encoded>
	</item>
</channel>
</rss>
