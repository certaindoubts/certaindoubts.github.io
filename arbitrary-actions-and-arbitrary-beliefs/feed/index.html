<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	
	>
<channel>
	<title>Comments on: Arbitrary Actions and Arbitrary Beliefs</title>
	<atom:link href="http://certaindoubts.com/arbitrary-actions-and-arbitrary-beliefs/feed/" rel="self" type="application/rss+xml" />
	<link>http://certaindoubts.com/arbitrary-actions-and-arbitrary-beliefs/</link>
	<description>devoted to matters epistemic</description>
	<lastBuildDate>Wed, 10 Apr 2019 16:37:28 +0000</lastBuildDate>
	<sy:updatePeriod>hourly</sy:updatePeriod>
	<sy:updateFrequency>1</sy:updateFrequency>
	<generator>https://wordpress.org/?v=4.9.10</generator>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/arbitrary-actions-and-arbitrary-beliefs/#comment-3384</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Thu, 16 Mar 2006 15:32:01 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=487#comment-3384</guid>
		<description><![CDATA[Hi Jon and Jim, There is another error in my last post. It has some consequence. I have been equivocating between two views of rational acceptance, one that finds agreement with Jim in [28, 30] the other which doesn&#039;t and drives [13, 22]. I&#039;ll try to be brief.

Kyburg would take both me and Jim to the woodshed for viewing rational acceptance in the broad terms that we do in [28-30]. It will *not* be true on Kyburg&#039;s conception of rational acceptance that a probability assessment based upon known evidence will necessarily live in one of Jim&#039;s models, nor even in an imprecise probability weakening such as I&#039;ve explored with Absorption. And this is intentional and one of the main causes of critics and Kyburg talking right past one another. By not pointing this out I&#039;m guilty of aiding and abetting.

This rejection of coherence in the Kyburgian program is interesting, particular to those familiar with Peter Walley&#039;s approach, which is driven by an attempt to provide a very general framework for imprecise probability within which the various interpretations can all live as specific add-ons applying to specific kinds of cases, all of which can be specified. That&#039;s the spirit I was following in absorption, and the reason why the debate shifts to syntax and semantics with Jim.

Kyburgian rational acceptance won&#039;t fit even in the Walley framework as it stands (much to my crushing disappointment), which is very, very interesting, because it **seems** to then provide some tools to isolate Kyburg&#039;s deep rejection of coherence constraints for models of assigning probability based upon evidence. This last point is what I had in mind when writing [13, 22].

I shift between these two conceptions sometimes without signally because I am interested in understanding how much of Kyburgian rational acceptance I can get into a (broadly) probabilistic logic framework.  And the answer is hard for me to answer still, because I am still struggling with how to frame the question. It is like the weekend mechanic that has car parts all over the garage floor.

I fretted about letting this blunder of mine pass, but it is a pretty big one conceptually. So, sorry about that.]]></description>
		<content:encoded><![CDATA[<p>Hi Jon and Jim, There is another error in my last post. It has some consequence. I have been equivocating between two views of rational acceptance, one that finds agreement with Jim in [28, 30] the other which doesn&#8217;t and drives [13, 22]. I&#8217;ll try to be brief.</p>
<p>Kyburg would take both me and Jim to the woodshed for viewing rational acceptance in the broad terms that we do in [28-30]. It will *not* be true on Kyburg&#8217;s conception of rational acceptance that a probability assessment based upon known evidence will necessarily live in one of Jim&#8217;s models, nor even in an imprecise probability weakening such as I&#8217;ve explored with Absorption. And this is intentional and one of the main causes of critics and Kyburg talking right past one another. By not pointing this out I&#8217;m guilty of aiding and abetting.</p>
<p>This rejection of coherence in the Kyburgian program is interesting, particular to those familiar with Peter Walley&#8217;s approach, which is driven by an attempt to provide a very general framework for imprecise probability within which the various interpretations can all live as specific add-ons applying to specific kinds of cases, all of which can be specified. That&#8217;s the spirit I was following in absorption, and the reason why the debate shifts to syntax and semantics with Jim.</p>
<p>Kyburgian rational acceptance won&#8217;t fit even in the Walley framework as it stands (much to my crushing disappointment), which is very, very interesting, because it **seems** to then provide some tools to isolate Kyburg&#8217;s deep rejection of coherence constraints for models of assigning probability based upon evidence. This last point is what I had in mind when writing [13, 22].</p>
<p>I shift between these two conceptions sometimes without signally because I am interested in understanding how much of Kyburgian rational acceptance I can get into a (broadly) probabilistic logic framework.  And the answer is hard for me to answer still, because I am still struggling with how to frame the question. It is like the weekend mechanic that has car parts all over the garage floor.</p>
<p>I fretted about letting this blunder of mine pass, but it is a pretty big one conceptually. So, sorry about that.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: kvanvig</title>
		<link>http://certaindoubts.com/arbitrary-actions-and-arbitrary-beliefs/#comment-3386</link>
		<dc:creator><![CDATA[kvanvig]]></dc:creator>
		<pubDate>Wed, 15 Mar 2006 21:53:27 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=487#comment-3386</guid>
		<description><![CDATA[Jim and Greg, very interesting discussion here, and much more fun to watch than participate in, since you guys are the experts here!  Anyway, sorry for the delay--my life has been so hectic that blogging has not been able to find its rightful place!

So I&#039;m not objecting to modeling belief by epistemic probability, at least not here.  What I was objecting two was a view that identifies epistemic rationality with epistemic probability and leaves belief alone.  The problem I pointed out with such an approach is that you can find contents of belief that are obviously rational but inconsistent.

That&#039;s not to say that there are not other more sophisticated approaches to the paradoxes that involve epistemic probability in a central way.  In particular, they won&#039;t leave belief alone, but will instead do as Jim does above.  That&#039;s more sophisticated and more interesting than the view I found problematic above.]]></description>
		<content:encoded><![CDATA[<p>Jim and Greg, very interesting discussion here, and much more fun to watch than participate in, since you guys are the experts here!  Anyway, sorry for the delay&#8211;my life has been so hectic that blogging has not been able to find its rightful place!</p>
<p>So I&#8217;m not objecting to modeling belief by epistemic probability, at least not here.  What I was objecting two was a view that identifies epistemic rationality with epistemic probability and leaves belief alone.  The problem I pointed out with such an approach is that you can find contents of belief that are obviously rational but inconsistent.</p>
<p>That&#8217;s not to say that there are not other more sophisticated approaches to the paradoxes that involve epistemic probability in a central way.  In particular, they won&#8217;t leave belief alone, but will instead do as Jim does above.  That&#8217;s more sophisticated and more interesting than the view I found problematic above.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/arbitrary-actions-and-arbitrary-beliefs/#comment-3385</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Tue, 14 Mar 2006 14:43:05 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=487#comment-3385</guid>
		<description><![CDATA[Hi Jim, yes this is the general idea. And maybe where we might begin to disagree is the nature (and importance) of the kind the problems that arise from attempting to do this. I see properties that we&#039;d like our syntactic language for rational belief to enjoy pulling us in one direction, and the model theory pulling us in another direction. (Which isn&#039;t unusual to have happen in logics). So, being an applied logic problem, arguments over how to effect a logic for rational belief will (or should) take the form of an optimization problem...namely, identify what properties you think are essential for the syntax to enjoy and then explain why it is worth giving up some precision in the semantics to secure these properties. That&#039;s the picture of the debate that I have in mind.]]></description>
		<content:encoded><![CDATA[<p>Hi Jim, yes this is the general idea. And maybe where we might begin to disagree is the nature (and importance) of the kind the problems that arise from attempting to do this. I see properties that we&#8217;d like our syntactic language for rational belief to enjoy pulling us in one direction, and the model theory pulling us in another direction. (Which isn&#8217;t unusual to have happen in logics). So, being an applied logic problem, arguments over how to effect a logic for rational belief will (or should) take the form of an optimization problem&#8230;namely, identify what properties you think are essential for the syntax to enjoy and then explain why it is worth giving up some precision in the semantics to secure these properties. That&#8217;s the picture of the debate that I have in mind.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Jim Hawthorne</title>
		<link>http://certaindoubts.com/arbitrary-actions-and-arbitrary-beliefs/#comment-3383</link>
		<dc:creator><![CDATA[Jim Hawthorne]]></dc:creator>
		<pubDate>Mon, 13 Mar 2006 17:48:12 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=487#comment-3383</guid>
		<description><![CDATA[Greg,

Is this the idea?

Suppose we represent rational acceptance, i.e. belief, by a predicate Bel, so that Bel[A] say that the agent believes that A. In formalizing this, one might take Bel to be a metalinguistic predicate, rather like &quot;truth under an interpretation&quot; is metalinguistic, or like epistemic probability, which is usually treated formally as a metalinguistic function on object language sentences (e.g. we write &#039;if P[A&#038;B] &#8805; P[C], then P[A] &#8805; P[C]&#039;
rather than &#039;P[A&#038;B] &#8805; P[C] &#8835; P[A] &#8805; P[C]&#039; because &#039;&#8835;&#039; is part of the object language, but we usually treat &#039;P&#039; and &#039;&#8805;&#039; and the &#039;if, then&#039; in this context metalinguistically).

So, given a metalinguist treatment of Bel, suppose we use a threshold model for belief:
we model Bel by a probability function P and a threshold t, so that, Bel[A] (for the agent) just when P[A]&#8805; t. Now, on such a model, we can treat the lottery and the preface quite plausibly. Provided that n+1 &#8805; t, the agent can have: Bel[A1] and ... and Bel[An] and Bel[-A1 v ... v -An]. But whenever m+1 &#060; the agent cannot coherently have such a collection m+1 such beliefs (on the threshold model).

As Greg points out, on this model we cannot have the following simple closure condition: if Bel[A] and Bel[B], then Bel[A&#038;B]. (It would lead to incoherence, to
Bel[A1&#038;...&#038;An&#038;(-A1v...v-An)] in the lottery case.) The probabilistic threshold model avoids this because on that model the closure condition is unsound.

Now, Greg wants to put Bel into the object language, so that the preface case, for example, looks something like this:
Bel[A1]&#038; ... &#038; Bel[An] &#038; Bel[-A1 v ... v -An].

I see no real problem with doing this. One may be able to put Bel and P into the object language, just as one can put &#039;is true&#039; into the object language, provided one takes proper care with how it&#039;s done. In doing so, one must still refuse &#039;(Bel[A] &#038; Bel[B]) &#8835; Bel[A&#038;B]&#039;, or there&#039;ll be trouble. But Greg&#039;s point, I think, is that if one puts Bel into the object language, one should want to specify inference rules that govern it. The threshold model has rules for probabilities, but not for Bel. The logic of Bel just supervenes on the logic of epistemic probabilities.
I don&#039;t see this as a problem, I guess. But the project of putting Bel into the object language, and spelling out the inference rules that govern it, is certainly worth doing.
However, I didn&#039;t think that this metalanguage/object language issue is what was bothering Jon about modeling belief with epistemic probability. But let&#039;s ask him. Is this the problem, Jon?]]></description>
		<content:encoded><![CDATA[<p>Greg,</p>
<p>Is this the idea?</p>
<p>Suppose we represent rational acceptance, i.e. belief, by a predicate Bel, so that Bel[A] say that the agent believes that A. In formalizing this, one might take Bel to be a metalinguistic predicate, rather like &#8220;truth under an interpretation&#8221; is metalinguistic, or like epistemic probability, which is usually treated formally as a metalinguistic function on object language sentences (e.g. we write &#8216;if P[A&amp;B] &ge; P[C], then P[A] &ge; P[C]&#8217;<br />
rather than &#8216;P[A&amp;B] &ge; P[C] &sup; P[A] &ge; P[C]&#8217; because &#8216;&sup;&#8217; is part of the object language, but we usually treat &#8216;P&#8217; and &#8216;&ge;&#8217; and the &#8216;if, then&#8217; in this context metalinguistically).</p>
<p>So, given a metalinguist treatment of Bel, suppose we use a threshold model for belief:<br />
we model Bel by a probability function P and a threshold t, so that, Bel[A] (for the agent) just when P[A]&ge; t. Now, on such a model, we can treat the lottery and the preface quite plausibly. Provided that n+1 &ge; t, the agent can have: Bel[A1] and &#8230; and Bel[An] and Bel[-A1 v &#8230; v -An]. But whenever m+1 &lt; the agent cannot coherently have such a collection m+1 such beliefs (on the threshold model).</p>
<p>As Greg points out, on this model we cannot have the following simple closure condition: if Bel[A] and Bel[B], then Bel[A&amp;B]. (It would lead to incoherence, to<br />
Bel[A1&amp;&#8230;&amp;An&amp;(-A1v&#8230;v-An)] in the lottery case.) The probabilistic threshold model avoids this because on that model the closure condition is unsound.</p>
<p>Now, Greg wants to put Bel into the object language, so that the preface case, for example, looks something like this:<br />
Bel[A1]&amp; &#8230; &amp; Bel[An] &amp; Bel[-A1 v &#8230; v -An].</p>
<p>I see no real problem with doing this. One may be able to put Bel and P into the object language, just as one can put &#8216;is true&#8217; into the object language, provided one takes proper care with how it&#8217;s done. In doing so, one must still refuse &#8216;(Bel[A] &amp; Bel[B]) &sup; Bel[A&amp;B]&#8217;, or there&#8217;ll be trouble. But Greg&#8217;s point, I think, is that if one puts Bel into the object language, one should want to specify inference rules that govern it. The threshold model has rules for probabilities, but not for Bel. The logic of Bel just supervenes on the logic of epistemic probabilities.<br />
I don&#8217;t see this as a problem, I guess. But the project of putting Bel into the object language, and spelling out the inference rules that govern it, is certainly worth doing.<br />
However, I didn&#8217;t think that this metalanguage/object language issue is what was bothering Jon about modeling belief with epistemic probability. But let&#8217;s ask him. Is this the problem, Jon?</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/arbitrary-actions-and-arbitrary-beliefs/#comment-3381</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Mon, 13 Mar 2006 15:44:41 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=487#comment-3381</guid>
		<description><![CDATA[Jim and Jon: What do you think about the following line:

What separates your cleaned-up positions on  rational acceptance is that Jon is thinking in terms of the syntax of rational acceptance, whereas Jim is thinking in terms of the semantics of rational acceptance. A person interested in a closure principle is interested in syntactic rules for how to manipulate collections of sentences labeled as rationally accepted, and to treat them as such *inside the object language*. Boolean conjunction + the probability axioms + allowing acceptance to take values less than unity add up to trouble, which I think we all can agree. There is nothing in here to carve out the models Jim is pointing to, in other words.

Jim&#039;s point is that we can isolate those models in which various combinations of rationally accepted sentences with high but less than total probability are sound. And, making some assumptions about the particulars of lottery and preface puzzles, we can pick all of them out. Then you can introduce a connective if you like that corresponds to that class of models that are sound with respect to a particular threshold point.

I&#039;ve argued that there is a tension between these two views of the matter. The kind of thing we want a connective to do in this case (e.g., to serve duty in effecting the application of a closure operation, not simply the meta-language description of one; to allow the encoding of rationally accepted statements in cases where we are ignorant of the probabilistic relationship between the events (propositions) &lt;i&gt;which includes not having grounds for assuming that they are independent&lt;/i&gt;) and the result that Jim provides. To get the kind of modularity behavior that one would like, you lose completeness i.e., there may be collections of rationally acceptable statements that live in Jim&#039;s model that our syntax cannot get to. Of course, all of the rules should yield only statements that live in Jim&#039;s model. (If we don&#039;t at least have soundness, then we don&#039;t have much of a logic.)

This kind of clash between syntax and semantics is familar in other branches of logic; it isn&#039;t unique to the discussion here. It is common to find two proofs of a theorem, so both proofs &#039;living&#039; in the same model, yet there be limits or quirks in exchanging the syntactic pieces of these proofs with one another and preserving theoremhood. All of this is not necessarily seen by the model theorist, since his job is to tell you &quot;Yes, this theorem is sound&quot; not &quot;Here is the recipe and ingredients for making sound theorems.&quot;

Jim might insist he&#039;s given us this, but describing procedures in the meta-language is very different than have those tools in the object language.

And the puzzle behind rational acceptance concerns how to construct rationally accepted sentences from other rationally accepted sentences, i.e., to provide the structure of rational acceptance.]]></description>
		<content:encoded><![CDATA[<p>Jim and Jon: What do you think about the following line:</p>
<p>What separates your cleaned-up positions on  rational acceptance is that Jon is thinking in terms of the syntax of rational acceptance, whereas Jim is thinking in terms of the semantics of rational acceptance. A person interested in a closure principle is interested in syntactic rules for how to manipulate collections of sentences labeled as rationally accepted, and to treat them as such *inside the object language*. Boolean conjunction + the probability axioms + allowing acceptance to take values less than unity add up to trouble, which I think we all can agree. There is nothing in here to carve out the models Jim is pointing to, in other words.</p>
<p>Jim&#8217;s point is that we can isolate those models in which various combinations of rationally accepted sentences with high but less than total probability are sound. And, making some assumptions about the particulars of lottery and preface puzzles, we can pick all of them out. Then you can introduce a connective if you like that corresponds to that class of models that are sound with respect to a particular threshold point.</p>
<p>I&#8217;ve argued that there is a tension between these two views of the matter. The kind of thing we want a connective to do in this case (e.g., to serve duty in effecting the application of a closure operation, not simply the meta-language description of one; to allow the encoding of rationally accepted statements in cases where we are ignorant of the probabilistic relationship between the events (propositions) <i>which includes not having grounds for assuming that they are independent</i>) and the result that Jim provides. To get the kind of modularity behavior that one would like, you lose completeness i.e., there may be collections of rationally acceptable statements that live in Jim&#8217;s model that our syntax cannot get to. Of course, all of the rules should yield only statements that live in Jim&#8217;s model. (If we don&#8217;t at least have soundness, then we don&#8217;t have much of a logic.)</p>
<p>This kind of clash between syntax and semantics is familar in other branches of logic; it isn&#8217;t unique to the discussion here. It is common to find two proofs of a theorem, so both proofs &#8216;living&#8217; in the same model, yet there be limits or quirks in exchanging the syntactic pieces of these proofs with one another and preserving theoremhood. All of this is not necessarily seen by the model theorist, since his job is to tell you &#8220;Yes, this theorem is sound&#8221; not &#8220;Here is the recipe and ingredients for making sound theorems.&#8221;</p>
<p>Jim might insist he&#8217;s given us this, but describing procedures in the meta-language is very different than have those tools in the object language.</p>
<p>And the puzzle behind rational acceptance concerns how to construct rationally accepted sentences from other rationally accepted sentences, i.e., to provide the structure of rational acceptance.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Michael Huemer</title>
		<link>http://certaindoubts.com/arbitrary-actions-and-arbitrary-beliefs/#comment-3364</link>
		<dc:creator><![CDATA[Michael Huemer]]></dc:creator>
		<pubDate>Mon, 13 Mar 2006 06:17:39 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=487#comment-3364</guid>
		<description><![CDATA[If you want to get the result that it&#039;s rational for the ass to eat either bale of hay, you can change the &quot;only if&quot; in my principle to an &quot;if and only if&quot;, thus:

&lt;blockquote&gt;It is rational to A iff there is no B such that it is more rational to B than to A.
&lt;/blockquote&gt;

It still isn&#039;t rational to believe P when the evidence is evenly balanced between P and ~P, because in any such situation, the alternative of withholding is more epistemically rational. (This is the asymmetry between belief and action: when actions A and B are equally practically justified, it doesn&#039;t follow that inaction is more rational than either. But the analogous conclusion does follow for beliefs.)

I don&#039;t deny that there are epistemic mere permissions, though: I&#039;ll allow that there could be a case in which believing P and withholding P are equally rational; in that case, I&#039;d think that the subject would be rational whichever he did (as long as he doesn&#039;t disbelieve P).

On the lottery/preface: I&#039;m confused -- I don&#039;t get what the problem is. As Jim pointed out (#18), if you give a simple interpretation of belief as having a high subjective probability, you can straightforwardly derive the rationality in certain possible circumstances (i.e., for certain coherent probability distributions) of believing each of A1...An while disbelieving their conjunction.

Now, one possible problem in connection with the lottery is that some people think intuitively that one &lt;i&gt;should not&lt;/i&gt; believe &quot;this ticket will lose&quot;, even though it is highly probable. Of course, this suggests that believing is not just assigning a high subjective probability.

But not to worry, because we can still use probability to explain preface cases. All we have to suppose is that having a high probability is a &lt;i&gt;necessary condition&lt;/i&gt; on a proposition&#039;s being rational to believe (but the probability need not be 1). There are also, perhaps, other necessary conditions. But if you take enough propositions that have a high probability (and are to some degree independent) and also satisfy the other necessary conditions, whatever they are, for being justified and conjoin them together, you&#039;ll get a conjunction that doesn&#039;t have a high enough probability to be justified. That&#039;s the explanation of the preface &#039;paradox&#039;, without assuming that high probability is sufficient for justification. In the lottery case, we have to find some other necessary condition on rational belief that the belief &quot;this ticket will lose&quot; fails to satisfy. (For example, it might be the condition that one should not be justified in believing that one&#039;s evidence for P is also equally strong evidence for a false proposition, or something in that vicinity.)]]></description>
		<content:encoded><![CDATA[<p>If you want to get the result that it&#8217;s rational for the ass to eat either bale of hay, you can change the &#8220;only if&#8221; in my principle to an &#8220;if and only if&#8221;, thus:</p>
<blockquote><p>It is rational to A iff there is no B such that it is more rational to B than to A.
</p></blockquote>
<p>It still isn&#8217;t rational to believe P when the evidence is evenly balanced between P and ~P, because in any such situation, the alternative of withholding is more epistemically rational. (This is the asymmetry between belief and action: when actions A and B are equally practically justified, it doesn&#8217;t follow that inaction is more rational than either. But the analogous conclusion does follow for beliefs.)</p>
<p>I don&#8217;t deny that there are epistemic mere permissions, though: I&#8217;ll allow that there could be a case in which believing P and withholding P are equally rational; in that case, I&#8217;d think that the subject would be rational whichever he did (as long as he doesn&#8217;t disbelieve P).</p>
<p>On the lottery/preface: I&#8217;m confused &#8212; I don&#8217;t get what the problem is. As Jim pointed out (#18), if you give a simple interpretation of belief as having a high subjective probability, you can straightforwardly derive the rationality in certain possible circumstances (i.e., for certain coherent probability distributions) of believing each of A1&#8230;An while disbelieving their conjunction.</p>
<p>Now, one possible problem in connection with the lottery is that some people think intuitively that one <i>should not</i> believe &#8220;this ticket will lose&#8221;, even though it is highly probable. Of course, this suggests that believing is not just assigning a high subjective probability.</p>
<p>But not to worry, because we can still use probability to explain preface cases. All we have to suppose is that having a high probability is a <i>necessary condition</i> on a proposition&#8217;s being rational to believe (but the probability need not be 1). There are also, perhaps, other necessary conditions. But if you take enough propositions that have a high probability (and are to some degree independent) and also satisfy the other necessary conditions, whatever they are, for being justified and conjoin them together, you&#8217;ll get a conjunction that doesn&#8217;t have a high enough probability to be justified. That&#8217;s the explanation of the preface &#8216;paradox&#8217;, without assuming that high probability is sufficient for justification. In the lottery case, we have to find some other necessary condition on rational belief that the belief &#8220;this ticket will lose&#8221; fails to satisfy. (For example, it might be the condition that one should not be justified in believing that one&#8217;s evidence for P is also equally strong evidence for a false proposition, or something in that vicinity.)</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Chad Mohler</title>
		<link>http://certaindoubts.com/arbitrary-actions-and-arbitrary-beliefs/#comment-3382</link>
		<dc:creator><![CDATA[Chad Mohler]]></dc:creator>
		<pubDate>Sun, 12 Mar 2006 20:12:21 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=487#comment-3382</guid>
		<description><![CDATA[Jon,

Your original post mentions that there is an important difference between actions and beliefs: the choice of one action from among equally attractive (but mutually exclusive) alternatives can be rational, whereas the choice of one belief from among (evidentially) equally attractive (but mutually exclusive) alternatives cannot be rational (at least in a cognitive/epistemic sense of rational).  I think, though, that if we were to formulate carefully the comparison between equally attractive beliefs and actions, we would see the difference disappear.

Let&#039;s start with the case of belief.  We might plausibly say that a belief that p is cognitively/epistemically rational if the belief is reasonable in light of our goal of now having true beliefs (and now not having false beliefs) concerning the subject matter of p.  What is it, then that keeps belief in one of several (evidentially) equally attractive (but mutually exclusive) alternatives from being epistemically rational?  Well, it&#039;s that because we know the alternatives to be mutually exclusive, we know that at most one of them is true.  But because our evidence does not reveal which one is true (they are, after all, equally evidentially attractive), believing one of them without additional evidence makes us run the risk of falling short of our goal of now having true beliefs and now not having false beliefs about the subject matter in question.  So believing one of them is not epistemically rational.

What&#039;s the proper analog here in the case of action?  It&#039;s not, I contend, a Buridan&#039;s ass-style case, in which the agent knows the value of each alternative, and that value is equal across the alternatives.  The salient feature of the belief case is that only one of the alternatives had the quality we desired (namely, truth), but that we did not know which alternative had that quality.  So in the case of action, we should construct our case similarly-- one in which only one of the actions under consideration has the attractive quality (say, for instance, the maximization of some utility), but we are not sure which.  Suppose, for example, each year you&#039;re in charge of awarding a small scholarship to one of a group of students, but this year you do not personally know what the qualifications of each student are-- you&#039;ve not looked at their applications or letters of recommendation, you&#039;ve never had them in class, and no one has revealed any relevant individual-specific info about them.  Suppose, however, that one of your colleagues reveals to you that several of the candidates are remarkably well-qualified, but also that several of the candidates are likely to squander the opportunity the scholarship presents.  Mysteriously enough, the colleague doesn&#039;t reveal anything about which ones are which (perhaps wanting you to make up your own mind about the candidates).  I contend here we have a case of action that is properly analogous to the case of equally epistemically attractive belief, with the moral being the same in each case.  Just as we risk falling short of our epistemic goal of having true beliefs now and now not having false beliefs about the subject matter in question in the belief case, so we also risk falling short of the goal of maximizing utility by awarding the scholarship without further exploration of the candidate&#039;s qualifications.  In each case, the move in question-- adopting the belief, performing the action-- is irrational.]]></description>
		<content:encoded><![CDATA[<p>Jon,</p>
<p>Your original post mentions that there is an important difference between actions and beliefs: the choice of one action from among equally attractive (but mutually exclusive) alternatives can be rational, whereas the choice of one belief from among (evidentially) equally attractive (but mutually exclusive) alternatives cannot be rational (at least in a cognitive/epistemic sense of rational).  I think, though, that if we were to formulate carefully the comparison between equally attractive beliefs and actions, we would see the difference disappear.</p>
<p>Let&#8217;s start with the case of belief.  We might plausibly say that a belief that p is cognitively/epistemically rational if the belief is reasonable in light of our goal of now having true beliefs (and now not having false beliefs) concerning the subject matter of p.  What is it, then that keeps belief in one of several (evidentially) equally attractive (but mutually exclusive) alternatives from being epistemically rational?  Well, it&#8217;s that because we know the alternatives to be mutually exclusive, we know that at most one of them is true.  But because our evidence does not reveal which one is true (they are, after all, equally evidentially attractive), believing one of them without additional evidence makes us run the risk of falling short of our goal of now having true beliefs and now not having false beliefs about the subject matter in question.  So believing one of them is not epistemically rational.</p>
<p>What&#8217;s the proper analog here in the case of action?  It&#8217;s not, I contend, a Buridan&#8217;s ass-style case, in which the agent knows the value of each alternative, and that value is equal across the alternatives.  The salient feature of the belief case is that only one of the alternatives had the quality we desired (namely, truth), but that we did not know which alternative had that quality.  So in the case of action, we should construct our case similarly&#8211; one in which only one of the actions under consideration has the attractive quality (say, for instance, the maximization of some utility), but we are not sure which.  Suppose, for example, each year you&#8217;re in charge of awarding a small scholarship to one of a group of students, but this year you do not personally know what the qualifications of each student are&#8211; you&#8217;ve not looked at their applications or letters of recommendation, you&#8217;ve never had them in class, and no one has revealed any relevant individual-specific info about them.  Suppose, however, that one of your colleagues reveals to you that several of the candidates are remarkably well-qualified, but also that several of the candidates are likely to squander the opportunity the scholarship presents.  Mysteriously enough, the colleague doesn&#8217;t reveal anything about which ones are which (perhaps wanting you to make up your own mind about the candidates).  I contend here we have a case of action that is properly analogous to the case of equally epistemically attractive belief, with the moral being the same in each case.  Just as we risk falling short of our epistemic goal of having true beliefs now and now not having false beliefs about the subject matter in question in the belief case, so we also risk falling short of the goal of maximizing utility by awarding the scholarship without further exploration of the candidate&#8217;s qualifications.  In each case, the move in question&#8211; adopting the belief, performing the action&#8211; is irrational.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: kvanvig</title>
		<link>http://certaindoubts.com/arbitrary-actions-and-arbitrary-beliefs/#comment-3380</link>
		<dc:creator><![CDATA[kvanvig]]></dc:creator>
		<pubDate>Fri, 10 Mar 2006 17:50:10 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=487#comment-3380</guid>
		<description><![CDATA[Mike, one version of the preface is the fallibility paradox, where you have a number of beliefs but also believe that some of your beliefs are false.  That&#039;s the version I was using to show how to get the contradiction.]]></description>
		<content:encoded><![CDATA[<p>Mike, one version of the preface is the fallibility paradox, where you have a number of beliefs but also believe that some of your beliefs are false.  That&#8217;s the version I was using to show how to get the contradiction.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Mike Almeida</title>
		<link>http://certaindoubts.com/arbitrary-actions-and-arbitrary-beliefs/#comment-3379</link>
		<dc:creator><![CDATA[Mike Almeida]]></dc:creator>
		<pubDate>Fri, 10 Mar 2006 17:45:57 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=487#comment-3379</guid>
		<description><![CDATA[Jon,

I see the contradiction, but your (4) misrepresents S&#039;s beliefs.

4. Some (first order) proposition S believes is false.
                     (Ep)Bs~p

I don&#039;t see how we might arrive at S&#039;s having such a belief.
What S believes, instead, is (4&#039;)

4&#039;. S believes that some proposition or other is false.
                     Bs(Ep)~p

S believes, for instance, that some proposition or other in the book is mistaken or that someone or other will win the lottery. But it is not true that there is some proposition in the book such that S believes it is false and it is not true that there is some person such that S believes he will win the lottery. I am of course restricting quantification to the two propositions b and a that S believes. And (4&#039;) is consistent with Bsb and Bsa.]]></description>
		<content:encoded><![CDATA[<p>Jon,</p>
<p>I see the contradiction, but your (4) misrepresents S&#8217;s beliefs.</p>
<p>4. Some (first order) proposition S believes is false.<br />
                     (Ep)Bs~p</p>
<p>I don&#8217;t see how we might arrive at S&#8217;s having such a belief.<br />
What S believes, instead, is (4&#8242;)</p>
<p>4&#8242;. S believes that some proposition or other is false.<br />
                     Bs(Ep)~p</p>
<p>S believes, for instance, that some proposition or other in the book is mistaken or that someone or other will win the lottery. But it is not true that there is some proposition in the book such that S believes it is false and it is not true that there is some person such that S believes he will win the lottery. I am of course restricting quantification to the two propositions b and a that S believes. And (4&#8242;) is consistent with Bsb and Bsa.</p>
]]></content:encoded>
	</item>
	<item>
		<title>By: Gregory Wheeler</title>
		<link>http://certaindoubts.com/arbitrary-actions-and-arbitrary-beliefs/#comment-3378</link>
		<dc:creator><![CDATA[Gregory Wheeler]]></dc:creator>
		<pubDate>Fri, 10 Mar 2006 14:21:46 +0000</pubDate>
		<guid isPermaLink="false">http://fleetwood.baylor.edu/certain_doubts/?p=487#comment-3378</guid>
		<description><![CDATA[Let me suggest an analogy that might be helpful. Forget about probability for the moment and think about another measure, temperature. (Actually, temperature is a magnitude. But this is an epistemology blog.) Temperature is not additive in the sense that a gallon of 50 degree F water added to another gallon of 50 degree F water does not give you two gallons of 100 degree F water. (Alas, &lt;i&gt;this&lt;/i&gt; would solve our energy crisis.)

Thinking about the structure of this magnitude is to think about the structure of the scale we &lt;i&gt;should&lt;/i&gt; use to measure temperature. We do not identify the structure of the natural numbers to be a good model, for example, because there 50 and 50 is 100, and we are looking for the story of why 50 plus 50 is 50. We press the natural numbers into service in delivering this story; but they don&#039;t wear the story on their sleeves.

This is not at all to say that temperatures cannot be added. We sum temperatures to calculate their average, for instance. We combine them arithmetically to constants in order to convert between scales, and combine them to other magnitudes for purposes of calculating yet other magnitudes or relationships between magnitudes.

The picture that I hope to install in readers minds is that there is a difference between the structure of temperature and operations that we can perform using temperatures.

With that picture in place, turn to probability. We can fashion a weakly aggregate logic for probabilities. We can define connectives or a logic that combines probabilities in a ways that are not strictly additive. But there is a tension in doing this, becuase this is going against the natural order of probability. It is a way of bending probability to one&#039;s will. And you lose things by doing this. (Or you keep things at the expensive of an ad hoc system.)

Discussions on this often get messed up when we confuse these mathematical constraints for normative constraints.]]></description>
		<content:encoded><![CDATA[<p>Let me suggest an analogy that might be helpful. Forget about probability for the moment and think about another measure, temperature. (Actually, temperature is a magnitude. But this is an epistemology blog.) Temperature is not additive in the sense that a gallon of 50 degree F water added to another gallon of 50 degree F water does not give you two gallons of 100 degree F water. (Alas, <i>this</i> would solve our energy crisis.)</p>
<p>Thinking about the structure of this magnitude is to think about the structure of the scale we <i>should</i> use to measure temperature. We do not identify the structure of the natural numbers to be a good model, for example, because there 50 and 50 is 100, and we are looking for the story of why 50 plus 50 is 50. We press the natural numbers into service in delivering this story; but they don&#8217;t wear the story on their sleeves.</p>
<p>This is not at all to say that temperatures cannot be added. We sum temperatures to calculate their average, for instance. We combine them arithmetically to constants in order to convert between scales, and combine them to other magnitudes for purposes of calculating yet other magnitudes or relationships between magnitudes.</p>
<p>The picture that I hope to install in readers minds is that there is a difference between the structure of temperature and operations that we can perform using temperatures.</p>
<p>With that picture in place, turn to probability. We can fashion a weakly aggregate logic for probabilities. We can define connectives or a logic that combines probabilities in a ways that are not strictly additive. But there is a tension in doing this, becuase this is going against the natural order of probability. It is a way of bending probability to one&#8217;s will. And you lose things by doing this. (Or you keep things at the expensive of an ad hoc system.)</p>
<p>Discussions on this often get messed up when we confuse these mathematical constraints for normative constraints.</p>
]]></content:encoded>
	</item>
</channel>
</rss>
